{
    "026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731": {
        "doc1": {
            "0": "paper_summary:",
            "1": "This paper proposes, as a form of data augmentation, to replace the one-hot sequences that are typically consumed by text classification models with an interpoloation between this one-hot distribution and a distribution over word-types obtained by running the sequence through BERT.",
            "2": "The authors show that this form of augmentation helps on its own as well as when combined with other standard data augmentation techniques.",
            "3": "summary_of_strengths:",
            "4": "- The paper obtains good results with a straightforward approach.",
            "5": "- The paper is written fairly clearly.",
            "6": "summary_of_weaknesses:",
            "7": "- The paper needs some light editing (especially Section 4.1) but I don't see any significant weaknesses.",
            "8": "comments,_suggestions_and_typos:",
            "9": "- One thing I wasn't sure I understood is whether the \"smoothed\" inputs are used on their own to train the model, or if they're used in addition to the standard inputs.",
            "10": "Lines 236-239 make me think they're used in addition.",
            "11": "This is fine, but should be emphasized more explicitly.",
            "12": "- Since, as the authors note, they are are merely approximating the token-distribution given by BERT (by not using any MASK tokens), it might be interesting to see whether this approximation is in fact hurting the performance or not.",
            "13": "That is, if we obtain token-level distributions by masking each token in the input in turn, and then use the resulting smoothed representations, is this better or worse for augmentation than the approximation the authors propose?"
        },
        "doc2": {
            "0": "Text Smoothing: Enhance Various Data Augmentation Methods on Text Classification Tasks",
            "1": "Abstract",
            "2": "Before entering the neural network, a token is generally converted to the corresponding onehot representation, which is a discrete distribution of the vocabulary. ",
            "3": "Smoothed representation is the probability of candidate tokens obtained from a pre-trained masked language model, which can be seen as a more informative substitution to the one-hot representation. ",
            "4": "We propose an efficient data augmentation method, termed text smoothing, by converting a sentence from its one-hot representation to a controllable smoothed representation. ",
            "5": "We evaluate text smoothing on different benchmarks in a low-resource regime. ",
            "6": "Experimental results show that text smoothing outperforms various mainstream data augmentation methods by a substantial margin. ",
            "7": "Moreover, text smoothing can be combined with those data augmentation methods to achieve better performance.",
            "8": "Introduction",
            "9": "Data augmentation is a widely used technique, especially in the low-resource regime. ",
            "10": "It increases the size of the training data to alleviate overfitting and improve the robustness of deep neural networks. ",
            "11": "In the field of natural language processing (NLP), various data augmentation techniques have been proposed. ",
            "12": "One most commonly used method is to randomly select tokens in a sentence and replace them with semantically similar tokens to synthesize a new sentence (Wei and Zou, 2019;Kobayashi, 2018;. (Kobayashi, 2018) proposes contextual augmentation to predict the probability distribution of replacement tokens by using the LSTM language model and sampling the replacement tokens according to the probability distribution. ) uses BERT's (Devlin et al., 2018) masked language modeling (MLM) task to extend contextual augmentation by considering deep bi-directional context. ",
            "13": "(Kumar et al., 2020) further propose to use different types of transformer based pre-trained models for MLM takes masked sentences as input, and typically 15% of the original tokens in the sentences will be replaced by the [MASK] token. ",
            "14": "Before entering MLM, each token in sentences needs to be converted to its one-hot representation, a vector of the vocabulary size with only one position is 1 while the rest positions are 0. ",
            "15": "MLM outputs the probability distribution of the vocabulary size of each mask position. ",
            "16": "Through large-scale pretraining, it is expected that the probability distribution is as close as possible to the ground-truth one-hot representation. ",
            "17": "Compared with the onehot representation, the probability distribution predicted by pre-trained MLM is a \"smoothed\" representation, which can be seen as a set of candidate tokens with different weights. ",
            "18": "Usually, most of the weights are distributed on contextual-compatible tokens. ",
            "19": "Multiplying the smooth representation by the word embedding matrix can obtain a weighted summation of the word embeddings of the candidate words, termed smoothed embedding, which is more informative and context-rich than the onehot's embedding obtained through lookup operation. ",
            "20": "Therefore, the use of smoothed representation instead of one-hot representation as the input of the model can be seen as an efficient weighted data augmentation method. ",
            "21": "To get the smoothed representation of all the tokens of the entire sentence with only one forward process in MLM, we do not explicitly mask the input. ",
            "22": "Instead, we turn on the dropout of MLM and dynamically randomly discard a portion of the weight and hidden state at each layer.",
            "23": "An unneglectable situation is that some tokens appear more frequently than others in similar contexts during pre-training, which will cause the model to have a preference for these tokens. ",
            "24": "This is harmful for downstream tasks such as fine-grained sentiment classification. ",
            "25": "For example, given \"The quality of this shirt is average .\", the \"average\" token is most relevant to the label. ",
            "26": "The smoothed representation through the MLM at the position of \"average\" is shown in Figure 2. ",
            "27": "Although the probability of \"average\" is the highest, more probabilities are concentrated on tokens conflict with the task label, such as \"high\", \"good\" or \"poor\". ",
            "28": "Such a smoothed representation is hardly a good augmented input for the task. ",
            "29": "To solve this problem,  proposed to train label embedding to constraint MLM predict label compatible tokens. ",
            "30": "However, under the condition of low resources, it is not easy to have enough label data to provide supervision. ",
            "31": "We get inspiration from the practical data augmentation method mixup (Zhang et al., 2017) in the computer vision field. ",
            "32": "We interpolate the smoothed representation with the original onehot representation. ",
            "33": "Through interpolation, we can enlarge the probability of the original token, and the probabilities are still mostly distributed on the context-compatible words, as shown in the figure 2.",
            "34": "We combine the two stages as text smoothing: obtaining a smooth representation through MLM and interpolating to constrain the representation more controllable. ",
            "35": "To evaluate the effect of text smoothing, we perform experiments with low-resource settings on three classification benchmarks. ",
            "36": "In all experiments, text smoothing achieves better performance than other data augmentation methods. ",
            "37": "Further, we are pleased to find that text smoothing can be combined with other data augmentation methods to improve the tasks further. ",
            "38": "To the best of our knowledge, this is the first method to improve a variety of mainstream data augmentation methods.",
            "39": "Related Work",
            "40": "Various NLP data augmentation techniques have been proposed and they are mainly divided into two categories: one is to modify raw input directly, and the other interferes with the embedding (Miyato et al., 2016;. The most commonly used method to modify the raw input is the token replacement: randomly select tokens in a sentence and replace them with semantically similar tokens to synthesize a new sentence. (Wei and Zou, 2019) directly uses the synonym table WordNet(Miller, 1998) for replacement. ",
            "41": "(Kobayashi, 2018) proposes contextual augmentation to predict the probability distribution of replacement tokens with two causal language models. ",
            "42": " extends contextual augmentation with BERT's (Devlin et al., 2018) masked language modeling (MLM) to consider bi-directional context. ",
            "43": "(Gao et al., 2019) softly augments a randomly chosen token in a sentence by replacing its one-hot representation with the distribution of the vocabulary provided by the causal language model in machine translation. ",
            "44": "Unlike (Gao et al., 2019), we use MLM to generate smoothed representation, which considers the deep bi-directional context more adequately. And our method has better parallelism, which can efficiently obtain the smoothed representation of the entire sentence in one forward process. ",
            "45": "Moreover, we propose to constrain smoothed representation more controllable through interpolation for classification tasks.",
            "46": "3 Our Method",
            "47": "Smoothed Representation",
            "48": "We use BERT as a representative example of MLM. ",
            "49": "Given a downstream task dataset, namely",
            "50": ", where N is the number of Listing 1: Codes to implement text smoothing in PyTorch instances, t i is the one-hot encoding of a text (a single sentence or a sentence pair), p i is the positional encoding of t i , s i is the segment encoding of t i and l i is the label of this instance. ",
            "51": "We feed the one-hot encoding t i , positional encoding p i as well as the segment encoding s i into BERT, and fetch the output of the last layer of the transformer encoder in BERT, which is denoted as:",
            "52": "where \u2212 \u2192 t i \u2208 R seq_len,emb_size is a 2D dense vector in shape of [sequence_len, embedding_size]. ",
            "53": "We then multiply \u2212 \u2192 t i with the word embedding matrix W \u2208 R vocab_size,embed_size in BERT, to get the MLM prediction results, which is defined as:",
            "54": "where each row in MLM(t i ) is a probability distribution over the token vocabulary, representing the context-compatible token choices in that position of the input text learned by pre-trained BERT.",
            "55": "Mixup Strategy",
            "56": "The mixup (Zhang et al., 2017) is defined as:",
            "57": "(x i , y i ) and (x j , y j ) are two feature-target vectors drawn at random from the training data, and \u03bb \u2208 [0, 1]. ",
            "58": "In text smoothing, the one-hot representation and smoothed representation are derived from the same raw input, their lables are identical and the interpolation operation will not change the label. ",
            "59": "So the mixup operation can be simplified to:",
            "60": "t i is the one-hot representation, MLM(t i ) is the smoothed representation, t i is the interpolated representation and \u03bb is the balance hyperparameter to control interpolation strength. ",
            "61": "In the downstream tasks, we use interpolated representation instead of the original one-hot representation as input. ",
            "62": "CBERT  masks some tokens and predicts their contextual substitutions with pretrained BERT. ",
            "63": "BERTexpand, BERTprepend (Kumar et al., 2020) conditions BERT by prepending class labels to all examples of given class. ",
            "64": "\"expand\" a the label to model vocabulary, while \"prepend\" without.",
            "65": "GPT2context (Kumar et al., 2020) provides a prompt to the pre-trained GPT model and keeping generating until the EOS token. ",
            "66": "BARTword, BARTspan (Kumar et al., 2020) conditions BART by prepending class labels to all examples of given class. ",
            "67": "BARTword masks a single word while BARTspan masks a continuous chunk.",
            "68": "Experiment Setting",
            "69": "Our experiment strictly follows the settings in the (Kumar et al., 2020)    TREC (Li and Roth, 2002) contains six question types collected from 4,500 English questions.",
            "70": "We randomly subsample 10 examples per class for each experiment for both training and development set to simulate a low-resource regime. ",
            "71": "Data statistics of the three datasets are shown in Table 1. ",
            "72": "Following (Kumar et al., 2020), we replace numeric class labels with their text versions.",
            "73": "We first compare the effects of text smoothing and baselines data augmentation methods on different datasets in a low-resource regime. ",
            "74": "Then we further explore the effect of combining text smoothing with each baseline method. ",
            "75": "Considering that the amount of data increases to 2 times after combination, we expand the data used in the baseline experiments to the same amount for the fairness of comparison. ",
            "76": "All experiments are repeated 15 times to account for stochasticity and results are reported as Mean (STD) accuracy on the full test set.",
            "77": "Experimental Results",
            "78": "As shown in Table2, text smoothing brings the largest improvement to the model on the three datasets compared with other data augmentation methods. ",
            "79": "Compared with training without data augmentation, text smoothing achieves an average improvement of 11.62% on the three datasets, which is significant. ",
            "80": "The previously best method is BARTspan, which is exceeded by Text smoothing with 1.17% in average.",
            "81": "Moreover, we are pleased to find that text smoothing can be well combined with various data augmentation methods, further improving the baseline data augmentation methods. ",
            "82": "As shown in Ta-ble3, text smoothing can bring significant improvements of 5.98%, 2.79%, 2.39%, 2.92%, 2.17%, 6.48%, 3.21%, 3.03% to EDA, BackTrans, CBERT, BERTexpand, BERTprepend, GPT2context, BARTword, and BARTspan, respectively. ",
            "83": "To the best of our knowledge, this is the first method to improve a variety of mainstream data augmentation methods.",
            "84": "Conclusoins",
            "85": "This article proposes text smoothing, an effective data augmentation method, by converting sentences from their one-hot representations to controllable smoothing representations. ",
            "86": "In the case of a low data regime, text smoothing is significantly better than various data augmentation methods. ",
            "87": "Furthermore, text smoothing can further be combined with various data augmentation methods to obtain better performance."
        }
    },
    "2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01": {
        "doc1": {
            "0": "paper_summary:",
            "1": "This well-written paper presents \"DIBIMT\", a manually annotated and curated benchmark to measure semantic biases in word sense disambiguation in machine translation via 4 newly defined metrics.",
            "2": "The framework has 5 translation directions (English to: Chinese, German, Italian, Russian and Spanish), and also provides statistical and linguistic analysis for 7 (non-)commerical neural MT systems.",
            "3": "summary_of_strengths:",
            "4": "-- thorough experimentation",
            "5": "-- interesting benchmark that will be both tough and useful for MT system comparison in terms of their ability to perform word sense disambiguation",
            "6": "-- especially liked the inclusion of 5 (diverse) languages and the comparison against a dedicated word sense disambiguation system",
            "7": "summary_of_weaknesses:",
            "8": "-- none really, maybe only the sheer amount of material that is almost too much for a conference paper format (given the long appendix, as well)",
            "9": "-- ...and maybe that only few annotators have been working on the task without the ability to check for agreements or majority votes",
            "10": "comments,_suggestions_and_typos:",
            "11": "-- some of the formal notations introduced in chapter 3 would probably be easier to understand if put in words",
            "12": "-- but I realize that that would make the paper yet longer again"
        },
        "doc2": {
            "0": "DIBIMT: ",
            "1": "A Novel Benchmark for Measuring Word Sense Disambiguation Biases in Machine Translation",
            "2": "Abstract",
            "3": "Lexical ambiguity poses one of the greatest challenges in the field of Machine Translation. ",
            "4": "Over the last years, multiple efforts have been undertaken to investigate incorrect translations caused by the polysemous nature of words. ",
            "5": "As a result, some studies posited that models pick up semantic biases existing in the training data, thus producing translation errors. ",
            "6": "In this paper, we present DIBIMT, the first entirely manually-curated evaluation benchmark which enables an extensive study of semantic biases in Machine Translation in five different language combinations, namely English and one of the following languages: ",
            "7": "Chinese, German, Italian, Russian and Spanish. ",
            "8": "Furthermore, we test state-of-the-art Machine Translation systems, both commercial and non-commercial, against our new test bed and provide a thorough statistical and linguistic analysis of the results.",
            "9": "Introduction",
            "10": "The polysemous nature of words poses a longstanding challenge in a wide range of Natural Language Processing (NLP) tasks such as Word Sense Disambiguation (Navigli, 2009)  Further research works investigated the disambiguation capabilities of MT systems by exploring their internal representations (Marvin and Koehn, 2018;Michel et al., 2019) or improving them via context-aware word embeddings (Liu et al., 2017).",
            "11": "More recently, Emelin et al. (2020) introduced a statistical method for the identification of disambiguation errors in neural MT (NMT) and demonstrated that models capture data biases within the training corpora, which leads models to produce incorrect translations. ",
            "12": "Although the authors expect their approach to be transferable to other language combinations, they only focus on German \u2192 English.",
            "13": "Based on the findings and open research questions raised in the aforementioned works, the present paper aims at investigating not only the presence, but also, most importantly, the nature and properties of semantic biases in MT in multiple language combinations, via a novel entirely manually-curated benchmark called DIBIMT and a thorough performance analysis.",
            "14": "Building DIBIMT",
            "15": "The DIBIMT benchmark focuses on detecting Word Sense Disambiguation biases in NMT, i.e., biases of certain words towards some of their more frequent meanings. ",
            "16": "The creation of such a dataset requires i) a set of unambiguous and grammaticallycorrect sentences containing a polysemous target word; ii) a set of correct and incorrect translations of each target word into the languages to be covered. ",
            "17": "Figure 1 depicts an example of a dataset item.",
            "18": "Preliminaries",
            "19": "BabelNet Similarly to previous works, we rely on BabelNet 1 (Navigli et al., 2021), a large multilingual encyclopedic dictionary whose nodes are concepts represented by synsets, i.e., sets of synonyms, containing lexicalizations in multiple languages and coming from various heterogeneous resources, including, inter alia, WordNet and Wiktionary. Let us define B as an abstraction that allows us to query BabelNet, specifically, the subset Specifically, we use the examples from WordNet Tagged Glosses (Langone et al., 2004), where each sentence's target word was manually associated with its synset 6 , gracefully providing the first batch of initial items.",
            "20": "As for Wiktionary, instead, we start by obtaining every usage example s and its associated definition d (filtering out archaic usages and slang), then, we automatically extract the target words from the corresponding example. ",
            "21": "7 The only step left to construct an initial item is to associate a sense \u03c3 with the word w i used in the example s. We perform this association in two phases: first, we try to map the definition d related to the example s to a Babel-Net synset by relying on the mappings available in BabelNet 5 between WordNet and Wiktionary, discarding examples for which this association could not be found; second, we manually validate and correct these successful associations to ensure high quality of our initial items.",
            "22": "Sentence Filtering",
            "23": "We apply a filtering step to the original sentences in order to select examples that are likely to be more challenging for the models to translate: i) we discard every sentence X for which \u03b4 EN (\u03bb X P ) < 3, i.e., we only retain sentences whose associated (lemma, POS) pair has a polysemy degree of at least 3 in B EN ; ii) we only retain at most one sentence per sense per source 8 ; iii) differently from previous works, which impose a strict requirement on synsets that are monosemous in the target language, we only require that all their lexicalizations uniquely identify them within all the other possible senses of \u03bb X P .",
            "24": "As an example, let us consider the nominal senses of the word \"bank\": among them, one represents a specific aviation maneuver. ",
            "25": "In Italian, this synset has lexicalizations \"sbandamento\" and \"avvitamento\": although both of these might take on different senses in Italian (e.g., \"sbandamento\" might represent a \"drift\" performed with a car), none of them would have \"bank\" as an English lexicalization, which, for Italian, respects our third condition. ",
            "26": "If the same holds true for all languages, the synset passes the test and thus the sentence is retained.",
            "27": "Annotating the Dataset",
            "28": "As the set of initial items is ready, we can proceed with the annotation phase, which will produce our annotated items.",
            "29": "For instance, given a language L and an initial item X = (s, w i , \u03c3), we associate a set of good (G L ) and bad (B L ) translation candidates to X, which represent words that we do (and do not) expect to see in a translation of sentence s in language L. Finally, we refer to X L as an annotated item, i.e., the tuple (s,",
            "30": "Given the expertise required to carry out this task, we rely on the work of three highly qualified translators: one for Italian, German and Russian, one for Spanish and one for Chinese.",
            "31": "Pre-annotation Item Creation",
            "32": "Before moving forward with the annotation phase, in a fashion similar to that of previous works based on BabelNet, we pre-populate the sets of good (G L )",
            "33": "and bad (B L ) lexicalizations of item X in language L with those in B. Formally, we assign G L = \u039b L (\u03c3), i.e., the set of lemmas in language L of the BabelNet synset associated with \u03c3; furthermore, we set B L = \u03c3\u2208\u2126 L (\u03bb X P )\\{\u03c3} \u039b L (\u03c3), i.e., the set of all lemmas in language L of BabelNet synsets associated with any \u03c3 excluding \u03c3. ",
            "34": "With this step,",
            "35": "we produce an automatically populated version of our annotated items.",
            "36": "Annotation Guidelines",
            "37": "We instruct annotators to update the set of good (G L ) and bad (B L ) translated lexicalizations of w i \u2208 s such that it does reflect what a human translator would use in the context of that sentence. ",
            "38": "9",
            "39": "We also instruct annotators to mark sentences as",
            "40": "This produces an analyzed item, which for sim-330 plicity we denote as X",
            "41": "where R is one of GOOD, BAD or MISS and \u03c9 L",
            "42": "represents the matched lemma in case there was a 333 match (GOOD or BAD), \u01eb otherwise. and metrics reported throughout this section.",
            "43": "10 A more detailed description of the analysis procedure is provided in the Appendix.",
            "44": "Compared Systems",
            "45": "We test a wide range of models, both commercial and non-commercial, and report their performances on DIBIMT's evaluation metrics:",
            "46": "\u2022 DeepL Translator 11 , a state-of-the-art com-      In Figure 3(a), we plot the number and percentage of errors made on average by the models, grouping items by \u00b5 \u03bb X P (\u03c3 X ), where X is a non-MISS analyzed item. ",
            "47": "As expected, the less frequent a meaning is for a given word, the harder it is for the model to correctly disambiguate it.",
            "48": "General Results",
            "49": "Finally, given a (model, language) pair, we define the Sense Frequency Index Influence (SFII) as the average percentage of mistakes, for each group, that we detected. ",
            "50": "Values are reported in Table 5: Frequency Analysis: MFS represents the average percentage of time the model mistakenly translated the target word to a lexicalization belonging to the Most Frequent Sense associated with \u03bb P . ",
            "51": "MFS+, instead, checks whether the wrong translation belongs to any synset that is more frequent than the target one. ",
            "52": "Lower is better.",
            "53": "Sense Polysemy Degree Importance Similarly to SFII, we also study the extent to which the polysemy degree, i.e., how many senses a given word can take upon, impacts the models' disambiguation capabilities. ",
            "54": "This experiment mirrors SFII, but groups items by their lemma's polysemy degree \u03b4 EN (\u03bb X P ) instead of \u00b5.  5.",
            "55": "We can observe a few interesting results: first, on average, almost 60% of the time a mistake reflects the Most Frequent Sense of the target word (secondlast column); second, almost 90% of the mistakes concern translations towards more frequent senses 14 In case there are multiple possible synsets, we take the most frequent according to \u00b5 \u03bb X P , as we need to rely on the assumption that the surface form represents the intrinsic disambiguation performed by the NMT system. ",
            "56": "  Given the low performances achieved by MT mod-490 els, we test a WSD system on the English sentences 491 within DIBIMT, both to assess the toughness of 492 our system and to establish an additional baseline.",
            "57": "We use ESCHER 17 (Barba et al., 2021), a state- items for each (model, language) pair. ",
            "58": "We report 505 these results in Table 6, whose accuracy scores can 506 be directly compared to Table 3.",
            "59": "languages. ",
            "60": "We also disregard DeepL and Google Translate as their architecture is proprietary. ",
            "61": "16 We skip item X if either X M L1 or X M L2 is a MISS. ",
            "62": "17 The publicly available version trained on SemCor only. ",
            "63": "As expected, the average MT accuracy is significantly lower than ESCHER's, with the sole exception of DeepL, which manages to surpass it on German and Russian. ",
            "64": "These results clearly prove that current NMT models are still not on par with dedicated systems, and thus that they might benefit from their inclusion within the NMT ecosystem.",
            "65": "Is this a decoding issue?",
            "66": "As a final experiment, we assess whether the semantic biases are caused by search errors (i.e., failures of the decoding algorithm) or model errors (i.e., the models deemed their translations the best possible). ",
            "67": "For each (model M, language L) pair, we sample a BAD translation (t BAD ) and ask annotators to translate it into L (t GOOD ), then compute the perplexities according to M with the corresponding English sentence s, i.e., p GOOD = p M (t GOOD |s)",
            "68": "and p BAD = p M (t BAD |s). ",
            "69": "We repeat this sampling 50 times per (M, L) pair and check how often p BAD > p GOOD . ",
            "70": "Table 7 shows that, on average, this happens in 93% of cases, thus confirming that most semantic biases are embedded within models and not caused by the decoding strategy.",
            "71": "Conclusions",
            "72": "In this work, we presented DIBIMT, a novel benchmark for measuring and understanding semantic biases in NMT, which goes beyond simple accuracy metrics and provides novel metrics that summarize how biased NMT models are. ",
            "73": "We tested DIBIMT on 7 widely adopted NMT systems, extensively discussing their performances and providing novel insights on the possible causes and relations of semantic biases within NMT models.",
            "74": "Furthermore, statistics of our annotations suggest that, when dealing with translations, synsets' lexicalizations cannot be used interchangeably, as their choice heavily depends on the context.",
            "75": "In the future, we plan to improve DIBIMT's handling of MISS, widen language coverage and expand the annotations.",
            "76": "Our analysis procedure, which we described in Section 3.4, involves steps that go beyond simple lemma matching. ",
            "77": "For instance, in case of multiword expressions, we allowed annotators to specify a wildcard, i.e., any number of tokens (including zero) were allowed to expand and still trigger a match. ",
            "78": "Additionally, since stanza has multi-word expansion tokenization for some of the languages in our list, when available, we try to perform matching on both the list of words (alongside the list of tokens) in the translated sentence. ",
            "79": "Finally, in case no match is produced by the aforementioned steps, we apply a surface-level string matching heuristic which, especially in Chinese, helps us increase coverage.",
            "80": "We use HuggingFace's Transformers library (Wolf et al., 2020) for all neural models. ",
            "81": "As per standard practice, we generate translations using beam search as decoding algorithm with beam size 5.",
            "82": "We include model-specific analyses with perlanguage breakdown of the scores achieved on our benchmark. ",
            "83": "The column named ESCHER represents the scores of the WSD system on the subset of sentences of that model in that language, and should be treated as an additional baseline to compare with the Accuracy achieved by the system.",
            "84": "Everything is detailed in Section 4 of the paper.",
            "85": "\u2022 DeepL",
            "86": "\u2022 OPUS",
            "87": "\u2022 M2M100",
            "88": "\u2022 M2M100-LG"
        }
    },
    "6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a": {
        "doc1": {
            "0": "paper_summary:",
            "1": "This paper propose a two-stage training for the complete many-to-many Multilingual NMT, where the model is first pretrained on the complete multilingual dataset, then finetuned only with the same target language data.",
            "2": "With the proposed training schedule, author reports a significant improvement over both bilingual and pivot-based baselines.",
            "3": "summary_of_strengths:",
            "4": "1. The proposed two-stage training is novel and highly effective.",
            "5": "Demonstrated on two large-scale datasets, this approach is able to achieve significant improvement over the baselines.",
            "6": "2. Their idea is simple and clearly conveyed.",
            "7": "With the experimental success, it is well-suited for a short paper.",
            "8": "summary_of_weaknesses:",
            "9": "1. The \"baseline\" may not be appropriate.",
            "10": "Their approach should be compared with the baseline setup where N many-to-one models are trained with only their corresponding target languages.",
            "11": "The approach in the paper ends up with N models, thus it's unfair to compare against the Many-to-Many baseline (one model) nor the bilingual baseline (one dataset).",
            "12": "2. Since their approach ends up solving a Many-to-Many translation problem with N Many-to-One models, it greatly drops the deployment efficiency provided by Multilingual NMT, and scales linearly to the number of languages.",
            "13": "3. The experiment section lacks comparison to existing works, e.g. the winning system of WMT-21",
            "14": "4. paper writing needs improvement (see below)",
            "15": "comments,_suggestions_and_typos:",
            "16": "L76: incomplete sentence \"... and .\"",
            "17": "I suggest changing all XY, EX, XE notations to X-Y, En-X, X-En, since they could be confusing."
        },
        "doc2": {
            "0": "Building Multilingual Machine Translation Systems That Serve Arbitrary XY Translations",
            "1": "Abstract",
            "2": "Multilingual Neural Machine Translation (MNMT) enables one system to translate sentences from multiple source languages to multiple target languages, greatly reducing deployment costs compared with conventional bilingual systems. ",
            "3": "The MNMT training benefit, however, is often limited to many-to-one directions. ",
            "4": "The model suffers from poor performance in one-to-many and zero-shot directions. ",
            "5": "To address the issue, this paper discusses how to practically build MNMT systems that serve arbitrary XY translation directions while leveraging multilinguality with the two-stage training strategy of pretraining and finetuning. ",
            "6": "Experimenting in the WMT'21 multilingual translation task, we demonstrate that our systems outperform the conventional baselines of direct bilingual models and pivot translation models for most directions, averagely giving +6.0 and +4.1 BLEU, without the need for architecture change or extra data collection. ",
            "7": "Moreover, we also examine our proposed approach in an extremely large-scale data setting to accommodate practical deployment scenarios.",
            "8": "Introduction",
            "9": "Multilingual Neural Machine Translation (MNMT) has attracted much attention in the machine translation area, enabling one system to serve translation for multiple directions (Zoph and Knight, 2016;Firat et al., 2016). ",
            "10": "Because the multilingual capability hugely reduces the deployment cost at training and inference, the MNMT has actively been employed as a machine translation system backbone in recent years (Johnson et al., 2017;Hassan et al., 2018).",
            "11": "Most MNMT systems are trained with multiple English-centric data for both directions (e.g. for English \u2192 {French, Chinese} (EX) and {French, Chi-nese} \u2192 English (XE)). ",
            "12": "However, recent work (Gu et al., 2019;Zhang et al., 2020;Yang et al., 2021) pointed out that such MNMT systems severely face an off-target translation issue, especially in translations from a non-English language X to another non-English language Y. In Freitag and Firat (2020), the authors have extended data resources with multi-way aligned data and reported that one complete many-to-many MNMT can be fully supervised, achieving competitive translation performance for all XY directions. ",
            "13": "In our preliminary experiment, we observed that the complete manyto-many training is still as challenging as one-tomany training (Johnson et al., 2017;Wang et al., 2020). ",
            "14": "The model suffers from handling many diverse languages, in contrast to successful manyto-one translation (Johnson et al., 2017).",
            "15": "In this paper, we propose a two-stage training for complete MNMT systems that serve arbitrary XY translations by 1) pretraining a complete multilingual many-to-many model and 2) finetuning the model to effectively transferring knowledge from the complete multilingual training. ",
            "16": "Considering that MNMT is a multi-task learner of translation task with \"multiple languages\", the complete multilingual model learns more diverse and general multilingual representations. ",
            "17": "We transfer the representations to a specifically targeted task via manyto-one multilingual finetuning, and eventually build multiple many-to-one MNMT models that cover all XY directions. ",
            "18": "Experimenting on multilingual translation tasks at WMT'21, we have confirmed that our systems show substantial improvement against the conventional bilingual approaches for most directions. ",
            "19": "Besides that, we discuss our proposal in the light of feasible deployment scenarios and show that the proposed approach also works well in an extremely large-scale data settings and .",
            "20": "Two-Stage Training for MNMT Models",
            "21": "To support all possible translations with |L| languages (including English), we first train a complete MNMT system on all available parallel data for |L| \u00d7 (|L| \u2212 1) directions. ",
            "22": "We assume that  (top) and Task 2 (bottom), with the respective improvement of (Base, BaseFT, Big, BigFT) = (+3.6, +4.7, +5.0, +6.0) and (+2.0, +2.9, +9.2 +3.2, +4.1) against the bilingual baseline (\"Bi\") and the pivot translation baslines (\"Pivot\"). \"Base / Big\" denote our two settings, with the suffix \"FT\" for finetuned systems.",
            "23": "there exist data of (|L| \u2212 1) English-centric language pairs and remaining (|L|\u22121)\u00d7(|L|\u22122) 2 non-English-centric language pairs, which lets the system learn multilingual representations across all |L| languages. ",
            "24": "Usually, the volume of English-centric data is much greater than non-English-centric one. ",
            "25": "Then, we transfer the multilingual representations by finetuning the system on the training data subset for XL directions (i.e., multilingual many-to-one finetuning). ",
            "26": "This step leads the decoder towards the specifically targeted language L rather than multiple languages. ",
            "27": "As a result, we obtain |L| multilingual many-to-one systems to serve all XY translation directions. ",
            "28": "We experiment with our proposed approach in two different settings using 1) WMT'21 large-scale multilingual translation data with 487M training data, and 2) our in-house production-scale dataset with 3.7B training data.",
            "29": "WMT'21 Multilingual Translation Task",
            "30": "We experiment with two small tasks provided at WMT'21 large-scale multilingual translation task 1 . ",
            "31": "The tasks provide multilingual multi-way parallel data 2 from the Flores 101 data, with (Englishcentric, Non-English-centric)=(321M, 166M) in total. ",
            "32": "The data size per direction varies in a range 1 https://www.statmt.org/wmt21/largescale-multilingual-translation-task.html 2 The data provided among English (en), 5 Central/East European languages of {Croatian (hr), Hungarian (hu), Estonian (et), Serbian (sr), Macedonian (mk)} for the task 1, and 5 Southeast Asian languages of {Javanese (jv), Indonesian (id), Malay (ms), Tagalog (tl), Tamil (ta)} for the task 2. ",
            "33": "of 0.07M-83.9M. ",
            "34": "To balance the data distribution across languages (Kudugunta et al., 2019), we upsample the low-resource languages with tempera-ture=5. ",
            "35": "We append language ID tokens at the end of source sentences to specify a target language (Johnson et al., 2017). ",
            "36": "We tokenize the data with the SentencePiece (Kudo and Richardson, 2018) and build a shared vocabulary with 64k tokens.",
            "37": "We train Transformer models Base and Big (Vaswani et al., 2017) in a complete multilingual many-to-many fashion, respectively. ",
            "38": "The model parameters are optimized by using RAdam (Liu et al., 2020) with an initial learning rate of 0.025, and warm-up steps of 10k and 30k for the Base and Big model training, respectively. ",
            "39": "The systems are pretrained on 64 V100 GPUs with a mini-batch size of 3072 tokens and graduation accumulation of 16. ",
            "40": "After pretraining, the models are finetuned on a subset of XL training data. ",
            "41": "We tune the model parameters gently on 8 V100 GPUs with the same mini-batch size, graduation accumulations, and the same optimizer with different learning rate scheduling of (init_lr, warm-up steps)=({1e-4, 1e-5, 1e-6}, 8k). ",
            "42": "The best checkpoints are selected based on development loss. ",
            "43": "The translations are obtained by a beam search decoding with a beam size of 4, unless otherwise stated.",
            "44": "Baselines For system comparison, we build two different baselines: 1) a direct bilingual system and 2) a pivot translation system via English (only applicable for non-English XY evaluation). ",
            "45": "Both are based on the Transformer Base architecture. ",
            "46": "The embedding dimension is set to 256 for jv, ms, ta, and tl, because of the training data scarcity. ",
            "47": "For the XY pivot translation, a source sentence in language X is translated to English with a beam size of 5 by the XE model, then the best output is translated to the final target language Y by the EY model.",
            "48": "All results on the test sets are displayed in Figure 1, where we report the case-sensitive sacreBLEU score (Post, 2018) for translation accuracy. ",
            "49": "Our best systems (\"BigFT\") are significantly better by \u2265 +0.5 sacreBLEU for 83% and 88% directions against the bilingual baselines and the pivot translation baselines, respectively. ",
            "50": "However, building a larger and larger model is not always feasible. ",
            "51": "We often have limitations in the computational resources at inference time, which leads to a trade-off problem between the performance and the decoding cost caused by the model architecture.",
            "52": "In the following section, we validate our proposed approach in an extremely large-scale data setting and also discuss how we can build lighter MNMT models without the performance loss.",
            "53": "In-house Extremely Large-Scale Setting",
            "54": "We validate our proposed approach in an extremely large-scale setting, while briefly touching the following three topics of 1) multi-way multilingual data collection, 2) English-centric vs. multi-centric pretraining for better XY, and 3) a lighter MNMT model that addresses the trade-off issue between performance and latency.",
            "55": "We build an extremely large-scale data set using our in-house English-centric data set, consisting of 10 European languages, ranging 19M-187M sentences per language 3 . ",
            "56": "From these English-centric data, we extract a multi-way multilingual XY data, by aligning EX and EY data via pivoting English. ",
            "57": "Specifically, we extracted {de, fr, es, it, pl}-centric data and concatenate them to the existing direct XY data, providing 24M-192M per direction. ",
            "58": "Similarly as in Section 2.1, we build a shared SentencePiece vocabulary with 128k tokens to address the largescale setting.",
            "59": "In a large-scale data setting, a question might come up which pretrained model provides generalized multilingual representations to achieve better XY translation quality? ",
            "60": "Considering the dominant text data is usually English, e.g., 70% tasks are English-centric in the WMT'21 News translation task, the model supervised on English-centric corpora might learn representations enough to transfer for XY translations. ",
            "61": "To investigate the usefulness of the multi-centric data training, we pretrain Transformer Big models with deeper 24-12 layers on the English-centric data and the L-centric data (L={en,de,fr}), individually. ",
            "62": "After pretraining, we apply the multilingual many-to-one finetuning with a subset of the training data and evaluate each system for the fully supervised XY directions, i.e. xx-{en,de,fr}, and the partially supervised XY directions, i.e. xx-{es,it,pl}. We followed the same training and finetuning settings as described in Section 2.1, unless otherwise stated.",
            "63": "MNMT with Light Decoder At the practical level, one drawback of the large-scale models would be latency at inference time. ",
            "64": "This is mostly caused by the high computational cost in the decoder layers due to auto-regressive models and the extra cross-attention network in each block of the decoder. ",
            "65": "Recent studies (Kasai et al., 2020;Hsu et al., 2020;Li et al., 2021) have experimentally shown that models with a deep encoder and a shallow decoder can address the the issue, without losing much performance. ",
            "66": "Fortunately, such an architecture also satisfies demands of the many-to-one MNMT training, which requires the encoder networks to be more complex to handle various source languages. ",
            "67": "To examine the light MNMT model architecture, we train the Transformer Base architecture modified with 9-3 layers (E9D3) in a bilingual setting and compare it with a standard Transformer Base model, with 6-6 layers (E6D6), as a baseline. ",
            "68": "Additionally, we also report direct XY translation performance, when distilling the best large-scale models alongside the light MNMT models as a student model. ",
            "69": "More specifically, following Kim and Rush (2016), we train five light MNMT student models (E9D3) that serve many-to-L translations (L={de, fr, es,it,pl}).",
            "70": "Results Table 1 reports average sacreBLEU scores in the in-house XY test sets. ",
            "71": "For the xx-{de,fr} directions, the proposed finetuning helps both English-centric and multi-centtric pretrained models to improve the accuracy. ",
            "72": "Over- all, the finetuned multi-centric models achieved the best, largely outperforming the English pivotbased baselines by +2.6 and +2.8 points. ",
            "73": "The multicentric models surpass the corresponding finetuned English-centric systems with a large margin of +0.9 and +0.8 points. ",
            "74": "This suggests that, by pretraining a model on more multi-centric data, the model learns better multilinguality to transfer. ",
            "75": "For the xx-{es,it,pl} directions 4 , each finetuned system gains similar accuracy improvement, significantly outperforming the conventional baselines.",
            "76": "Figure 2 shows the effectiveness of our light NMT model architecture for 5 EX directions, reporting the translation performance in sacreBLEU scores and the latency measured on CPUs. ",
            "77": "Our light NMT model (E9D3) successfully achieves almost 2x speed up, without much drop of the performance for all directions. ",
            "78": "Employing this light model architecture as a student model, we report the distilled many-to-one model performance in Table 1, measured by sacreBLEU and COMET (Rei et al., 2020)  that are distilled from the bilingual Teachers then obtained the English pivot-based translation performance. ",
            "79": "For all xx-{de,fr,es,it,pl} directions, our proposed models show the best performance in both metrics. ",
            "80": "We also note that, at inference time, our direct XY systems save the decoding cost with 75% against the pivot translation.",
            "81": "Conclusion",
            "82": "This paper proposes a simple but effective twostage training strategy for MNMT systems that serve arbitrary XY translations. ",
            "83": "To support translations across languages, we first pretrain a complete multilingual many-to-many model, then transfer the representations via finetuning the model in a many-to-one multilingual fashion. ",
            "84": "In the WMT'21 translation task, we experimentally showed that the proposed approach substantially improve translation accuracy for most XY directions against the strong conventional baselines of bilingual systems and pivot translation systems. ",
            "85": "We also examined the proposed approach in the extremely large-scale setting, while addressing the practical questions such as multi-way parallel data collection, the usefulness of multilinguality during the pretraining and finetuning, and how to save the decoding cost, achieving the better XY quality.",
            "86": "Barret Zoph and Kevin Knight. 2016. ",
            "87": "Multi-source neural translation. ",
            "88": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 30-34, San Diego, California. ",
            "89": "Association for Computational Linguistics."
        }
    },
    "dd0a83852cb35c8d0ac0c40da6b7ebeb385f9c7e4816ef51f43abd6910a9adacc0daf460dfda4e56f37fa542a749961a78ddbbe74af0326f41e5cd38e867b121": {
        "doc1": {
            "0": "paper_summary:",
            "1": "The purpose of this paper is to build a system to detect negation cues and negation focus by gathering additional data and utilizing negation masking strategies.",
            "2": "Results indicate some improvement as well as some transferability of negation across domains.",
            "3": "summary_of_strengths:",
            "4": "In order to build AugNB, the authors collected a large number of negation sentences.",
            "5": "This large corpus might be useful for future research. ",
            "6": "Using their proposed models (e.g., AugNB and CueNB), the authors conduct extensive experiments on a variety of datasets. ",
            "7": "Based on the reported results, detecting negation cues and detecting focus of negation appear promising for most of the datasets.",
            "8": "summary_of_weaknesses:",
            "9": "While the results are promising, the innovation of this paper is somewhat limited as the approaches are quite well known.",
            "10": "The paper indicates that only gold cue information is used for scope resolution (I. 219).",
            "11": "Now, the question is what happens if you use predicted cues?",
            "12": "Is it the case that a few wrong cues can give devastating results on focus detection?",
            "13": "More details should be there for the approach \"negation-focused data collection\" approach.",
            "14": "It seems additional cues are collected only from a biomedical domain.",
            "15": "Therefore, the model AugNB can be biased to perform better on the biomedical datasets only.",
            "16": "Further, there should be more information on CueNB model as well.",
            "17": "How is the masking objective utilized on top of AugNB?",
            "18": "comments,_suggestions_and_typos:",
            "19": "Numbers in Tables 2 and 3 look more crowded. I would round to one decimal point in these tables."
        },
        "doc2": {
            "0": "Improving negation detection with negation-focused pre-training",
            "1": "Abstract",
            "2": "Negation is a common linguistic feature that is crucial in many language understanding tasks, yet it remains a hard problem due to diversity in its expression in different types of text. ",
            "3": "Recent works show that state-of-the-art NLP models underperform on samples containing negation in various tasks, and that negation detection models do not transfer well across domains. ",
            "4": "We propose a new negation-focused pre-training strategy, involving targeted data augmentation and negation masking, to better incorporate negation information into language models. ",
            "5": "Extensive experiments on common benchmarks show that our proposed approach improves negation detection performance and generalizability over the strong baseline Neg-BERT (Khandelwal and Sawant, 2020).",
            "6": "Introduction",
            "7": "Negation is an important linguistic phenomenon that appears commonly in natural language but is underrepresented in common NLP benchmarks (Hossain et al., 2020). ",
            "8": "Furthermore, the Checklist benchmark (Ribeiro et al., 2020) shows that most sentiment analyzers and machine comprehension models struggle with samples containing negation. ",
            "9": "Negation is even more important in biomedical domain text, where patients are carefully defined as having/not having specific characteristics. ",
            "10": "Even within the biomedical domain, there are many types of text such as clinical notes, lab reports, or research publications, each with particular characteristics in relation to the use of negation. ",
            "11": "A recent work on negation detection in English texts found that negation detection models do not transfer well across domains, due to variations in expression of negation (Khandelwal and Sawant, 2020). ",
            "12": "It remains a challenge to solve negation in general, even with state-of-the-art NLP models.",
            "13": "Negation detection is typically defined as consisting of the two sub-tasks of: (1) cue detection, detecting the cue phrase that triggers the negation; and (2) scope resolution, determining the affected spans that are negated. ",
            "14": "Three primary datasets are used to evaluate negation; the BioScope corpus (Vincze et al., 2008) includes full papers and abstracts of biological papers, the SFU corpus (Konstantinova et al., 2012) is a collection of product reviews, and the Sherlock dataset (Morante and Blanco, 2012) consists of short literary works. ",
            "15": "There are differences in annotation schemes across the datasets, such as whether or not the cues are included inside scope annotation, and sub-optimal cross-dataset results have been observed, providing clear indications that the datasets are highly divergent in language use and negation types.",
            "16": "In this work, we aim to extend the transfer learning capability of NegBERT (Khandelwal and Sawant, 2020) through additional pre-training with task-related augmented training data, and a new masking objective. ",
            "17": "Our contributions are:",
            "18": "\u2022 We introduce an approach to augmenting data to emphasize negation in pre-training. ",
            "19": "\u2022 We propose a novel extension to the standard random masked language model objective in pre-training to explicitly mask negation cues, to make the models more robust to negation. ",
            "20": "\u2022 We conduct extensive experiments on different benchmarks to evaluate cross-domain performance of large pre-trained language models as well as the effectiveness of the proposed pre-training strategies; code available at http://ANONYMISED.",
            "21": "Related work",
            "22": "To date, negation detection has been heavily-reliant on rule-based systems. ",
            "23": "Chapman et al. (2001) proposed a simple system, NegEx, based on regular expressions to detect negation cues in a sentence given a concept of interest (the scope). ",
            "24": "NegEx remains the most popular approach to negation detection, especially in the clinical domain where target clinical concepts can be detected (e.g., with MetaMap (Aronson and Lang, 2010)). ",
            "25": "Further research has extended NegEx with syntactic information (Mehrabi et al., 2015;Peng et al., 2018), and shown that rule-based systems can achieve relatively good performance for detecting negation, especially in the biomedical domain, but do not generalize well to other domains or datasets.",
            "26": "To approach negation cue and negation scope detection with supervised machine learning, two classification tasks are defined: finding negation tokens and classifying tokens as the first or last (or neither) token within the scope of negation. ",
            "27": "Most works follow a common scheme in extracting various features from the sentence, in using a classifier to classify each token as the beginning, inside, or outside of a negation cue or scope span (Morante and Daelemans, 2009;Ou and Patrick, 2015;Cruz et al., 2016). ",
            "28": "Recently, research has shifted to applying deep learning methods to the task. ",
            "29": "Most approaches make use of RNN-based architectures to encode the input sentences, combined with a softmax layer for classification (Lazib et al., 2019;Chen, 2019). ",
            "30": "Despite the high performance on common benchmarks, results are biased by the fact that negation scope is often delimited by punctuation and other dataset artefacts (Fancellu et al., 2017). ",
            "31": "As such, they are potentially only learning domain-specific surface features rather than capturing the true semantics of negation. ",
            "32": "NegBERT applies a large pre-trained language model to the problem of negation detection, outperforming previous deep learning methods on negation detection, with especially high gains on scope resolution.",
            "33": "Method",
            "34": "Our proposed pre-training strategy consists of two main components: (1) negation-focused data collection in which we first collect relevant data that contain negation; and (2) negation-focused pretraining that makes use of the negation-focused data to emphasize negation instances, and adopts a novel negation-specific masking strategy.",
            "35": "Negation-focused data collection",
            "36": "We aim to construct a dataset that is enriched for negation, to support negation-sensitized pretraining of large language models. ",
            "37": "To obtain sentences with negations, we extend the NegEx lexicon with additional negation cues obtained from biomedical texts (Morante, 2010), and apply it to sentences extracted from a corpus using the SpaCy English sentence tokenizer, keeping only those sentences with at least one identified negation.",
            "38": "For the biomedical domain, we use texts in the TREC-CDS 2021 snapshot 1 of the clinical trials registry. ",
            "39": "2 Clinical trials are documents describing the protocols and relevant patient characteristics of a clinical research study. ",
            "40": "Description of clinical trials can be quite long, but a core aspect of the trial description is the patient inclusion/exclusion criteria, specifying what types of characteristics or conditions a patient must have/not have in order to be suitable for the trial. ",
            "41": "The reasons for choosing this data are that: (1) it is in-domain for the biomedical domain; ",
            "42": "(2) the texts are well-formed sentences with proper grammatical structure; and",
            "43": "(3) the texts contain many negations, especially in the inclusion/exclusion criteria sections. ",
            "44": "For the general domain, we apply this approach to wikitext (Merity et al., 2016), a set of verified articles in Wikipedia. ",
            "45": "We sample the data equally from these two sets, obtaining 1, 381, 948 negation sentences.",
            "46": "Negation-focused pre-training",
            "47": "Adaptive pre-training on target domain data has been shown to be an effective strategy for domain adaptation (Gururangan et al., 2020). ",
            "48": "We therefore hypothesize that pre-training language models on text with negations will help the model incorporate information about negation, and learn better representation for sentences containing negation. ",
            "49": "Using the negation-focused data, we first apply the standard random word masking strategy (Devlin et al., 2019) and train the model with the masked language model objective.",
            "50": "As part of the collection of the negation-focused data, we obtain predictions of negation cues in all the sentences, which can be explicitly incorporated to make the model more robust to negation. ",
            "51": "Inspired by various works on entity and span masking (Joshi et al., 2020;Yamada et al., 2020), we explore explicitly incorporating information about negation cues into the model by masking these cues, and targeting prediction of the masked cue in the pre-training stage. ",
            "52": "Below is an example of how a sentence is tokenized under our masking scheme: No serious complications such as hypertension, diabetes. ",
            "53": "\u21d2 [CUE] serious complications such as [MASK], diabetes.",
            "54": "A new type of token [CUE] is introduced under this masking scheme, and the model needs to reconstruct the original sentence by predicting both the [CUE] token to be No, and the randomlymasked token [MASK] to be hypertension. ",
            "55": "By always masking negation cues in all the sentences, we force the model to focus more on this type of token, and thus, aim to learn better embeddings incorporating information of how a negation cue is represented in the context of the sentence. ",
            "56": "Moreover, by using a different token to mask negation cues, we ensure that the model learns to distinguish between different types of tokens. ",
            "57": "In this work, we replace the BERT encoder of NegBERT with RoBERTa  and apply whole-word masking, meaning that all the sub-word tokens that constitute a word will be masked.",
            "58": "Experiments",
            "59": "Experimental settings",
            "60": "Following the experiment settings in NegBERT, we use the three standard benchmarks for negation cue detection and scope resolution tasks, i.e. BioScope (Vincze et al., 2008) (separated into two subsets, sourced from abstracts and full-text papers, resp.), the SFU product reviews dataset (Konstantinova et al., 2012), and the Sherlock dataset (Morante and Blanco, 2012). ",
            "61": "In addition, we use the negationannotated subset of VetCompass UK 3 (Cheng et al., 2017), consisting of clinical notes in the veterinary domain, which are very informal compared to BioScope. ",
            "62": "It also contains abbreviations and shortening of terms, as well as certain unique negation cues. ",
            "63": "To investigate the cross-domain performance, we perform cue detection and scope resolution for all 4 datasets, based on training on one dataset and evaluating on all datasets. ",
            "64": "Detailed statistics of these datasets are presented in Table 1.",
            "65": "We formulate the two tasks as sequence labeling problems, where each token is tagged with a corresponding label. ",
            "66": "For cue detection, we use the annotation scheme {0: Affix, 1: Normal Cue, 2: Part of multiword cue, 3: Not part of cue}. For scope resolution, we use gold cue information and two labels {0: Outside negation scope, 1: Part of negation scope}. We adopt the same hyperparameters as NegBERT. ",
            "67": "Following the standard evaluation scheme in previous negation detection works, all systems are evaluated using token-level F 1 -score, based on whether it is inside or outside of any",
            "68": "Main results",
            "69": "Tables 2 and 3 report the performance of negation cue detection and negation scope resolution, respectively. ",
            "70": "Results reported are the average of 5 runs with different random seeds. ",
            "71": "NegBERT results are produced using the official implementation. ",
            "72": "4 We observe similar trends across all datasets for both cue detection and scope resolution. ",
            "73": "Regarding the in-dataset setting (training and evaluating on the same dataset), AugNB outperforms the baseline NegBERT on all datasets except for Sherlock. ",
            "74": "Gains are more noticeable in the biomedical datasets (BioScope, VetCompass). ",
            "75": "For Sherlock, however, we observe a slight degradation in performance with the proposed pre-training scheme. ",
            "76": "This is likely due to the fact that Sherlock has major differences in annotation scheme compared to other corpora, specifically including scopes to the left of cues, while in BioScope and SFU, the scope is usually annotated only to the right of cues. ",
            "77": "Also, the cue itself is not considered to be part of the scope in Sherlock or SFU, unlike in BioScope.",
            "78": "In the cross-dataset setting, we record gains across all benchmarks. ",
            "79": "The largest cross-dataset improvements over NegBERT are for SFU, perhaps due to SFU being the largest dataset in size, containing a relatively large number of unique cues. ",
            "80": "CueNB further improves the performance of AugNB, confirming our hypothesis that explicitly masking the cue will help the model learn better representations for negation cues and thus, better distinguish between cues and normal words. ",
            "81": "These results show that our negation-focused pre-training strategy is effective for improving the transfer learning performance of pre-trained language models on the negation detection task.",
            "82": "Discussion",
            "83": "We training strategy. ",
            "84": "Table 4 presents the results of different variations of the proposed pre-training scheme on the BioScope-Abstract validation split. ",
            "85": "We consider two variations, pre-training with: (1) only the negation-focused data (equivalent to the AugNB model); and (2) only the cue masking objective. ",
            "86": "To model the latter variation, we explicitly mask the cue in the BioScope training set, then pretrain on this training set. ",
            "87": "From the results, we see that both strategies help improve the baseline Neg-BERT on cue detection and scope resolution, with explicitly masking the cues being the most important. ",
            "88": "Combining both strategies (CueNB) further improves the overall results.",
            "89": "Conclusion",
            "90": "In this work, we propose a new negation-focused pre-training strategy to explicitly incorporate negation information into pre-trained language models. ",
            "91": "Empirical results on common benchmarks show that the proposed strategy helps improve the performance of pre-trained language models on the negation detection task when evaluating on the same source dataset, as well as their transferability to target data in different domains. ",
            "92": "Despite the gains over previous methods, the sub-optimal results on some benchmarks show that negation remains a big challenge in NLP."
        }
    },
    "04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5": {
        "doc1": {
            "0": "paper_summary:",
            "1": "This paper proposes NoisyTune, which is a matrix-wise perturbing method by adding different uniform noises according to the standard deviations of different parameter matrices.",
            "2": "summary_of_strengths:",
            "3": "A simple method",
            "4": "summary_of_weaknesses:",
            "5": "There are some questions:",
            "6": "Q1. Even simple, I doubt its effectiveness.",
            "7": "The experimental results of both GLUE and XTREAMRE show that the improvements are range from 0.2 to 0.8 (Avg score), which seems not significant.",
            "8": "Besides, please report the standar\u00ad\u00add deviations of 5 times runs in Table 1 and Table 2.",
            "9": "Q2. The over-fitting problem is more serious in large models, so the experiment of applying NoisyTune on larger PLMs will be more convincing.",
            "10": "Q3. The right Fig of Fig3 makes me confusing, which is not consistent with the claim \u201cthe relative change of L1-norms becomes smaller when NoisyTune is applied\u201d.",
            "11": "Q4. Missing the most relevant work, \u00ad\u00ad\u00adraise a child in Large Language Model: Towards Effective and Generalizable Fine-tuning. Xu et al., EMNLP 2021. It masks out the gradients of the parameters during the backward process.",
            "12": "The difference between NoisyTune and above-mentioned paper is using masking or noisy to perturb parameters.",
            "13": "Thus I am very interested to see which method (masking or noisy) will have better benefits.",
            "14": "comments,_suggestions_and_typos:",
            "15": "See above"
        },
        "doc2": {
            "0": "NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better",
            "1": "Abstract",
            "2": "Effectively finetuning pretrained language models (PLMs) is critical for their success in downstream tasks. ",
            "3": "However, PLMs may have risks in overfitting pretraining signals, and there are some gaps between downstream tasks and the pretraining tasks. ",
            "4": "It can be difficult for vanilla finetuning methods to overcome the barrier between pretraining and downstream tasks, which leads to suboptimal performance. ",
            "5": "In this paper, we propose a very simple yet effective method named NoisyTune which can help better finetune PLMs in downstream tasks by adding some noise to the parameters of PLMs before finetuning. ",
            "6": "More specifically, we propose a matrix-wise perturbing method by adding different uniform noises according to the standard deviations of different parameter matrices, which can consider the varied characteristics of different types of parameters in PLMs. ",
            "7": "Extensive experiments on the GLUE English benchmark and the XTREME multilingual benchmark show that NoisyTune can consistently improve the performance of different PLMs in many downstream tasks.",
            "8": "Introduction",
            "9": "In recent years, pretrained language models (PLMs) have achieved huge success in NLP (Qiu et al., 2020). ",
            "10": "Many PLMs such as BERT (Devlin et al., 2019) and RoBERTa  have played critical roles in various applications, such as reading comprehension, machine translation and text classification (Dong et al., 2019). ",
            "11": "In addition, crosslingual pretrained language models like XLM (Conneau and Lample, 2019) and Unicoder (Huang et al., 2019) can even generate cross-lingually transferable language representations to universally handle downstream tasks in different languages.",
            "12": "How to effectively finetune PLMs to better empower downstream tasks is an important research problem (Zheng et al., 2021a). ",
            "13": "Besides naively finetuning PLMs with labeled data in downstream tasks, many works explore more effective and robust PLM finetuning methods Jiang et al., 2020;Lee et al., 2020;Aghajanyan et al., 2021;Xu et al., 2021). ",
            "14": "For example,  proposed a RecAdam approach that adds a penalty item to minimize the L 2 distance between fine-tuned models and the pretrained models, where the pernalty intensity is timevariant during finetuning. ",
            "15": "Lee et al. (2020) proposed a Mixout method to randomly replace parts of model parameters with their original pretrained weights. ",
            "16": "These finetuning methods mainly focus on preventing PLMs from overfitting limited labeled data in downstream tasks. ",
            "17": "However, PLMs have been well trained in the self-supervised pretraining tasks, and it can be difficult for them to overcome the barrier between pretraining and downstream tasks as well as the gaps of their domains during finetuning (Roberts et al., 2020), which may lead to a suboptimal performance especially when labeled data in downstream tasks is insufficient.",
            "18": "In this paper, we propose a very simple yet effective method named NoisyTune, which can help better finetune PLMs for downstream tasks. ",
            "19": "The key idea of NoisyTune is to add a little noise to perturb PLMs parameters before finetuning, which can help prevent them from overfitting the signals in the pretraining tasks, and reduce the gap between pretraining and downstreaming tasks. ",
            "20": "Since different types of parameters in PLMs may have different characteristics, we propose a matrix-wise perturbing method that adds uniform noise with different intensities according to the standard deviations of different parameter matrices for better adaptation. ",
            "21": "We conduct experiments on two widely used NLP benchmarks, i.e., GLUE (Wang et al., 2018) for English language understanding and XTREME (Hu et al., 2020) for multilingual language understanding. ",
            "22": "The results show that NoisyTune can consistently boost the performance of different PLMs in many downstream NLP tasks.",
            "23": "NoisyTune",
            "24": "In this section, we introduce our proposed Noisy-Tune approach that adds noise to perturb PLMs for more effective finetuning. ",
            "25": "Since the parameters of PLMs are well tuned in the pretraining tasks and may overfit self-supervision signals, it may be difficult for them to adapt to downstream tasks especially when labeled data in downstream tasks are rather limited. ",
            "26": "Motivated by the dueling bandits mechanism (Yue and Joachims, 2009) that adds randomness to the model for exploration, we explore adding noise to PLMs before finetuning to \"explore\" other parameter spaces to reduce the problem of overfitting pretraining tasks. ",
            "27": "We denote the parameter matrices (or scalars/vectors) in a PLM ",
            "28": "as",
            "29": "where N is the number of parameter types. ",
            "30": "In many noisy training methods, the noise added to the parameters obeys the same distribution (LeCun et al., 2015). ",
            "31": "However, different parameter matrices in the PLM have very different characteristics. ",
            "32": "For example, the self-attention parameters and the feed-forward network parameters usually have very different properties . ",
            "33": "Thus, adding global noise may not be optimal for keeping good model utility. ",
            "34": "To solve this challenge, we propose a matrix-wise perturbing method that adds noise with different intensities according to the variance of parameter matrices. ",
            "35": "We denote the perturbed version of the parameter matrix W i as Wi , which is computed as follows:",
            "36": "where std stands for standard deviation, the function U (a, b) means uniform distribution noise ranged from a to b, and \u03bb is a hyperparameter that controls the relative noise intensity. ",
            "37": "1 In this way, parameters with higher variance will be added with stronger noise. ",
            "38": "In addition, in some PLMs there exist constant matrices, such as token type embeddings in RoBERTa . ",
            "39": "They will not be perturbed because their standard deviation is 0. ",
            "40": "This will ensure that these constant matrices will not be accidentally activated by additional noise.",
            "41": "3 Experiments",
            "42": "Datasets and Experimental Settings",
            "43": "We conduct extensive experiments on two widely used benchmarks for PLM evaluation. ",
            "44": "The first one is GLUE (Wang et al., 2018), which is a benchmark for English language understanding that contains tasks like natural language inference, sentiment analysis and sentence similarity evaluation.",
            "45": "The second one is XTREME (Hu et al., 2020), which is a benchmark for multilingual language understanding. ",
            "46": "It covers 40 languages and contains four groups of tasks, including sentence classification, structured prediction, sentence retrieval and question answering. ",
            "47": "More details of these benchmarks can refer to their original papers and official websites. ",
            "48": "Since the test labels of GLUE are not released, following (Bao et al., 2020) we report results on the dev set of GLUE. ",
            "49": "The XTREME results are evaluated on the test set. ",
            "50": "The hyperparameter \u03bb is 0.15 on GLUE and is 0.1 on XTREME.",
            "51": "The detailed hyperparameter settings are in the appendix. ",
            "52": "Following (Zheng et al., 2021b), in sentence retrieval tasks we first train the models on the XNLI dataset, and then use the average of token representations produced by the hidden layer that yields the best performance. ",
            "53": "In order not to harm the alignment of token embeddings across different languages, We do not add noise to the token embeddings in multilingual PLMs. ",
            "54": "We repeat experiments 5 times with different random seeds and report the average scores.",
            "55": "Performance Evaluation",
            "56": "On the GLUE benchmark, we compare directly finetuning the base version of BERT (Devlin et al., 2019), XLNET , RoBERTa  and ELECTRA (Clark et al., 2020) as well as finetuning them after applying NoisyTune.",
            "57": "On the XTREME benchmark, we compare both base and large versions of XLM-R (Conneau et al., 2020) and their variants processed by NoisyTune.",
            "58": "The results on the two benchmarks are shown in Tables 1 and 2, respectively. ",
            "59": "On the XTREME datasets, we report two types of results, i.e., zeroshot crosslingual transfer from English to other languages or learning models on both English and translated data. ",
            "60": "From the results, we can see that NoisyTune can consistently improve the performance of different PLMs on different tasks. ",
            "61": "In addition, the performance improvement on relatively small datasets is usually larger (e.g., RTE, CoLA and WNLI). ",
            "62": "This indicates that when labeled training data in downstream tasks is not redundant, it may be more difficult to well adapt PLMs to downstream tasks from the parameter space well tuned in  pretraining tasks. ",
            "63": "Thus, properly perturbing PLMs with noise can explore different parameter spaces and meanwhile keep useful knowledge encoded in pretraining tasks.",
            "64": "Influence of Noise Type",
            "65": "Next, we study the influence of using different kinds of noise on NoisyTune. ",
            "66": "We compare five methods, including (1) basic method without noise;",
            "67": "(2) Gaussian noise with a global distribution; (3) uniform noise with a global distribution; (4) matrixwise Gaussian noise; (5) matrix-wise uniform noise. ",
            "68": "The results on GLUE are shown in Fig. 1. ",
            "69": "We find that adding global noise with same distributions to the PLM parameters will harm the model perfor- mance. ",
            "70": "This is because different parameter matrices have very different distributions, and simply adding global noise is not appropriate. ",
            "71": "In addition, we find an interesting phenomenon that adding uniform noise is better than using Gaussian noise. ",
            "72": "This may be because Gaussian noise has wider ranges and some outliers may affect model performance. ",
            "73": "Thus, we prefer using matrix-wise uniform noise in our NoisyTune method.",
            "74": "Analysis on NoiseTune",
            "75": "We then analyze the influence of NoisyTune on finetuning. ",
            "76": "We show the accuracy of the BERT model with or without NoisyTune on the MRPC dataset in  Fig. 2. ",
            "77": "2 The interval between two adjacent checkpoints is 50 iterations. ",
            "78": "We find NoisyTune can consistently improve PLMs at different finetuning steps. ",
            "79": "This may be because the perturbed PLMs may have lower risks in overfitting pretraining tasks and have better generalization abilities.",
            "80": "To further study the impact of NoisyTune on model finetuning, we show the relative changes of the L1-norms of different kinds of paramters in the BERT model during training on MRPC and STS-B in Fig. 3. ",
            "81": "3 Since the noise we added is zeromean, the absolute parameter L1-norms will not be changed too much. ",
            "82": "However, we can see that the relative change of L1-norms becomes smaller when NoisyTune is applied, which indicates that the model uses smaller paces towards convergence. ",
            "83": "This means that directly finetuning PLMs may need more updates to adapt to downstream tasks, which may be due to the overfitting of pretraining tasks and their gaps with downstream tasks. ",
            "84": "Our Noisy-Tune approach provides a simple way to mitigate this problem to empower PLM finetuning.",
            "85": "Empower Other Finetuning Methods",
            "86": "Our NoisyTune method also has the potential to empower other PLM finetuning techniques. ",
            "87": "We compare the performance of the original RecAdam  and Mixout (Lee et al., 2020) method and their variants combined with NoisyTune. ",
            "88": "The results are shown in Fig. 4. ",
            "89": "We find that combining NoisyTune with existing PLM finetuning techniques can further improve the performance. ",
            "90": "This is because NoisyTune aims to address the overfitting of pretraining signals while these methods aim to prevent overfitting in downstream tasks, thereby they can be empowered by NoisyTune to improve model performance.",
            "91": "Conclusion",
            "92": "In this paper, we propose a very simple but effective method named NoisyTune, which adds a little noise to PLMs before finetuning for better transferability from pretraining tasks to downstream tasks. ",
            "93": "In NoisyTune, we propose a matrix-wise perturbing method that adds noise with different intensities according to the variance of different parameter matrices in PLMs, which can consider the varied characteristics of different types of parameters. ",
            "94": "Extensive experiments on the monolingual GLUE benchmark and the multilingual XTREME benchmark demonstrate the NoisyTune can consistently improve the performance of different PLMs in various downstream tasks."
        }
    },
    "88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148": {
        "doc1": {
            "0": "paper_summary:",
            "1": "This paper presents an interesting finding, i.e., fine-tuning only the bias terms of pre-trained language models is competitive with fine-tuning the entire model.",
            "2": "The authors compared the proposed method Bias-terms Fine-tuning (BitFit) with other parameter-efficient fine-tuning methods (e.g., Adapters, Diff-Pruning).",
            "3": "The experimental results on GLUE benchmark show that BitFit can achieve strong performance with less trainable parameters.",
            "4": "summary_of_strengths:",
            "5": "- The paper is well written and easy to understand.",
            "6": "- The proposed method (BitFit) is neat and novel.",
            "7": "- The authors show strong empirical results on GLUE benchmark.",
            "8": "summary_of_weaknesses:",
            "9": "I do not have any concerns about this paper.",
            "10": "comments,_suggestions_and_typos:",
            "11": "It would be helpful to compare BitFit with Adapter and Diff-Pruning base on other language models (e.g.,RoBERTa, T5).",
            "12": "But current version is good enough for a short paper."
        },
        "doc2": {
            "0": "BitFit: ",
            "1": "Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
            "2": "Abstract",
            "3": "We show that with small-to-medium training data, fine-tuning only the bias terms (or a subset of the bias terms) of pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. ",
            "4": "For larger data, bias-only fine-tuning is competitive with other sparse fine-tuning methods. ",
            "5": "Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.",
            "6": "Introduction",
            "7": "Large pre-trained transformer based language models, and in particular bidirectional masked language models from the BERT family (Devlin et al., 2018;, are responsible for significant gains in many NLP tasks. ",
            "8": "Under the common paradigm, the model is pre-trained on large, annotated corpora with the LM objective, and then finetuned on task-specific supervised data. ",
            "9": "The large size of these models make them expensive to train and, more importantly, expensive to deploy. ",
            "10": "This, along with theoretical questions on the extent to which finetuning must change the original model, has led researchers to consider finetuning variants where one identifies a small subset of the model parameters which need to be changed for good performance in end-tasks, while keeping all others intact ( \u00a72).",
            "11": "We present a simple and effective approach to fine tuning ( \u00a73), which has the following benefits:",
            "12": "1. ",
            "13": "Changing very few parameters per fine-tuned task. ",
            "14": "2. ",
            "15": "Changing the same set of parameters for every tasks (task-invariance). ",
            "16": "3. ",
            "17": "The changed parameters are both isolated and localized across the entire parameter space.",
            "18": "4. ",
            "19": "For small to medium training data, changing only these parameters reaches the same task accuracy as full fine-tuning, and sometimes even improves results.",
            "20": "Specifically, we show that freezing most of the network and fine-tuning only the bias-terms is surprisingly effective. ",
            "21": "Moreover, if we allow the tasks to suffer a small degradation in performance, we can fine-tune only two bias components (the \"query\" and \"middle-of-MLP\" bias terms), amounting to half of the bias parameters in the model, and only 0.04% of all model parameters. ",
            "22": "This result has a large practical utility in deploying multi-task fine-tuned models in memoryconstrained environments, as well as opens the way to trainable hardware implementations in which most of the parameters are fixed.",
            "23": "2 Background: fine-tuning and parameter-efficient fine-tuning  2020) (\"Diff-Pruning\"), achieves the same goal by adding a sparse, task-specific difference-vector to the original parameters, which remain fixed and are shared between tasks. ",
            "24": "The difference-vector is regularized to be sparse. ",
            "25": "Both methods allow adding only a small number of trainable parameters per-task (criteria ii), and each task can be added without revisiting previous ones (criteria iii). ",
            "26": "They also partially fulfill criteria (i), suffering only a small drop in performance compared to full fine-tuning. ",
            "27": "The Adapter method, but not the Diff-Pruning method, also supports criteria (iv). ",
            "28": "However, Diff-Pruning is more parameter efficient than the Adapter method, and also achieves better task scores. ",
            "29": "We compare against Diff-Pruning and Adapters in the experiments section, and show that we perform favorably on many tasks while also satisfying criteria (iv).",
            "30": "3 Bias-terms Fine-tuning (BitFit)",
            "31": "We propose a method we call BitFit (BIas-Term FIne-Tuning), in which we freeze most of the transformer-encoder parameters, and train only the bias-terms and the task-specific classification layer.",
            "32": "The approach is parameter-efficient: each new task requires storing only the bias terms parameter vectors (which amount to less than 0.1% of the total number of parameters), and the task-specific final linear classifier layer.",
            "33": "Concretely, the BERT encoder is composed of L layers, where each layer starts with M selfattention heads, where a self attention head (m, ) has key, query and value encoders, each taking the form of a linear layer:",
            "34": "Where x is the output of the former encoder layer (for the first encoder layer x is the output of the embedding layer). ",
            "35": "These are then combined using an attention mechanism that does not involve new parameters:",
            "36": "and then fed to an MLP with layer-norm (LN):",
            "37": "The collection of all matrices W The bias terms are additive, and correspond to a very small fraction of the network, in BERT BASE and BERT LARGE bias parameters make up 0.09% and 0.08% of the total number of parameters in each model, respectively.",
            "38": "We show that by freezing all the parameters W (\u2022) and g (\u2022) and fine-tuning only the additive bias terms b (\u2022) , we achieve transfer learning performance which is comparable (and sometimes better!) than fine-tuning of the entire network.",
            "39": "We also show that we can fine-tune only a subset of the bias parameters, namely those associated with the query and the second MLP layer (only b",
            "40": "m 2 ), and still achieve accuracies that rival full-model fine-tuning.",
            "41": "Experiments and Results",
            "42": "Datasets. ",
            "43": "We evaluate BitFit on the GLUE benchmark (Wang et al., 2018). ",
            "44": "2 Consistent with previous work (Houlsby et al., 2019;Guo et al., 2020) we exclude the WNLI task, on which BERT models do not outperform the majority baseline. ",
            "45": "Models and Optimization. ",
            "46": "We use the publicly available pre-trained BERT BASE , BERT LARGE (Devlin et al., 2018) and RoBERTa BASE  models, using the HuggingFace interface and implementation. ",
            "47": "Appendix \u00a7A.2 lists optimization details.",
            "48": "Comparison to Diff-Pruning and Adapters (Table 1) In the first experiment, we compare Bit-Fit to Diff-Pruning method and Adapters method, when using a fewer number of parameters. ",
            "49": "(2019) (respectively), on their least-parameters setting. ",
            "50": "This experiment used the BERT LARGE model. ",
            "51": "On validation set, BitFit outperforms Diff-Pruning on 5 out of 9 tasks, and underperforms in 3, while using fewer trainable parameters 3 . ",
            "52": "Test-set results are less conclusive (only one clear win compared to Diff-Pruning and 3 clear wins compared to Adapters), though BitFit is still competitive with both Diff-Pruning and Adapters. ",
            "53": "Different Base-models (Table 2) We repeat the BERT LARGE results on different base-models (the smaller BERT BASE and the better performing RoBERTa BASE ). ",
            "54": "The results in Table 2 show that the trends remain consistent.",
            "55": "Are bias parameters special? are the bias parameters special, or will any random subset do? We sampled the same amount of parameters as in BitFit from the entire model, and fine-tuned only them (\"rand 100k\" line in Table 3). ",
            "56": "The result are substantially worse across all tasks.",
            "57": "Fewer bias parameters (Table 3) Can we finetune on only a subset of the bias-parameter?",
            "58": "We define the amount of change in a bias vector b to be",
            "59": "that is, the average absolute change, across its dimensions, between the initial LM values b 0 and its fine-tuned values b F . ",
            "60": "Figure 1 shows the change per bias term and layer, for the RTE task (other tasks look very similar, see Appendix \u00a7A.4). ",
            "61": "The 'key' bias b k has zero change, consistent with the theoretical observation in (Cordonnier et al., 2020). ",
            "62": "In contrast, b q , the bias of the queries, and b m2 , the bias of the intermediate MLP layers (which take the input from 768-dims to 3072), change the most. ",
            "63": "all BERT BASE layers (\"Frozen\" row) yields much worse results. ",
            "64": "Generalization gap. ",
            "65": "When considering the generalization gap, we see that it is substantially smaller for the BitFit models: while for full fine-tuning the train set accuracy reaches nearly 100%, in the bias-only fine-tuned models the difference between the train and test set performance is often less than 2%.",
            "66": "Token-level tasks. ",
            "67": "The GLUE tasks are all sentence level. ",
            "68": "We also experimented with token-level PTB POS-tagging. ",
            "69": "Full-FT results for BERT BASE , BERT LARGE and RoBERTa BASE are 97.2, 97.4, 97.2, while BitFit results are 97.2, 97.4, 97.1.",
            "70": "Size of training data. ",
            "71": "The GLUE results suggest a reverse correlation between BitFit ability to reach Full-FT performance, and training set size. ",
            "72": "To test this (and to validate another token-level task), we train on increasing-sized subsets of SQuAD v1.0 (Rajpurkar et al., 2016). ",
            "73": "The results on Figure 2 show a clear trend: BitFit dominates over Full-FT in the smaller-data regime, while the trend is reversed when more training data is available. ",
            "74": "We conclude that BitFit is a worthwhile targetted finetuning method in small-to-medium data regimes.",
            "75": "Related Work",
            "76": "Bias terms and their importance are rarely discussed in the literature 4 . ",
            "77": "Zhao et al. (2020) describe a masking-based fine-tuning method, and explicitly mention ignoring the bias terms, as handling them \"did not observe a positive effect on performance\". ",
            "78": "An exception is the work of Wang et al. ( 2019) who analyzed bias terms from the perspective of attribution method. ",
            "79": "They demonstrate that the last layer bias values are responsible for the predicted class, and propose a way to back-propagate their importance. ",
            "80": "Michel and Neubig (2018) finetuned the biases of the output softmax in an NMT systems, to personalize the output vocabulary. ",
            "81": "Finally, Cai et al. (2020) demonstrate that bias-only finetuning similar to ours is effective also for adaptation of pre-trained computer vision models.",
            "82": "Discussion",
            "83": "Besides its empirical utility, the remarkable effectiveness of bias-only fine-tuning raises intriguing questions on the fine-tuning dynamics of pretrained transformers, and the relation between the bias terms and transfer between LM and new tasks. ",
            "84": "We aim to study those questions in a future work.",
            "85": "For convenience, we relate the notation used in the paper with the names of the corresponding parameters in the popular HuggingFace implementation.",
            "86": "To perform classification with BERT, we follow the approach of Devlin et al. (2018), and attach a linear layer to the contextual embedding of the CLS token to predict the label. ",
            "87": "The GLUE tasks are fed into BERT using the standard procedures. ",
            "88": "We optimize using AdamW (Loshchilov and Hutter, 2017), with batch sizes of 8. ",
            "89": "For full finetuning, we used initial learning rates in {1e-5, 2e-5, 3e-5, 5e-5}, and for the bias-only experiments we used initial learning rates in {1e-4, 4e-4, 7e-4, 1e-3} as the smaller rates took a very long time to converge on some of the tasks. ",
            "90": "With the larger learning rates, the bias-only fine-tuning converged in 7 or fewer epochs for most tasks, and up to 20 epochs on the others. ",
            "91": "We did not perform hyperparameter optimization beyond the minimal search over 4 learning rates.",
            "92": "As Mosbach et al. (2020) show, fine-tuning BERT LARGE and RoBERTa BASE is a unstable due to vanishing gradients. ",
            "93": "BitFit allows for the usage of bigger learning rates, and overall the optimization process is much more stable, when compared with a full fine-tuning.",
            "94": "We provide information on the GLUE tasks we evaluated on, as well as on the evaluation metrics. ",
            "95": "We test our approach on the following subset of the GLUE (Wang et al., 2018)    The metrics that we used to evaluate GLUE Benchmark are in Table 5. ",
            "96": "Learning rate configurations for best performing models are in Table 6."
        }
    },
    "af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66": {
        "doc1": {
            "0": "paper_summary:",
            "1": "The paper argues that jointly adopting both label smoothing and vocabulary sharing techniques can be conflicting.",
            "2": "This comes from a simple intuition that words have zero overlap with the possible target words should have no chance to appear in the target sentence.",
            "3": "They propose to use a very simple approach (Masked label smoothing) that addresses the conflict.",
            "4": "Results show that the method gains very incremental improvement in translation accuracy and some improvement in model calibration.",
            "5": "Overall the conflict is interesting but the result is very mixed.",
            "6": "summary_of_strengths:",
            "7": "The paper presents an interesting conflict between these two approaches.",
            "8": "I personally never thought about this before.",
            "9": "summary_of_weaknesses:",
            "10": "Cons:",
            "11": "From translation accuracy perspective, the improvement from fixing the conflict is very incremental.",
            "12": "This is clearly shown as the translation improvement is only +0.47 at its best.",
            "13": "In other cases (e.g. in the specific case (EN-RO - Table 5)), the improvement is +0.04 (from 23.15 to 23.19), which I would say it does not imply anything that is particularly meaningful.  able 5 also shows that assigning B_s to 0 seems does not matter that much (i.e. it does not hurt the performance that much).",
            "14": "This shows that despite the intuition, the conflict actually does not matter in reality.",
            "15": "There might be improvement in model calibration, but with the current paper it is not clear from Table 4 the improvement means big thing or not.",
            "16": "comments,_suggestions_and_typos:",
            "17": "Typo + writing",
            "18": "- as shown in Table 6,7 for both BLEU 207 and chrF -> 6, 7",
            "19": "- As shown in Table 3 , MLS 212 achieves consistent improvement over the origi213 nal label smoothing in both the original and the 214 balanced multilingual translation dataset under all 215 translation directions -> Table 3, MLS",
            "20": "- About ECE score: There should be an explanation from the paper.",
            "21": "- Writing style: the authors use the word outperform a lot in the paper.",
            "22": "Clearly the improvement does not reflect an \"outperforming\" by anymeans.",
            "23": "I suggest the authors lower the use of word..."
        },
        "doc2": {
            "0": "Focus on the Target's Vocabulary: Masked Label Smoothing for Machine Translation",
            "1": "Abstract",
            "2": "Label smoothing and vocabulary sharing are two widely used techniques in neural machine translation models. ",
            "3": "However, we argue that jointly adopting these two techniques can be conflicting and even leads to sub-optimal performance, since the soft label produced by label smoothing still considers the source-side words that would not appear at the target side. ",
            "4": "To address this issue, we propose Masked Label Smoothing (MLS), a new mechanism that masks the soft label probability of source-side words to zero. ",
            "5": "Simple yet effective, MLS manages to better integrate label smoothing with vocabulary sharing and hence improves the quality of the translation. ",
            "6": "Our extensive experiments show that MLS consistently yields improvement over original label smoothing on different datasets, including bilingual and multilingual translation in both BLEU and calibration scores.",
            "7": "Introduction",
            "8": "Recent advances in Transformer-based (Vaswani et al., 2017) models have achieved remarkable success in Neural Machine Translation (NMT). ",
            "9": "For most NMT studies (Vaswani et al., 2017;Song et al., 2019;Lin et al., 2020;Pan et al., 2021), there are two widely used techniques to improve the quality of the translation: Label Smoothing (LS) and Vocabulary Sharing (VS). ",
            "10": "Label smoothing (Pereyra et al., 2017) turns the hard one-hot labels into a soft weighted mixture of the golden label and the uniform distribution over the whole vocabulary, which serves as an effective regularization technique to prevent over-fitting and overconfidence (M\u00fcller et al., 2019) of the model. ",
            "11": "In addition, vocabulary sharing (Xia et al., 2019) is another commonly used technique, which unifies the vocabulary of both source and target language into a whole vocabulary, and therefore the vocabulary is shared. ",
            "12": "It enhances the semantic correlation   (Vaswani et al., 2017). ",
            "13": "Jointly adopting label smoothing and vocabulary sharing techniques cannot achieve further improvements, but leads to sub-optimal performance.",
            "14": "between the two languages and reduces the number of total parameters of the embedding matrices. ",
            "15": "However, in this paper, we argue that jointly adopting both label smoothing and vocabulary sharing techniques can be conflicting, and leads to suboptimal performance. ",
            "16": "Specifically, with vocabulary sharing, the shared vocabulary can be divided into three parts as shown in Figure 1. But with label smoothing, the soft label still considers the words at the source side that are impossible to appear at the target side. ",
            "17": "This would mislead the translation model and exerts a negative effect on the translation performance. ",
            "18": "As shown in Table 1, although introducing label smoothing or vocabulary sharing alone can outperform the vanilla Transformer, jointly adopting both of them cannot obtain further improvements but achieves sub-optimal results.",
            "19": "To address the conflict of label smoothing and",
            "20": "Background",
            "21": "Label Smoothing The original label smoothing can be formalized as:",
            "22": "K denotes the number of classes, \u03b1 is the label smoothing parameter, \u03b1/K is the soft label, \u0177 is a vector where the correct label equals to 1 and others equal to zero and \u0177LS is the modified targets.",
            "23": "Label smoothing is first introduced to image classification (Szegedy et al., 2016)  Vocabulary Sharing Vocabulary sharing is widely applied in most neural machine translation studies (Vaswani et al., 2017;Song et al., 2019;Lin et al., 2020;Pan et al., 2021). ",
            "24": "Researchers have conducted in-depth studies in Vocabulary Sharing.",
            "25": "Conflict Between Label Smoothing and Vocabulary Sharing",
            "26": "Words or subwords in a language pair's joint dictionary can be categorized into three classes: source, common and target using Venn Diagram according to their belonging to certain language as depicted in Figure 1. ",
            "27": "This can be achieved by checking whether one token in the joint vocabulary also belongs to the source/target vocabulary. ",
            "28": "We formalized the categorization algorithm in Appendix A.",
            "29": "Then we compute the tokens' distribution in different translation directions as shown in Table 2. ",
            "30": "Tokens in source class account for a large proportion up to 50%. ",
            "31": "When label smoothing and vocabulary sharing are together applied, the smoothed probability will be allocated to words that belong to the source class. ",
            "32": "Those words have zero overlap with the possible target words, therefore they have no chance to appear in the target sentence, which might introduce extra bias for the translation system during training process.",
            "33": "Table 3 reveals the existence of conflict, that the joint use of label smoothing and vocabulary sharing doesn't compare with solely use one technique in all language pairs with a maximum loss of 0.32 BLEU score.",
            "34": "Methods",
            "35": "Weighted Label Smoothing",
            "36": "To deal with the conflict when executing label smoothing, we propose a plug-and-play Weighted Label Smoothing mechanism to control the smoothed probability's distribution. ",
            "37": " Weighted Label Smoothing(WLS) has three parameters \u03b2 t , \u03b2 c , \u03b2 s apart from the label smoothing parameter \u03b1, where the ratio of the three parameters represents the portion of smoothed probability allocated to the target, common and source class and the sum of the three parameters is 1. ",
            "38": "The distribution within token class follows a uniform distribution. ",
            "39": "WLS can be formalized as:",
            "40": "where \u0177 is a vector where the element corresponding to the correct token equals to 1 and others equal to zero. ",
            "41": "\u03b2 is a vector that controls the distribution of probability allocated to incorrect tokens. ",
            "42": "We use t i , c i , s i to represent probability allocated to the i-th token in the target,common,source category, all of which form the distribution controlling vector \u03b2 with K i \u03b2 i = \u03b1. ",
            "43": "The restriction can be formalized as:",
            "44": "Masked Label Smoothing",
            "45": "Based on the Weight Label Smoothing mechanism, we can now implement Masked Label Smoothing by set \u03b2 s to 0 and regard the target and common category as one category. ",
            "46": "In this way, Masked Label Smoothing is parameter-free and implicitly injects external knowledge to the model. And we have found out that this simple setting can reach satisfactory results according our experiments.",
            "47": "We illustrate different label smoothing methods in Figure 2. ",
            "48": "It is worth noticing that MLS is different from setting WLS's parameters to 1-1-0 since there might be different number of tokens in the common and target vocab. ",
            "49": " The height of each bar in the graph denoted the probability allocated to each token. ",
            "50": "y is the current token during current decoding phase. ",
            "51": "We assume that there are only 10 tokens in the joint vocabulary and t1-t3 belongs to target class, c1-c3 belongs to common class and s1-s3 belongs to source class.",
            "52": "Experiments",
            "53": "Task Settings",
            "54": "For bilingual translation, we conduct experiments on 7 translation tasks. ",
            "55": "We choose language pairs that have different ratio of common subwords. ",
            "56": "For multilingual translation, we combine the IWSLT'16  to formulate a RO,DE-EN translation task. ",
            "57": "We make a balanced multilingual dataset that has equal numbers of DE-EN and RO-EN training examples to reduce the impact of imbalance languages.",
            "58": "We use the Transformer base (Vaswani et al., 2017) model as our baseline model. ",
            "59": "During training, we fix the label smoothing parameter \u03b1 to 0.1 whenever LS is applied. ",
            "60": "We list the concrete training and evaluation settings in Appendix B.",
            "61": "Results",
            "62": "Bilingual Table 3 shows the results of bilingual translation experiments. ",
            "63": "The results reveal the conflict between LS and VS that models with only LS outperform models with both LS and VS in all experiments. ",
            "64": "Our Masked Label Smoothing obtained consistent improvements over original LS+VS in all tested language pairs significantly.",
            "65": "The effectiveness of MLS maintained under different \u03b1 value as shown in Table 6,7 for both BLEU and chrF, which further proves that not only the increase in target vocabulary, but also the decrease of probabilities in source vocabulary matters in the improvement of translation performance.",
            "66": "Multilingual As shown in Table 3 , MLS achieves consistent improvement over the original label smoothing in both the original and the balanced multilingual translation dataset under all translation directions. ",
            "67": "Compared with the imbalanced version, the balanced version gave better BLEU scores in DE-EN direction while much worse performance in RO-EN translation. ",
            "68": "We discuss more of the multilingual result in Appendix C.",
            "69": "MLS proves its robustness and effectiveness under different translation settings.",
            "70": "Discussion",
            "71": "6.1 Improvement in Model's Calibration Guo et al. (2017) have pointed out by softening the estimation targets, label smoothing prevents the model from becoming over-confident therefore improve the calibration of model as analyzed in (M\u00fcller et al., 2019). ",
            "72": "Inference ECE score  reflects models calibration during inference. ",
            "73": "We compute the inference ECE scores of our models as shown in Table 4. ",
            "74": "The results indicate that MLS will lead to better calibration.",
            "75": "Exploring of Weighted Label Smoothing",
            "76": "As reported in Table 5, we further explore the influence of different weighted label smoothing settings on multiple translation tasks including IWSLT'16  According to the result, though the best BLEU score's WLS setting vary from different tasks, we still have two observations: First, applying WLS can generally boost the quality of translation compared to the original label smoothing. ",
            "77": "Second, only WLS with \u03b2 t , \u03b2 c , \u03b2 s each equals to 1/2-1/2-0 can outperform the original label smoothing on all tasks, which suggests the setting is the most robust one. ",
            "78": "Thus we recommend using this setting as the initial setting when applying our WLS.",
            "79": "Furthermore, the winner setting agrees with the form of Masked Label Smoothing since they both allocate zero probability to the source category's tokens, which further proves the effectiveness and robustness of Masked Label Smoothing.",
            "80": "Conclusion",
            "81": "We reveal and analyse the conflict between label smoothing and vocabulary sharing techniques, and point out that jointly adopting them may lead to sub-optimal performance. ",
            "82": "To address this issue, we introduce a plug-and-play Masked Label Smoothing mechanism to eliminate the conflict. ",
            "83": "Simple yet effective, MLS shows a consistent and significant improvement over original label smoothing with vocabulary sharing.",
            "84": "Algorithm 1 Divide Token Categories Input: List: S, T, J Output: List: A,B,C Description: S is the vocabulary list for source language, T for target language, J for joint vocabulary. ",
            "85": "A is the output vocabulary for source tokens, B for common tokens, C for target tokens.",
            "86": "We use the official train-dev-test split of WMT'14 and IWSLT'14,15,16 dataset. ",
            "87": "For CASIA ZH-EN dataset, we randomly select 5000 sentences as development set and 5000 sentences test set the total dataset.",
            "88": "We evaluate our method upon Transformer-Base (Vaswani et al., 2017) and conduct experiments under same hyper-parameters for fair comparison. ",
            "89": "We use compound_split_bleu.sh from fairseq to compute the final bleu scores. ",
            "90": "The inference ECE score 1 and chrF score 2 are computed through open source scripts.",
            "91": "Before training, we first apply BPE (Sennrich et al., 2016) to tokenize the corpus for 16k steps each language and then learn a joint dictionary. ",
            "92": "During training, the label smoothing parameter \u03b1 is set to 0.1 in all experiments. ",
            "93": "We use Adam optimizer with betas to be (0.9,0.98) and learning rate is 0.0007. ",
            "94": "During warming up steps, the initial learning rate is 1e-7 and there are 5000 warm-up steps. ",
            "95": "We use a batchsize of 4096 together with an update-freq of 4 on two Nvidia 3090 GPUs. ",
            "96": "Dropout rate is set to 0.3 and weight decay is set to 0.0001 for all experiments. ",
            "97": "We use beam size as 5 during all testing."
        }
    },
    "14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6": {
        "doc1": {
            "0": "paper_summary:",
            "1": "This paper studies the Machine Translation of the endangered language Livonian.",
            "2": "The authors first collect a parallel corpus on LIV, LV, ET and EN, by assembling available digital resources and via manual translations.",
            "3": "Then different \"base\" machine translations are trained and finetuned on the LIV->EN data.",
            "4": "Backtranslations and pivot translation are also explored.",
            "5": "summary_of_strengths:",
            "6": "NMT for endangered languages is a critical and interesting research direction.",
            "7": "The proposed parallel corpus is valuable and make great contributions to Livonian NMT.",
            "8": "Interesting discoveries are made, e.g., the authors find that ET is a preferred pivot than LV in LIV-EN translation.",
            "9": "summary_of_weaknesses:",
            "10": "Some evaluations results are mixing.",
            "11": "Please see the detailed comments below.",
            "12": "comments,_suggestions_and_typos:",
            "13": "Question:",
            "14": "1. In line 257, it's strange that including the collected LIV-EN parallel data for finetuning actually makes the NMT system perform worse.",
            "15": "Could you provide more discussions/explanations on this?",
            "16": "2. Could you provide some insight why the multilingual model is \"noticeably weaker\" (line 236) on the ET\u2192En and LV\u2192EN evaluations?",
            "17": "3. It's interesting that the LV\u2192EN model performs better than the ET\u2192EN model, especially in section 2 authors mentioned that Livonian and Latvian languages are similar in many aspects.",
            "18": "minor:",
            "19": "1. Maybe the information that \"the translation is done by hired experts (section 3)\" can be added to the footnote 2 on page 2 (section 1)?",
            "20": "I was a bit confused when first reading that footnote since I wasn't sure how the translation is done.",
            "21": "2. Line 194, maybe I'm not familiar with the context, could you explain what does \"implement the support of ...\" mean?",
            "22": "3. For future work, it may be worth considering adding cross-lingual contrastive learning to the training.",
            "23": "4. Line 210, on what ET, EN, LV data is the Sentencepiece tokenizer obtained on?",
            "24": "Those in the 4-language parallel corpus?",
            "25": "5. Line 255: typo, \"perform performed\""
        },
        "doc2": {
            "0": "Machine Translation for Livonian: Catering for 20 Speakers",
            "1": "Abstract",
            "2": "Livonian is one of the most endangered languages in Europe with just a tiny handful of speakers and virtually no publicly available corpora. ",
            "3": "In this paper we tackle the task of developing neural machine translation (NMT) between Livonian and English, with a two-fold aim: on one hand, preserving the language and on the other -enabling access to Livonian folklore, lifestories and other textual intangible heritage as well as making it easier to create further parallel corpora. ",
            "4": "We rely on Livonian's linguistic similarity to Estonian and Latvian and collect parallel and monolingual data for the four languages for translation experiments. ",
            "5": "We combine different low-resource NMT techniques like zero-shot translation, cross-lingual transfer and synthetic data creation to reach the highest possible translation quality as well as to find which base languages are empirically more helpful for transfer to Livonian. ",
            "6": "The resulting NMT systems and the collected monolingual and parallel data, including a manually translated and verified translation benchmark, are publicly released. ",
            "7": "1",
            "8": "Introduction",
            "9": "Many state-of-the-art natural language processing tasks have reached admirable quality on languages with abundant linguistic resources (Vaswani et al., 2017;Devlin et al., 2019). ",
            "10": "Furthermore, some neural language models and translation systems have been created for 100 and more languages (e.g. Conneau et al., 2020;Fan et al., 2021). ",
            "11": "However smaller, less or not at all spoken languages continue to struggle not only in terms of applicable computational approaches, but 1 https://opus.nlpl.eu/liv4ever.php, agreed to be kept anonymous with OPUS administration.",
            "12": "Models and corpora will also be added to the Huggingface repository after de-anonymization (https://huggingface.co/). ",
            "13": "more critically -in terms of usable resources for training natural language processing (NLP) models or even just linguistic exploration.",
            "14": "In this paper we set the goal of developing usable machine translation between English and Livonian. ",
            "15": "Currently there are over 20 fluent speakers of the language (Ern\u0161treits, 2016). ",
            "16": "Although some digital linguistic resources exist for Livonian (including a dictionary with example sentences and a written monolingual corpus, (Ern\u0161treits, 2016)), there is virtually no open parallel corpora with it, with the single exception of 35 parallel sentences in the OPUS Tatoeba corpus (Tiedemann, 2020).",
            "17": "At the same time, cross-lingual transfer learning has recently helped improve the performance of several low-resource NLP tasks with the support of related languages (e.g. Hu et al., 2020). ",
            "18": "This also includes zero-shot translation (Johnson et al., 2017), the ability of multilingual NMT systems to translate between seen languages that were not represented in the parallel training data as a pair. ",
            "19": "The case of Livonian is especially interesting in this regard, as there are two different sources of such support: on one hand, it is a Uralic language, closely related to Estonian and Finnish. ",
            "20": "On the other hand, Livonian has taken part in forming Latvian language and Livonian speakers have historically co-existed side-by-side with Latvian speakers. ",
            "21": "As a result of mutual influence these two languages also share a number of grammatical, lexical and orthographic similarities.",
            "22": "Our main contributions are two-fold. ",
            "23": "vian/Estonian/English so that each sentence would have all four translations. ",
            "24": "2",
            "25": "The second half of our work focuses on neural machine translation (NMT, Vaswani et al., 2017), mainly targeting Livonian\u2194English. ",
            "26": "We explore several options of coping with the extremely lowresource settings and use Estonian and Latvian for cross-lingual transfer. ",
            "27": "Our experiments answer the following research questions:",
            "28": "1. Can we achieve machine translation for Livonian\u2194English at a usable level?",
            "29": "2. ",
            "30": "Which base language suits better for serving as base for cross-lingual transfer to Livonian, Estonian or Latvian?",
            "31": "3. Does zero-shot multilingual translation deliver better translation quality than pivottranslation through Estonian or Latvian?",
            "32": "Next we briefly describe the Livonian Language in Section 2, then introduce the collected parallel and monolingual data in Section 3. ",
            "33": "Section 4 provides the details of our NMT experiments and Section 5 concludes the paper. ",
            "34": "the mid-20th century around 1500 speakers. ",
            "35": "Nowadays Livonian is listed in UNESCO's Atlas of the World's Languages in Danger as a critically endangered language (Moseley, 2014). ",
            "36": "According to the 2011 census, there are 250 Livonians in Latvia. ",
            "37": "Although there are just over 20 people who can speak the language, the Livonian community is active in preserving and developing the Livonian heritage (Ern\u0161treits, 2016) and language plays key role in this process (Ern\u0161treits and Kl , ava, 2020).",
            "38": "The Livonian language developed in the contact area of Baltic and Finnic languages. ",
            "39": "Livonian and Latvian share a similar geographical location over a prolonged period of time, as a result of which they both contain traces of contact. ",
            "40": "Next to other loanwords, the Livonian loanword strata consists of words borrowed from Latvian (Suhonen, 1973;Winkler, 2014) and vice versa. ",
            "41": "The most obvious Latvian influence on Livonian grammar is found in the Livonian case system (Ern\u0161treits and Kl , ava, 2014). ",
            "42": "Livonian has the prosodic characteristics typical of a Finnic language such as word-initial stress and the phonological opposition of short and long phoneme duration. ",
            "43": "It is the only Finnic language that differentiates lexical tones -the plain tone and the broken tone or st\u00f8d -and therefore shares similar characteristics with Latvian as well as Danish (Tuisk, 2016).",
            "44": "Collected Data",
            "45": "The first step in developing (supervised) machine translation is collecting parallel data. ",
            "46": "While there was no pre-existing open parallel corpus with Livonian, we used all the possible sources of translations. ",
            "47": "This was limited to already digital resources, future work might include texts extracted by scanning older books and other materials. ",
            "48": "The main sources of data included Livonian-Latvian as well as Livonian-Estonian translations. ",
            "49": "Thus we use these two languages as base for crosslingual transfer and e.g. leave Finnish out, as there was no data for it.",
            "50": "The sources of data included:",
            "51": "\u2022 the Constitution of the Republic of Latvia, translated into 9 languages, including Livonian, Estonian and English,",
            "52": "\u2022 a database of dictionary entries, phrases and example sentences from the University of Latvia Livonian Institute's website 5 , with examples in Livonian, Estonian and Latvian",
            "53": "\u2022 the Livonian Institute's Facebook page posts, partially parallel between our 4 languages",
            "54": "\u2022 books (Stalte, 2011;Kurs and et al., 2016;Ern\u0161treit et al., 2020) with prefaces and content in Livonian-Estonian or Livonian-Latvian",
            "55": "\u2022 and abstracts from the Journal of Estonian and Finno-Ugric Linguistics' (JEFUL) Special Issues on Livonian Studies (2014,2016,2018) in Livonian, Estonian and English Concerning sentence alignment, the dictionary examples consisted of already aligned Livonian sentences. ",
            "56": "We aligned the rest of the data manually with the help of language experts -first on paragraph level, then on sentence level.",
            "57": "The total amount of sentences in the resulting dataset is shown in Table 1.",
            "58": "We separated balanced portions of development (503 sentences) and evaluation (749 sentences) splits from the full dataset. ",
            "59": "The splits are balanced in terms of the original source of the texts to resemble proportions from the remaining training data.",
            "60": "We hired professional translators to create translations for any missing parts so that these splits would be parallel between all four languages. ",
            "61": "We further turned to experts of the Livonian language to make sure that the newly created translations truly convey the meaning of the original text as a quality control measure.",
            "62": "The resulting benchmark and the whole corpus is published in the OPUS collection (currently, anonymously). ",
            "63": "6",
            "64": "Machine Translation Experiments",
            "65": "Having just over 10, 000 parallel examples constitutes extremely low-resource settings for neural machine translation. ",
            "66": "Added to this, the number of monolingual Livonian sentences (about 40, 000) is also too small for approaches like unsupervised machine translation (Artetxe et al., 2018;.",
            "67": "We implement the support of neighboring and related languages (Estonian and Latvian) via multilingual machine translation (Johnson et al., 2017). ",
            "68": "As a first step the model is pre-trained with the larger languages (Estonian, Latvian, English) and then used as base for following experiments.",
            "69": "Technical Setup",
            "70": "We used FairSeq (Ott et al., 2019) to train transformer architecture models with 6 encoder and decoder layers, 8 transformer attention heads per layer, word embeddings and hidden layers of size 512, dropout of 0.3, maximum sentence length of 128 symbols, and a batch size of 1024 words. ",
            "71": "All models were trained until they reached convergence (no improvement for 10 checkpoints) on development data. ",
            "72": "We used Sentencepiece (Kudo and Richardson, 2018)  ies of size 25,000, and SacreBLEU 7 (Post, 2018) to generate BLEU scores for translations.",
            "73": "Base models were trained on LV\u2192EN, ET\u2192EN, ET+LV\u2192EN data, and a multilingual model using the tagged approach (Johnson et al., 2017) for translating in all directions between ENG/EST/LAT. ",
            "74": "The base models were then used as initialization for tuning on LIV\u2192EN data. ",
            "75": "For training the base models we used all available parallel data from Opus (Tiedemann and Nygaard, 2004). ",
            "76": "To facilitate further use of the base models for tuning on Livonian data, all Livonian sentences were used in addition to other data when creating the shared vocabularies. ",
            "77": "Finally, we used the highest-scoring tuned model to perform performed backtranslation on the monolingual LIV data to generate additional training data for training the final models.",
            "78": "Results",
            "79": "Table 2 shows results from MT experiments. ",
            "80": "All BLEU scores are calculated for translations of our evaluation We compare the base single direction MT models to our multidirection model, as well as online translations from Google Translate 8 and Neurotolge 9 to evaluate performance from ET and LV into EN. ",
            "81": "While the multilingual model was noticeably weaker, the others hold comparable results to the online systems. ",
            "82": "However, when attempting to perform zero-shot translation from LIV into EN, the ET\u2192EN model was able to outperform LV\u2192EN (3.22 vs. 2.20), and the multilingual model achieved a very respectable BLEU score 8.92.",
            "83": "We then turned to tuning each of these models with LIV-EN data mixed 1:1 with a random equal amount of the original training data for each of the models. ",
            "84": "In the case of the multilingual model, we also added LV/ET-LIV data to the mix. ",
            "85": "This improved all scores by 1-3 BLEU points, but the multilingual model remained on top with 11.62 for LIV\u2192EN. ",
            "86": "In order to perform backtranslation models for both directions are required, so we scored the tuned multilingual model on the EN\u2192LIV data as well, reaching 8.10 BLEU. ",
            "87": "Finally, to verify how much the tiny amount (503 sentences) of LIV-EN parallel data brings for tuning we ran a separate experiment excluding it and achieved 11.87 BLEU for LIV-EN and 8.25 BLEU for EN-LIV, which is even slightly higher than with the data there.",
            "88": "For comparison we also used the same tuned multilingual model to perform pivotal translation by first translating into ET or LV and then into the desired target language. ",
            "89": "Results from these experiments in Table 3 show that not only ET is better than LV to pivot translate between EN and LIV, but also that this approach was able to reach the highest BLEU scores so far. ",
            "90": "Tuning on the backtranslated data seemed to overwhelm the far lower amount of clean parallel data and did not produce a higher BLEU score in any direction.",
            "91": "To answer the research questions, posed in the introduction, it seems that the resulting translation quality is still far from being usable. ",
            "92": "Comparisons between the base languages have shown slight preference towards Estonian over Latvian. ",
            "93": "Interestingly, pivot-translation through Estonian showed higher translation quality than direct Livonian\u2194English trained in a zero-shot / few-shot manner.",
            "94": "Conclusion",
            "95": "In this paper we presented a novel dataset for the highly endangered Livonian language, which can be useful for machine translation, language modelling and many other natural language processing and computational linguistic research tasks.",
            "96": "In our experiments we show how far one can get in training modern machine translation models with very scarce data, and which languages are more suitable for transfer learning when working with Livonian data. ",
            "97": "While perhaps not being usable as-is in any kind of production scale, the achieved BLEU scores of 13.00 and 9.47 show that some transfer of meaning can still be achieved between Livonian and English with the currently available resources."
        }
    },
    "ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475": {
        "doc1": {
            "0": "paper_summary:",
            "1": "This paper used chi-squared measures, t-statistics, and raw frequency to build a token merging pre-processing step, in order to improve the results of LDA in languages without marked word boundaries.",
            "2": "It is valuable to leverage collocations for languages without marked word boundaries (e.g., Chinese and Thai), and the experimental results indicated positive contributions of token emerging.",
            "3": "However, this paper only compared the proposed approaches with LDA, without employing the existing similar approaches, e.g., the methods proposed in (Lau et al., 2013), as baselines.",
            "4": "Besides, the related work section is quite sketchy.",
            "5": "summary_of_strengths:",
            "6": "1. The experiments were conducted on 7 languages, which was valuable to test whether representing bigrams collocations in the input could improve topic coherence or not in general.",
            "7": "2. The experimental results indicated that the t-statistic and raw frequency approaches for token merging could improve the topic modeling results across 7 languages.",
            "8": "summary_of_weaknesses:",
            "9": "1. The descriptions of several important sentences are unclear, e.g., in \u201cFor all languages, we use the reduced version of Wikipedia database\u201d, it is suggested to provide the links to the reduced version of Wikipedia database for all languages (except for English).",
            "10": "To evaluate the influence of these large collocation training corpora, it is also suggested to compute the collocation measures for all bigrams on each document collection (i.e., without any external corpora).",
            "11": "2. Figure 1 should be illustrated in more details.",
            "12": "Why only show the results of three languages in Figure 1?",
            "13": "Are the results of other languages consistent with these three languages?",
            "14": "3. The related work section is quite sketchy.",
            "15": "There are limited reviews on collocations and topic models in recent years.",
            "16": "One of the relevant early studies was cited, i.e., (Lau et al., 2013), in which, Lau et al. compared topic models learned from unigram bag-of-words data, with topic models learned from bag-of-words data that includes preextracted bigram collocations.",
            "17": "As shown in (Lau et al., 2013), they considered four different bigram replacement methods.",
            "18": "Particularly, they first extracted bigrams for each document collection using the N-gram Statistics Package, identifying the top bigrams based on the Student\u2019s t-test.",
            "19": "Then, they used the top 1k, 10k, and 100k as the three different bigram replacement methods.",
            "20": "Unfortunately, the above method was not employed as the baseline in this study, in order to validate the effectiveness of using large collocation training corpora to compute the collocation measures for all bigrams.",
            "21": "comments,_suggestions_and_typos:",
            "22": "1. The presentation can be improved, e.g., \u201cTo train word embeddings\u201d (line 298) and \u201cto obtain word embeddings\u201d (line 301) are repetitive.",
            "23": "2. The reference (El-Kishky et al., 2014) had been published at Proc. VLDB Endow. 8(3): 305-316 (2014).",
            "24": "Besides, the information of reference (Merity et al., 2016) are incomplete."
        },
        "doc2": {
            "0": "More Than Words: ",
            "1": "Collocation Retokenization for Latent Dirichlet Allocation Models",
            "2": "Abstract",
            "3": "Traditionally, Latent Dirichlet Allocation (LDA) ingests words in a collection of documents to discover their latent topics using word-document co-occurrences. ",
            "4": "Previous studies show that representing bigrams collocations in the input can improve topic coherence in English. ",
            "5": "However, it is unclear how to achieve the best results for languages without marked word boundaries such as Chinese and Thai. ",
            "6": "Here, we explore the use of retokenization based on chi-squared measures, tstatistics, and raw frequency to merge frequent token ngrams into collocations when preparing input to the LDA model. ",
            "7": "Based on the goodness of fit and the coherence metric, we show that topics trained with merged tokens result in topic keys that are clearer, more coherent, and more effective at distinguishing topics than those of unmerged models. ",
            "8": "settings but not significantly higher than t retok-328 enization. ",
            "9": "However, we observe mixed results from 329 \u03c7 2 retokenization for some languages. ",
            "10": "This is quite 330 surprising because raw frequency was previously 331 found to be an inferior measure of collocation. ",
            "11": "This 332 suggests that t and frequency-based retokenization 333 might be a more reliable method for improving the 334 goodness of fit of the LDA model. ",
            "12": "This also sug-335 gests that Japanese and Korean might have some 336 specific quality that interacts well with all three 337 types of retokenization. ",
            "13": "338 Similarly, we observe a general improvement in 339 coherence for the t and frequency retokenization 340 (Table 3). ",
            "14": "The higher CBES score indicates that 341 topic-keys are more semantically coherent and top-342 ics are more distinct. ",
            "15": "The coherence improves after 343 t and frequency-based retokenization for English, 344 Japanese, Korean, and Arabic corpora regardless of 345 the number of topics. ",
            "16": "The improvement for Thai is 346 426 of 0.41, 0.77, and 0.68 for the models with 10, 50, 427 and 100 topics respectively for PTLL. ",
            "17": "As for the 428 coherence metric, we found the correlation coeffi-429 cients of 0.73, 0.76, and 0.79 for the models with 430 10, 50, and 100 topics respectively for CBES. ",
            "18": "This 431 means the models with higher merge percentage 432 are better than their corresponding word models 433 in reproducing the statistics of the held-out data. ",
            "19": "434 This suggests that the quality of the LDA models 435 depends on the generalizability of the retokenizers. ",
            "20": "436 The LDA model results become more under-437 standable when certain tokens are retokenized. ",
            "21": "We 438 see merged tokens in the topic key sets of almost 439 all topics in all corpora when retokenized based 440 on t or raw frequency. ",
            "22": "Many of these represent 441 non-compositional meanings that might have been 442 lost without retokenization: for example, the col-443 location \"social security\" is not fully represented 444 by the individual tokens \"social\" or \"security\" sep-445 arately. ",
            "23": "More strikingly, the collocation 'k\u014dn s\u01d4a 446 d\u0101ng' refers to a political movement group in Thai-447 land. ",
            "24": "When it is separated into k\u014dn (people) s\u01d4a 448 (shirt) d\u0101ng (red), the key meaning is totally lost.",
            "25": "449 When we compare by looking at the topic-keys of 450 the word and multi-word models, we can come up 451 with similar topics because we as a human who of-the-art tokenization for German web and social 580 media texts. ",
            "26": "In Proceedings of the 10th Web as Cor-581 pus Workshop (WAC-X) and the EmpiriST Shared",
            "27": "582",
            "28": "Introduction",
            "29": "Latent Dirichlet allocation (LDA) models provide useful insights into themes and trends in a large text collection through the unsupervised inference of topics, or probability distributions over unigram word types in the corpus (Blei et al., 2003). ",
            "30": "Topics from these models are often interpreted based on their highest-probability words, with documents expressed as vectors of proportions of each topic. ",
            "31": "Unfortunately, the context in which these tokens arise can be obscured in the bag-of-words rendering of text as unigram counts in documents. ",
            "32": "For instance, a topic with high probabilities of both \"coffee\" and \"table\" is tempting to interpret as focusing on the furniture item \"coffee table\", but both words could be frequent in a discussion of cafes containing no coffee tables. ",
            "33": "This problem is amplified in languages without marked word boundaries, such as Chinese and Thai: while existing tokenizers in these languages can segment characters into words, there is always a question about to what extent the tokenizers should group words together.",
            "34": "Words that have been segmented by tokenizers may not express the concept of the original text if they were found as parts of collocations. ",
            "35": "Meaningful interpretation of topics can be lost without careful recombination of these words.",
            "36": "We hypothesize that the morphology of the language should play an important role in determining the suitable pre-processing steps that would improve the results of topic models. ",
            "37": "The main morphological types we consider are synthetic language and analytic language. ",
            "38": "Synthetic languages use many morphemes to compose a word and can be further divided into fusional and agglutinative languages. ",
            "39": "Fusional languages such as German differ from agglutinative languages such as Korean and Japanese: a single morpheme in fusional languages can code for many morphosyntactic features. ",
            "40": "On the other hand, analytic languages such as Thai and Chinese convey meanings by relating many words together, and morphological devices are more rarely used. ",
            "41": "Under our hypothesis, analytic languages should benefit from token merging, but synthetic languages might not because the meaning is conveyed by inflection (through bound morphemes) and agglutination (through free morphemes).",
            "42": "In this project, we investigate the effects of token merging as a pre-processing step, and study how those effects vary based on the writing systems and the morphological features of the languages. ",
            "43": "We evaluate three measures to determine when to merge multiple adjacent words into conceptuallyunified phrasal tokens prior to LDA model training: chi-squared statistics, t-statistics, and raw frequency counts of phrases. ",
            "44": "We test these merging strategies on English, German, Chinese, Japanese, Korean, Thai, and Arabic. ",
            "45": "This set of languages are drawn from various writing systems and different morphological typology to see which type of language favors which type of merging strategy.",
            "46": "The main contributions of this paper are as fol-lows:",
            "47": "\u2022 We determine through empirical studies that a t-statistic and raw-frequency approach to token merging improves the topic modeling results across all language types and writing systems for the corpora that do not differ much from the collocation training data.",
            "48": "\u2022 We also show the positive consequences of token merging: the percentage of merged tokens in the LDA training data is correlated with the quality of the topic modeling results.",
            "49": "\u2022 Finally, we provide evidence that the popular approach of applying a \u03c7 2 measure to token merging tends to overfit to the collocation training data and result in a low percentage of merged tokens in a number of languages, making it a less suitable general-purpose approach than t-statistics.",
            "50": "Related Work",
            "51": "Pre-processing steps can substantially alter the results of the LDA models even in languages with good tokenization heuristics such as English (Schofield and Mimno, 2016;May et al., 2016). ",
            "52": "We believe that languages that do not have clear tokenization standards deserve investigation into what kind of processing is appropriate. ",
            "53": "Many works recognize that LDA results can be improved when input are including phrases (Lindsey et al., 2012;Lau et al., 2013;Yu et al., 2013;El-Kishky et al., 2014;Wang et al., 2016;Bin et al., 2018;Li et al., 2018). ",
            "54": "We consider it valuable to specifically assess approaches to determining these phrases. ",
            "55": "Despite their popularity in analyzing large amounts of text data, LDA models are notoriously complex to evaluate. ",
            "56": "One must evaluate both the statistical fit of a model and the human-registered thematic coherence of the words found to arise in the high-probability words, or keys, of a topic, which may not correlate (Chang et al., 2009). ",
            "57": "Analyses often combine evaluations of fit (Wallach et al., 2009) and automated approximations of human judgments of coherence (Bouma, 2009;Mimno et al., 2011) based on mutual information, even with the expectation these may only somewhat correlate with true human judgments (Lau et al., 2014). ",
            "58": "A limitation of these existing approaches, however, is that they expect the vocabulary and tokenization to remain constant between two models. ",
            "59": "For our evaluation, we use a normalized log likelihood approach to capture fit while accounting for changes in vocabulary (Schofield and Mimno, 2016).",
            "60": "Collocations as LDA Token",
            "61": "Collocations consist of two or more words that express conventional meaning, which can convey information about multi-word entities, context, and word usage. ",
            "62": "We hypothesize that the introduction of multi-word tokens, which capture collocations as bigrams or trigrams by way of concatenation of adjacent tokens, can help achieve more useful and coherent topic models. ",
            "63": "For languages without clear word boundaries, there is a possible additional benefit to multi-word tokens: it can be hard to intuit whether inferred word boundaries will have a large impact on the final results. ",
            "64": "Merging adjacent words into 'multi-word' tokens may help remedy the potential problem of a segmentation that is not optimal for topic modeling purposes.",
            "65": "Many methods are possible to select collocations to merge from tokenized text (Manning and Schutze, 1999). ",
            "66": "In this paper, we evaluate the chisquared statistics (\u03c7 2 ), the t-statistic and raw frequency as approaches to develop a threshold for merging collocations into multi-word tokens prior to topic model training. ",
            "67": "The chi-squared measure \u03c7 2 (w 1 , w 2 ) and t(w 1 , w 2 ) t-statistic for two adjacent tokens w 1 and w 2 are defined as: \u03c7 2 (w 1 , w 2 ) = (P (w 1 , w 2 ) \u2212 P (w 1 )P (w 2 )) 2 P (w 1 )P (w 2 ) (1)",
            "68": "We first compute the collocation measures for all bigrams on a large collocation training corpus.",
            "69": "Then we select the top bigrams that score the highest on the collocation measures and add those to our lexicon. ",
            "70": "After we tokenize and pre-process the collection of documents on which we would like to train LDA, we retokenize the data based on the collocation training corpus. ",
            "71": "We find all of the bigrams in the LDA training data that are also found in the top bigram lexicons that we obtain from the collocation training corpus. ",
            "72": "Then, the LDA training process proceeds as usual but with some of the original tokens merged into a multi-word tokens as defined from the collocation training data.",
            "73": "Evaluation Metrics",
            "74": "We consider two primary evaluation metrics for exploring the effect of merging tokens: one based on log likelihood, and one based on silhouette coefficients.",
            "75": "Held-Out Likelihood. ",
            "76": "When words are represented as a vector, this is exactly what the silhouette coefficients measure.",
            "77": "To compute them, we first compute the a(i), which is the mean cosine distance between topic-key i and other topic-keys in the same topic.",
            "78": "221",
            "79": "After obtaining a(i) and b(i), the silhouette coeffi-229 cient for topic-key i is defined as:",
            "80": "The silhouette coefficient for the entire model is the  To train word embeddings, we use the gensim ( \u0158eh\u016f\u0159ek and Sojka, 2010) implementation with the Continuous Bag-of-Word (CBOW) algorithm (Mikolov et al., 2013) to obtain word embeddings.",
            "81": "The training corpora and their collocation versions are prepared based on the tokenizers that we discuss above. ",
            "82": "We preprocess the word embedding training data and the LDA training data the same way. ",
            "83": "For English, we lemmatize and lowercase the data. ",
            "84": "For Korean, Japanese, and Arabic, we lemmatize the data. ",
            "85": "For German, Chinese, and Thai, we do not do any normalization.",
            "86": "We use MALLET (McCallum, 2002) implementation of LDA with the default hyperparameters to train and evaluate topic models in both word and multi-word (collocation) documents with 10, 50, 100 topics. ",
            "87": "We run the experiment 3 times for each combination of corpus, type of retokenization (no retokenization, \u03c7 2 , t or frequency) and number of topics to compute the means of the normalized held-out likelihood and CBES, discussed in section 4.",
            "88": "Results and Discussion",
            "89": "The normalized log-likelihood per token of the t and frequency-based retokenization is significantly higher than the baseline for English, German, Chinese, Japanese, Korean, and Arabic for all text collections and the number of topics except EN-Yelp, TH-BEST, and TH-TNC (Table 3 ). ",
            "90": "Frequency-   (bottom) for between the baseline and retokenization models: \u03c7 2 , textitt, and raw frequency. ",
            "91": "Shaded cells mean that the results are inferior to the baseline, while bolded cells show the best results for each corpus and number of topics.",
            "92": "based retokenization gives the best results for most spotty, and Chinanews is the only Chinese corpus in which we see improvement. ",
            "93": "This suggests that the choice of retokenization strategy might depend on the language types or the content of corpora itself. ",
            "94": "Consistent with the normalized log-likelihood results, Japanese and Korean corpora interact well with all three types of retokenization, suggesting that the morphology or typology of these two languages consistently benefit from collocation before training LDA models.",
            "95": "What could account for this discrepancy across languages and corpora? ",
            "96": "First, we observe a large variation of percentages of merged tokens across corpora. ",
            "97": "Because we fix the number of bigrams types to merge during the tokenizer training process to 50,000 for all three criteria (Table 1), we can use this analysis to find trends in the relative frequency of merged tokens. ",
            "98": "We see that \u03c7 2 retokenizer only merges barely 1% of all the tokens before training the LDA models for English, Chinese, \u03c7 2 : dvenadsat apostolov, jormp jomp, malwae tweep, aboul gheit, achduth vesholom, adavari matalaku, adeste fideles, afforementionede oughtt, agoraf drws, aht urhgan, akanu ibiam, aksak maboul, alberthiene endah, alfava metraxis, alfonsas eidintas, allasani peddana, alteram partem, amantes clandestinos, amarin winitchai, amel oluna t: united states, new york, world war, km h, take place, miles km, los angeles, united kingdom, first time, high school, tropical storm, new zealand, war ii, video game, mph km, h mph, north america, air force, two years, peak number frequency: united states, new york, world war, km h, take place, miles km, first time, los angeles, united kingdom, high school, tropical storm, new zealand, video game, war ii, mph km, two years, h mph, north america, air force, peak number the same language, news corpora have higher percentages of merged words when merged with t and frequency collocations, while corpora containing restaurant and movie reviews tend to see lower percentages (Table 1). ",
            "99": "This could be because the news corpora are in a similar domain to that of"
        }
    },
    "32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223": {
        "doc1": {
            "0": "paper_summary:",
            "1": "This paper proposes to use the synonyms of the ICD codes to enrich the code representations.",
            "2": "A multiple synonyms matching network (MSMN) is proposed to encode multiple synonyms and improve the task of automatic ICD coding.",
            "3": "Experimental results show that the proposed method improves the performance over the baseline methods, especially when evaluated using AUC.",
            "4": "The idea of using synonyms of the ICD codes is simple and effective.",
            "5": "The paper is well-written and concise.",
            "6": "The experimental results show promising improvement over previous work.",
            "7": "I like that the authors include discussion of varying m and different scoring functions in the paper to make it more complete.",
            "8": "Overall, I think this paper introduces a simple but effective method to improve automatic ICD coding and conducts experiments to verify its effectiveness.",
            "9": "I think the material presented is well-suited for a short paper.",
            "10": "summary_of_strengths:",
            "11": "- The idea of using synonyms of the ICD codes is simple.",
            "12": "- The paper is well-written and concise.",
            "13": "- The experimental results show promising improvement over previous work.",
            "14": "summary_of_weaknesses:",
            "15": "I have some questions regarding the results, see Questions below",
            "16": "comments,_suggestions_and_typos:",
            "17": "- The proposed method obtains larger gains on the top-50 setting.",
            "18": "My intuition was that the top codes would be benefited less since we have enough training data for those codes.",
            "19": "Why do you think this happens?",
            "20": "- For the full setting, AUC and precision improved a lot, but not F1.",
            "21": "Do you have any observation or explanation for this?"
        },
        "doc2": {
            "0": "Code Synonyms Do Matter: Multiple Synonyms Matching Network for Automatic ICD Coding",
            "1": "Abstract",
            "2": "Automatic ICD coding is defined as assigning disease codes to electronic medical records (EMRs). ",
            "3": "Existing methods apply label attention with code representations to match related text snippets for coding. ",
            "4": "Unlike these works that model the label with the code hierarchy or description, we argue that the code synonyms can provide more comprehensive knowledge based on the observation that the code expressions in EMRs vary from their descriptions in ICD. ",
            "5": "By aligning codes to concepts in UMLS, we collect synonyms of every code in ICD. ",
            "6": "Then, we propose a multiple synonyms matching network to leverage synonyms for better code representation learning, and finally help the code classification. ",
            "7": "Experiments on two settings of the MIMIC-III dataset show that our proposed method outperforms previous stateof-the-art methods.",
            "8": "Introduction",
            "9": "International Classification of Diseases (ICD) is a classification and terminology that provides diagnostic codes with descriptions for diseases 1 . ",
            "10": "The task of ICD coding refers to assigning ICD codes to electronic medical records (EMRs) which is highly related to clinical tasks or systems including patient similarity learning (Suo et al., 2018), medical billing (Sonabend et al., 2020), and clinical decision support systems (Sutton et al., 2020). ",
            "11": "Traditionally, healthcare organizations have to employ specialized coders for this task, which is expensive, time-consuming, and error-prone. ",
            "12": "As a result, many methods have been proposed for automatic ICD coding since the 1990s (de Lima et al., 1998).",
            "13": "Deep learning methods usually treat this task as a multi-label classification problem (Xie and Xing, 2018;Li and Yu, 2020;Zhou et al., 2021), which learn deep representations of EMRs with an RNN or CNN encoder and then predict codes with a multi-label classifier. ",
            "14": "Recent state-of-the-art methods propose label attention that uses the code representations as attention queries to extract the code-related representations 2 (Mullenbach et al., 2018). ",
            "15": "Following this idea, many works further propose using code hierarchical structures (Falis et al., 2019;Xie et al., 2019;Cao et al., 2020) and descriptions (Cao et al., 2020;Song et al., 2020) for better label representations.",
            "16": "In this work, we argue that the synonyms of codes can provide more comprehensive information. ",
            "17": "For example, the description of code 244.9 is \"Unspecified hypothyroidism\" in ICD. ",
            "18": "However, this code can be described in different forms in EMRs such as \"low t4\" and \"subthyroidism\". ",
            "19": "Fortunately, these different expressions can be found in the Unified Medical Language System (Bodenreider, 2004), a repository of biomedical vocabularies that contains various synonyms for all ICD codes. ",
            "20": "Therefore, we propose to leverage synonyms of codes to help the label representation learning and further benefit its matching to the EMR texts.",
            "21": "To model the synonym and its matching to EMR text, we further propose a Multiple Synonyms Matching Network (MSMN). ",
            "22": "Specifically, we first apply a shared LSTM to encode EMR texts and each synonym. ",
            "23": "Then, we propose a novel multisynonyms attention mechanism inspired by the multi-head attention (Vaswani et al., 2017), which considers synonyms as attention queries to extract different code-related text snippets for codewise representations. ",
            "24": "Finally, we propose using a biaffine-based similarity of code-wise text representations and code representations for classification.",
            "25": "We conduct experiments on the MIMIC-III dataset with two settings: full codes and top-50 codes. ",
            "26": "Results show that our method performs better than previous state-of-the-art methods. ",
            "27": "We will release our codes for further research.",
            "28": "Approach",
            "29": "Consider free text S (usually discharge summaries) from EMR with words {w i } N i=1 . ",
            "30": "Let C be the ICD codes set, for each code l \u2208 C with code description l 1 from ICD, the task is to assign a binary label y l \u2208 {0, 1} based on S. Figure 1 shows an overview of our method.",
            "31": "Code Synonyms",
            "32": "We extend the code description l 1 by synonyms from the medical knowledge graph (i.e., UMLS Metathesaurus). ",
            "33": "We first align the code to the Concept Unique Identifiers (CUIs) from UMLS. ",
            "34": "Then we select corresponding synonyms of English terms from UMLS with same CUIs and add additional synonyms by removing hyphens and the word \"NOS\" (Not Otherwise Specified). ",
            "35": "We denote the code synonyms as {l 2 , ..., l m } in which each code synonym l j is composed of words {l j i }",
            "36": "Encoding",
            "37": "Previous works (Ji et al., 2021;Pascual et al., 2021) have shown that pretrained language models like BERT (Devlin et al., 2019) cannot help the ICD coding performance, hence we use an LSTM (Hochreiter and Schmidhuber, 1997) as our encoder. ",
            "38": "We use pre-trained word embeddings to map words w i to x i . ",
            "39": "A d-layer bi-directional LSTM layer with output size h is followed by word embeddings to obtain text hidden representations H.",
            "40": "For code synonym l j , we apply the same encoder with a max-pooling layer to obtain representation q j \u2208 R h .",
            "41": "Multi-synonyms Attention",
            "42": "To interact text with multiple synonyms, we propose a multi-synonyms attention inspired by the multi-head attention (Vaswani et al., 2017). ",
            "43": "We split",
            "44": "Then, we use code synonyms q j to query H j . ",
            "45": "We take the linear transformations of H j and q j to calculate attention scores \u03b1 j l \u2208 R N . ",
            "46": "Text related to code synonym l j can be represented by H\u03b1 j l . ",
            "47": "We aggregate code-wise text representations v l \u2208",
            "48": "Figure 1: The architecture of our proposed MSMN.",
            "49": "Different colors indicate different code synonyms. ",
            "50": "We also split hidden representations into different heads for multi-synonyms attention.",
            "51": "R h using max-pooling of H\u03b1 j l since the text only needs to match one of the synonyms.",
            "52": "Classification",
            "53": "We classify whether the text S contains code l based on the similarity between code-wise text representation v l and code representation. ",
            "54": "We aggregate code synonym representations {q j } to code representation q l \u2208 R h by max-pooling. ",
            "55": "We then propose using a biaffine transformation to measure the similarity for classification:",
            "56": "Previous works (Mullenbach et al., 2018;Vu et al., 2020) classify codes via 3 :",
            "57": "Their work need to learn code-dependent parameters [w l ] l\u2208C \u2208 R \u2225C\u2225\u00d7h for classification, which suffers from training rare codes. ",
            "58": "On the contrary, our biaffine function that replaces Wq l to w l only needs to learn code-independent parameters W \u2208 R h\u00d7h .",
            "59": "Training",
            "60": "We optimize the model using binary cross-entropy between predicted probabilities \u0177l and labels y l :",
            "61": "Precision@N Macro Micro Macro Micro P@8 P@15 CAML (Mullenbach et al., 2018) 89.5 98.6 8.8 53.9 70.9 56.1 MSATT-KG (Xie et al., 2019) 91.0 99.2 9.0 55.3 72.8 58.1 MultiResCNN (Li and Yu, 2020) 91.0 98.6 8.5 55.2 73.4 58.4 HyperCore (Cao et al., 2020) 93.0 98.9 9.0 55.1 72.2 57.9 LAAT (Vu et al., 2020) 91.9 98.8 9.9 57.5 73.8 59.1 JointLAAT (Vu et al., 2020) 92  2020) to truncate discharge summaries at 4,000 words. ",
            "62": "We measure the results using macro AUC, micro AUC, macro F 1 , micro F 1 and precision@k (k = 5 for MIMIC-III 50, 8 and 15 for MIMIC-III full). ",
            "63": "Detailed statistics of the MIMIC-III dataset are listed in Appendix A.",
            "64": "Implementation Details",
            "65": "We sample m = 4 and 8 synonyms per code for MIMIC-III full and MIMIC-III 50 respectively. ",
            "66": "We use the same word embeddings as Vu et al. (2020) which are pretrained on the MIMIC-III discharge summaries using CBOW (Mikolov et al., 2013) with hidden size 100. ",
            "67": "We apply R-Drop with \u03b1 = 5 (Liang et al., 2021) to regularize the model to prevent over-fitting. ",
            "68": "We train MSMN with AdamW (Loshchilov and Hutter, 2019) with a linear learning rate decay. ",
            "69": "We optimize the threshold of classification using the development set.",
            "70": "Baselines",
            "71": "CAML (Mullenbach et al., 2018) uses CNN to encode texts and proposes label attention for coding. ",
            "72": "MSATT-KG (Xie et al., 2019) applies multi-scale attention and GCN to capture codes relations.",
            "73": "MultiResCNN (Li and Yu, 2020) encodes text using multi-filter residual CNN.",
            "74": "HyperCore (Cao et al., 2020) embeds ICD codes into the hyperbolic space to utilize code hierarchy and uses GCN to leverage the code co-occurrence.",
            "75": "LAAT & JointLAAT (Vu et al., 2020) propose a hierarchical joint learning mechanism to relieve the imbalanced labels, which is our main baseline since it is most similar to our work.",
            "76": "Main Results",
            "77": "Table 1 and 2 show the main results under the MIMIC-III full and MIMIC-III 50 settings, respectively. ",
            "78": "Under the full setting, our MSMN achieves 95.0 (+2.0), 99.2 (+0.0), 10.3 (-0.4), 58.4 (+0.9), 75.2 (+1.4), and 59.9 (+0.8) in terms of macro-AUC, micro-AUC, macro-F 1 , micro-F 1 , P@8, and P@15 respectively (parentheses shows the differences against previous best results), which shows that MSMN obtains state-of-the-art results in most metrics. ",
            "79": "Under the top-50 codes setting, MSMN performs better than LAAT in all metrics and achieves state-of-the-art scores of 92.8 (+0.3), 94.7 (+0.1), 68.3 (+1.7), 72.5 (+0.9), 68.0 (+0.5) on macro-AUC, micro-AUC, macro-F 1 , micro-F 1 , and P@5, respectively. ",
            "80": "We notice that the macro F 1 has large variance in MIMIC-III full setting because it is more sensitive in a long tail problem.",
            "81": "Discussion",
            "82": "To explore the influence of leveraging different numbers of code synonyms, we search m among {1, 2, 4, 8, 16} on the MIMIC-III 50 dataset. ",
            "83": "Results are shown in aging more synonyms from UMLS consistently improves the performance. ",
            "84": "Using m = 4, 8 achieves the best performances in AUC, and m = 8 achieves the best performances in terms of F 1 and P@5. ",
            "85": "In addition, the median and mean count of UMLS synonyms are 5.0 and 5.4 respectively, which echoes why the results of m = 4 or 8 are better.",
            "86": "To evaluate the effectiveness of our proposed biaffine-based similarity function, we compare it with the baseline LAAT in Table 3. ",
            "87": "We also provide a simple function by removing W to v T l q l in Equation 7. ",
            "88": "Results show the biaffine-based similarity scoring performs best among others.",
            "89": "To better understand what MSMN learns from the multi-synonyms attention, we plot the synonym representations q j under MIMIC-III 50 setting via t-SNE (van der Maaten and Hinton, 2008) in Figure 2. ",
            "90": "We observe for some codes like 585.9 (\"chronic kidney diseases\"), all synonym representations cluster together, which indicates that synonyms extract similar text snippets. ",
            "91": "However, codes like 410.71 (\"subendocardial infarction initial episode of care\" or \"subendo infarct, initial\") and 403.90 (\"hypertensive chronic kidney disease, unspecified, with chronic kidney disease stage i through stage iv\" or \"unspecified orhy kid w cr kid i iv\") with very different synonyms learn different representations, which benefits to match different text snippets. ",
            "92": "Furthermore, we observe it has similar representations for sibling codes 37.22 (\"left heart cardiac catheterization\") and 37.23 (\"rt/left heart card cath\"), which indicates the model can also implicitly capture the code hierarchy.",
            "93": "Related Work",
            "94": "Automatic ICD coding is an important task in the medical NLP community. ",
            "95": "Earlier works use ma-",
            "96": "Conclusions",
            "97": "In this paper, we propose MSMN to leverage code synonyms from UMLS to improve the automatic ICD coding. ",
            "98": "Multi-synonyms attention is proposed for extracting different related text snippets for code-wise text representations. ",
            "99": "We also propose a biaffine transformation to calculate similarities among texts and codes for classification. ",
            "100": "Experiments show that MSMN outperforms previous methods with label attention and achieves state-ofthe-art results in the MIMIC-III dataset. ",
            "101": "Ablation studies show the effectiveness of multi-synonyms attention and biaffine-based similarity."
        }
    },
    "eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88": {
        "doc1": {
            "0": "paper_summary:",
            "1": "This submission proposed a model for offensive span detection (OSD).",
            "2": "Specifically, the authors train GPT-2 in a dual-training setting with reinforcing learning to generate synthetic training data for OSD.",
            "3": "summary_of_strengths:",
            "4": "- Good writing and well organized",
            "5": "- Interesting topic of offensive span detection",
            "6": "summary_of_weaknesses:",
            "7": "- Limited novelty: simple application via utilizing GPT-2 to generate data of a specific domain",
            "8": "- Insufficient evaluation: - only one dataset is used - lack of detailed dataset statistics - the comparison is not persuasive",
            "9": "comments,_suggestions_and_typos:",
            "10": "Although OSD is an interesting topic, the submission should address the following issues before it can be published:",
            "11": "- Only one dataset is used for evaluation, is there anything more?",
            "12": "- Is it possible to add more evaluations about the synthetic data itself?",
            "13": "In the current paper, the comparison is between the whole pipeline and other baselines.",
            "14": "Not sure how much performance is made by the data itself, or the training mode itself?",
            "15": "- Is it possible to generalize the application?",
            "16": "For example, from the OSD to general opinion and aspect detection?",
            "17": "- The method needs the manual labels as the start (the BIO labels mentioned in the experiments), is it possible to add more statistics about this label set?",
            "18": "- The definitions of usefulness and diversity seem quite intuitive.",
            "19": "Any motivations to justify this definition, or is there any alternative form of definition?"
        },
        "doc2": {
            "0": "Data Augmentation with Dual Training for Offensive Span Detection",
            "1": "Abstract",
            "2": "Recognizing offensive text is an important requirement for every content management system, especially for social networks. ",
            "3": "While the majority of the prior work formulate this problem as text classification, i.e., if a text excerpt is offensive or not, in this work we propose a novel model for offensive span detection (OSD), whose goal is to identify the spans responsible for the offensive tone of the text. ",
            "4": "One of the challenges to train a model for this novel setting is the lack of enough tanning data. ",
            "5": "To address this limitation, in this work we propose a novel method in which the large-scale pre-trained language model GPT-2 is employed to generate synthetic training data for OSD. ",
            "6": "In particular, we propose to train the GPT-2 model in a dual-training setting using the REINFORCE algorithm to generate in-domain, natural and diverse training samples. ",
            "7": "Extensive experiments on the benchmark dataset for OSD reveal the effectiveness of the proposed method.",
            "8": "Introduction",
            "9": "It's no secret that social networks are growing in popularity. ",
            "10": "However, growth in popularity also brings some challenges, including the toxicity associated with the content posted by users. ",
            "11": "It may take different forms in social media, including insults, mockery, threats, discrimination, or swearing. ",
            "12": "The presence of offensive text in social networks can have a detrimental effect on their users, making it desirable to identify and remove them from the text.",
            "13": "Since this is an important requirement, the task of offensive language detection has been extensively studied in NLP community (Schmidt and Wiegand, 2017;Wulczyn et al., 2017;Feng et al., 2018;Borkan et al., 2019;Pavlopoulos et al., 2019;Sivanaiah et al., 2020;Yasaswini et al., 2021) Most existing works, however, only classify a text snippet as offensive or not, failing to provide further information on which specific words and phrases in the text contribute the most to its offensive tone. ",
            "14": "If the text snippet is lengthy, the moderators will need this information to decide how to proceed with the offenses flagged. ",
            "15": "As such, in this work, we fill this gap by proposing a novel model for the task of offensive span detection (OSD). ",
            "16": "As an example, in the given text \"This live streamer clearly has no brain; he is such a tool!\", the phrase \"has no brain\" and the slang word \"tool\" are two offensive spans responsible for the toxicity of the text. ",
            "17": "One of the barriers to this task is the lack of labeled data. ",
            "18": "Inspired by the recent advances in the application of pre-trained language models to augment training data for low-resources tasks (Zhang et al., 2020;Yang et al., 2020;Peng et al., 2020;Kumar et al., 2020;Anaby-Tavor et al., 2020), we propose to employ the GPT-2 model to overcome the data scarcity of OSD. ",
            "19": "To address this limitation, we propose a novel model in which the OSD training data are augmented with the synthetic samples generated by a transformer-based language model. ",
            "20": "In particular, the original labeled samples of OSD, with special markers before and after each offensive span, are employed to fine-tune the parameters of the GPT-2 model to generate sentences containing offensive spans. ",
            "21": "Moreover, in order to increase the quality of the generated samples, we propose to explicitly encourage the GPT-2 model to generate diverse sentences while keeping them similar to the original training samples. ",
            "22": "Also, the model is encouraged to generate sentences that will result in improvement of the performance of the OSD task. ",
            "23": "To fulfill these objectives, in a dual training setting, the REINFORCE algorithm (Williams, 1992) is exploited to train the GPT-2 model. ",
            "24": "We evaluate the proposed model on a recently released dataset for offensive span detection. ",
            "25": "Our extensive experiments show the effectiveness of the proposed model by outperforming the strong baselines.",
            "26": "Model",
            "27": "Formal Task Description: The input to the model is the document D = [w 1 , w 2 , . . . , w n ] consisting of n words. ",
            "28": "The label provided for the document is also the sequence Y = [y 1 , y 2 , . . . , y n ] in which y i is the label for the word w i in BIO format. ",
            "29": "This problem is modeled as a sequence labeling task in which the model predicts the label of every word w i in the document D. In this work, we propose a method to augment the original training samples O, with synthetic labeled text G generated by a fine-tuned GPT-2 model. ",
            "30": "The rest of this section describes the base model and the data augmentation process.",
            "31": "Base Model",
            "32": "In our approach, we employ the pre-trained BERT base transformer as the base sequence labeling model which is trained on D = O G. Specifically, the document D \u2208 D is fed into the BERT model in the form of [CLS]w 1 w 2 . . . ",
            "33": "w n [SEP ] to obtain the word representations X = [x 1 , x 2 , . . . , x n ]. ",
            "34": "Note that for the words consisting of multiple word pieces we take the average of their corresponding word-piece representations. ",
            "35": "Next, the representations x i are sent to a feed-forward network to predict the label distribution P (\u2022|D, \u03b8), where \u03b8 is the parameters of the BERT model. ",
            "36": "To train the model, we employ the negative log-likelihood:",
            "37": "where y j is the gold label for j-th word of the document D i .",
            "38": "Data Augmentation",
            "39": "One of the limitations for OSD is the lack of enough labeled data. ",
            "40": "To address this limitation, inspired by the success of the generative language models to augment data for other tasks, we propose to employ GPT-2 to generate labeled synthetic data. ",
            "41": "We first discuss the generation process, then we provide details on how the generative model is encouraged to generate high-quality data. ",
            "42": "Generation: Following prior works (Zhang et al., 2020), to generate synthetic data we employ GPT-2 (Radford et al., 2019) model. ",
            "43": "GPT-2 is a transformer-based language model pre-trained on 40 GB of textual data. ",
            "44": "In order to fine-tune GPT-2 for generating labeled data for OSD, we propose to employ the original labeled data G. Specifically, the document D \u2208 G is first augmented with special tokens at the beginning and the end of the document and also around the offensive spans:",
            "45": "where t is the length of the offensive span in D. Note that there might be multiple offensive span in a document. ",
            "46": "Next, the GPT-2 model is trained in an auto-regressive manner on the labeled augmented documents D . ",
            "47": "Specifically, the following loss is employed for the fine-tuning process:",
            "48": "where w j is the j-th word in the label augmented document D i , D <j is the left context of the word w j in the document D i , and \u03b1 is the parameters of the GPT-2 model. ",
            "49": "Finally, the fine-tuned GPT-2 model is employed to generate |O| synthetic data. ",
            "50": "Specifically, the model is prompted with [BOS] token and the generation is stopped by generating the [EOS] token. ",
            "51": "In order to ensure that the generated data are labeled, we keep only the generated samples with at least one pair of [OF F EN SIV E S ] and [OF F EN SIV E E ] tokens. ",
            "52": "The generated samples, i.e., G, are combined with the original samples O, to obtain the final D dataset to train the base model. ",
            "53": "Improving Quality of Generated Samples: While the fine-tuning process of GPT-2 is supposed to be effective to generate high-quality data, it has been shown that the generated data might be noisy or have repeated sentences (Veyseh et al., 2021) Training Procedure: In order to simultaneously update the parameters of the base model and also the GPT-2 model, we propose a dual training procedure. ",
            "54": "Specifically, at the first epoch, the parameters of the GPT-2 model are updated using the loss f . ",
            "55": "Next, GPT-2 model is employed to generate the labeled synthetic data to obtain the combined dataset D. After one epoch of training the base model using the loss L base , the parameters of the GPT-2 model are updated using the REINFORCE algorithm. ",
            "56": "The updated GPT-2 model is employed to generate a new set of synthetic data to be replaced with the previously generated data in D. The new combined data will be next employed to update the base model. ",
            "57": "This process is repeated until the convergence of training.",
            "58": "Experiments",
            "59": "In order to evaluate the effectiveness of the proposed model, called GAOSD (Generation-based Augmentation for Offensive Span Detection), in our experiments, we use the dataset of SemEval 2021 Task 5 (John Pavlopoulos and Laugier, 2021). ",
            "60": "This dataset contains annotations for 10,000 posts (comments) obtained from the archive of Civil Comment platform (a platform for community to share comments about various civility issues). ",
            "61": "We use the official splits with 7939/690/2000 documents in train/development/test sets. ",
            "62": "For each document, the word indices of offensive spans are provided. ",
            "63": "In our experiments, we create the BIO labels (2) BERT+CRF: BERT base parameters are finetuned on OSD task and the task-specific head, i.e., CRF, is employed for label prediction; (3) HITSZ-HLT (Zhu et al., 2021): This baseline is the existing SOTA model on SemEval 2021 Task 5 dataset; (4) SANER (Nie et al., 2020): This baseline is the SOTA model for sequence labeling on usergenerated text; (5) DUAL-MRC (Mao et al., 2021): This is the SOTA model for opinion and aspect term extraction. ",
            "64": "Note that since there are not target annotations in SemEval dataset, we skip the aspect term extraction task to train this baseline. ",
            "65": "To evaluate the performance we use the official metric, i.e. char-level F1-score, as the evaluation metric. ",
            "66": "Following prior work (Zhu et al., 2021), we also report the average of char-level precision and recall (Note that due to averaging, F 1 = 2(P * R)/(P + R)). ",
            "67": "Results: Table 1 shows the performance of the models on the test set. ",
            "68": "There are several observations from this table. ",
            "69": "First, the BiLSTM-CRF model significantly underperforms the other baselines that employ BERT embedding. ",
            "70": "It clearly shows that the background knowledge encoded in the BERT model is necessary for the task of offensive span detection. ",
            "71": "Second, both DUAL-MRC and SANER baseline outperform the BERT-CRF model. ",
            "72": "This higher performance could be attributed to their capability to enhance the representation of the words obtained from the BERT model. ",
            "73": "Third, among all baselines, our proposed model achieves the highest performance. ",
            "74": "Our hypothesis for the achieved improvement is that in the proposed method we employ more diverse sets of patterns for expressing toxic. ",
            "75": "The increased diversity is realized by generating more diverse  sentences. ",
            "76": "Also, this improvement proves that the generated sentences are in-domain and task specific, as such resulting in an improvement. ",
            "77": "The better performance of our model is impressive, especially considering that we use relatively simple base model compared to other baselines (in particular HITSZ-HLT which is an ensemble model).",
            "78": "Analysis: To study the contribution of the proposed techniques, we conduct an ablation study on the development set of the SemEval 2021 Task 5 dataset. ",
            "79": "Specifically, we ablate the quality improvement component which ensures the usefulness and diversity of the generated samples. ",
            "80": "In particular, we study the performance of the model when the Usefulness Reward (UR \u2212 ), the Diversity Reward (DR \u2212 ), or both of them (UDR \u2212 ) are ablated. ",
            "81": "Also, we study the performance of the model when no dual training is employed (DT \u2212 ). ",
            "82": "Specifically, we first pre-train the base model on the available original data. ",
            "83": "Next, we fix the parameters of the base model and we use it to compute the usefulness reward. ",
            "84": "The results are shown in Table 2. ",
            "85": "This table shows that all components are necessary, as removing each will hurt the performance. ",
            "86": "Specifically, the dual training has the largest effect on the final performance, indicating the importance of the proposed method. ",
            "87": "Also, among the two proposed rewards to improve the quality of the generated data, we observe that usefulness reward is more critical, indicating the importance of task-specific generation for data augmentation.",
            "88": "Finally, in order to provide more insight into the quality of the generated data, we provide some randomly selected text generated by the fine-tuned GPT-2 model. ",
            "89": "The results are shown in table 3. ",
            "90": "This table shows that the generated samples are natural and also they contain the offensive spans. ",
            "91": "The generative model is able to correctly locate the offensive spans in the generated text, thereby provided high-quality training samples for the base model. ",
            "92": "It is worth noting that the offensive spans generated by the fine-tuned GPT-2 model can be either short spans, as in samples 1 and 3 in table 3, or longer phrases, as in sample 2.",
            "93": "Related Work",
            "94": "Prior works related to this task can be categorized into two groups: (i) Toxicity Detection: These works aim to classify a piece of text as toxic or nontoxic (Wulczyn et al., 2017;Borkan et al., 2019;Schmidt and Wiegand, 2017;Pavlopoulos et al., 2017aPavlopoulos et al., ,b, 2019Zampieri et al., 2019). ",
            "95": "The main limitation of these works is that they cannot recognize the spans in the text that are responsible for the toxicity of the text. ",
            "96": "(ii) Opinion Word Extraction: In this group of prior works, models perform a sequence labeling task to identify the spans in the text that convey the sentiment (Liu et al., 2015;Xu et al., 2018;Yin et al., 2016;Wang et al., 2016Wang et al., , 2017Li and Lam, 2017;Mao et al., 2021). ",
            "97": "The major limitation of all these models is that they require the existence of the target opinion (i.e., the word or phrase that the text has a sentiment polarity toward it).",
            "98": "Conclusion",
            "99": "In this work, we propose a novel method for augmenting data for offensive span detection tasks. ",
            "100": "Specifically, we employ the pre-trained language model GPT-2 to be fine-tuned on the available training samples for OSD. ",
            "101": "The fine-tuned model is able to generate in-domain texts with special tokens indicating the offensive spans in them. ",
            "102": "Moreover, to improve the quality of the generated documents, we propose a novel dual training setting in which the feedback from the OSD model is employed to guide the GPT-2 model to generate more impact synthetic data. ",
            "103": "Together with a reward for encouraging the diversity of the generated data, the proposed method is effective to augment the training data for OSD, resulting in the state-of-the-art performance on the recent benchmark datasets."
        }
    },
    "3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d": {
        "doc1": {
            "0": "paper_summary:",
            "1": "This paper investigates the aspect sentiment triplet extraction task and optimized bidirectional machine reading comprehension method with 4 improvements.",
            "2": "The evaluation results look good against baseline models.",
            "3": "summary_of_strengths:",
            "4": "The bidirectional machine reading comprehension is suited for this task and the design of span matching and probability generation is useful(through ablation study).",
            "5": "The overall performance is good.",
            "6": "summary_of_weaknesses:",
            "7": "1. In the backward query of Fig 4, why the span is \"must visit\" instead of \"a must visit\" given the probability of the word \"a\" is 86%, which is the same as the word \"must\"?",
            "8": "2. What if the forward query and backward query give conflict prediction?",
            "9": "For example in the example sentence of Fig 2, if the answer to the first query is only \"small\" (without \"good\"), what will happen?",
            "10": "The example in Fig 4 is too simple.",
            "11": "3. What are the differences between two versions of ASTE-Data datasets?",
            "12": "4. The evaluation metrics are not explained, is it an exact match rule: the prediction is correct only if all three elements in the triplet are correct?",
            "13": "comments,_suggestions_and_typos:",
            "14": "The words in Fig 4 is too small. It can hard be recognized is the paper is printed out on an A4 paper.",
            "15": "The variables in formula 1 and 2 should be explained, for example, what is pair_asp."
        },
        "doc2": {
            "0": "A Robustly Optimized BMRC for Aspect Sentiment Triplet Extraction",
            "1": "Abstract",
            "2": "Aspect sentiment triplet extraction (ASTE) is a challenging subtask in aspect-based sentiment analysis. ",
            "3": "It aims to explore the triplets of aspects, opinions and sentiments with complex correspondence from the context. ",
            "4": "The bidirectional machine reading comprehension (BMRC) can effectively deal with ASTE task, but several problems remains, such as query conflict and probability unilateral decrease. ",
            "5": "Therefore, this paper presents a robustly optimized BMRC method by incorporating four improvements. ",
            "6": "The word segmentation is applied to facilitate the semantic learning. ",
            "7": "Exclusive classifiers are designed to avoid the interference between different queries. ",
            "8": "A span matching rule is proposed to select the aspects and opinions that better represent the expectations of the model. ",
            "9": "The probability generation strategy is also introduced to obtain the predicted probability for aspects, opinions and aspectopinion pairs. ",
            "10": "We have conducted extensive experiments on multiple benchmark datasets, where our model achieves the state-of-the-art performance.",
            "11": "Introduction",
            "12": "Aspect-based sentiment analysis (ABSA) is an important research area of natural language processing (NLP), which aims to mine fine-grained opinions and sentiments based on a specific aspect. ",
            "13": "In recent years, it has attracted extensive attention of researchers (Hu and Liu, 2004). ",
            "14": "ABSA includes three basic subtasks: aspect term extraction (Yin et al., 2016;Li et al., 2018;Ma et al., 2019), opinion term extraction (Liu et al., 2015;Wu et al., 2021), and aspect level sentiment classification (Wang et al., 2016;Chen et al., 2017;Jiang et al., 2019;Zhang and Qian, 2020).",
            "15": "Substantial progress has been achieved in recent studies, integrating multiple subtasks into a more complex task (Chen and Qian, 2020;He et al., 2019;Luo et al., 2019;Zhao et al., 2020). ",
            "16": "Among them, aspect sentiment triplet extraction (ASTE) (Peng et al., 2020) becomes a subject of great interest, which is also the goal of our work. ",
            "17": "The details of ASTE task are shown in Figure 1. ",
            "18": "Many research efforts have been made (Xu et al., 2021;Mao et al., 2021;Chen et al., 2021), for example, using bidirectional machine reading comprehension (BMRC) for ASTE. ",
            "19": "It is a great work, but problems still remain. ",
            "20": "In the structure of BMRC, the shared classifiers may lead to query conflicts based on specific context, thus affecting the model performance. ",
            "21": "Some important strategies are also ignored, such as word segmentation, span matching and probability generation.",
            "22": "In this paper, we present a robustly optimized BMRC method for ASTE. ",
            "23": "The task is transformed into a machine reading comprehension problem. ",
            "24": "The complex correspondence between the aspect and opinion is processed through bidirectional query based on specific context. ",
            "25": "Such relationship can be effectively used to make their extraction mutually beneficial, thus facilitating better prediction of various sentiments. ",
            "26": "In order to deal with the ASTE task more efficiently, we incorporate the word segmentation and exclusive classifiers, and improve the span matching where the priority rule of the combination of probability and position relationship has been added. ",
            "27": "We also optimize the generation of probability to avoid its unilateral decrease. ",
            "28": "Our contributions can be summarized as follows:",
            "29": "\u2022 Exclusive classifiers are designed in BMRC, so as to avoid the interference between dif-ferent question answering steps and the query conflict.",
            "30": "\u2022 We further advance the prediction performance by adding word segmentation, improving span matching and probability generation.",
            "31": "\u2022 Extensive experiments are conducted on four benchmark datasets, where our model achieves the state-of-the-art performance.",
            "32": "Methodology",
            "33": "In this section, we briefly review the BMRC (Chen et al., 2021), and then introduce our four improvements in detail.",
            "34": "BMRC",
            "35": "BMRC can put forward the corresponding query according to the context, and the model then outputs the desired answer.",
            "36": "Forward Query BMRC will query all aspects based on context; Then, according to the aspect of each prediction, all opinions describing it are queried from the context. ",
            "37": "Backward Query BMRC will query all opinions based on context; Then, according to the opinion of each prediction, all aspects describing it are queried from the context. ",
            "38": "Sentiment Prediction Once the aspect-opinion pairs are obtained, the sentiment queries can be constructed to predict the sentiments of the corresponding pairs according to the context. ",
            "39": "After that, the sentiments and aspect-opinion pairs are combined into triplets. ",
            "40": "The whole process is illustrated in Figure 2.",
            "41": "Word Segmentation",
            "42": "We use the tokenizer based on wordpiece in BERT (Devlin et al., 2019) to segment words into subwords. ",
            "43": "Wordpiece is a common technique for word segmentation in NLP tasks.",
            "44": "The role of word segmentation has been investigated. ",
            "45": "Suppose the word \"walking\" is fed into the model, unless it appears many times in the training corpus, the model may fail to handle the word well. ",
            "46": "When similar words like \"walked\", \"walker\" or \"walks\" show up, without word segmentation, they will be treated as completely different words. ",
            "47": "However, if they are subdivided into \"walk ##ing\", \"walk ##ed\", \"walk ##er\", and \"walk ##s\", their sub-word \"walk\" contains the same semantics which is quite common during training. ",
            "48": "In this sense, the model is able to learn more information through word segmentation.",
            "49": "Exclusive Classifiers",
            "50": "Bidirectional queries are performed in BMRC, and the model needs to perform multiple different types of queries based on context. ",
            "51": "For example, the aspect query in forward query is different from the opinion query in backward query. ",
            "52": "The former queries all the aspects in the context, while the latter queries all the opinions in the context, requiring different entities. ",
            "53": "Another example is the aspect query in the forward query and the aspect query in the backward query. ",
            "54": "Although the entities of the two queries are the same, the latter conveys opinion information and searches for all the aspects described by it, while the former does not carry any context information, namely, all the aspects in the context.",
            "55": "In the original BMRC, all queries share one classifier. ",
            "56": "However, if different types of queries use the same classifier, it cannot serve any part very well. ",
            "57": "These different types of queries will interfere with each other and cause the query conflict. ",
            "58": "By adding exclusive classifiers, each different type of query can use a unique classifier, as shown in Figure 3, which can effectively avoid the problem of query conflict and greatly improve the performance of the model.",
            "59": "Span Matching",
            "60": "Recently, there is a lot of work to deal with ABSA tasks based on span extraction (Hu et al., 2019;Xu et al., 2021), so does BMRC. ",
            "61": "After obtaining the predicted value of each position as the start or end position of span through binary classifiers, the predicted value is converted into probability using softmax function (Chen et al., 2021).",
            "62": "When predicting the span, many start and end positions may be predicted, and how to match them is very important, since the matching rule will seriously affect the performance of the model. ",
            "63": "The matching should consider the relationship between probability and position, because the value of the former represents the optimistic degree of the model for the position, while the latter is the judgment of the cognition that the start and end positions of span are as close as possible, and the priority of probability is higher. ",
            "64": "So, the overview of our span matching rule is: make each end position match the start position with the highest probability after the previous end position. ",
            "65": "If there is a start position with the same probability, select the one whose position is closest to the end position.",
            "66": "Probability Generation",
            "67": "Once the bidirectional queries and span matching are completed, aspects, opinions and pairs with corresponding relationship are obtained. ",
            "68": "In BMRC, the probability product of the start and end positions is taken as the probability of the span, and the probability of pair is the probability product of aspect and opinion. ",
            "69": "In this way, the probability of pair decreases unilaterally and cannot well represent the prediction of the pair by the model. ",
            "70": "For example, the probability of the four positions of pair is 0.9, while the probability of pair is 0.9 4 = 0.6561, which seems not so reasonable.",
            "71": "By probability generation, we can effectively solve the problem of unilateral decrease in the probability of span and pair, so that their probability can better reflect the expectation of the model. ",
            "72": "The operations are shown in Equation 1 and 2, where we balance the probability of span and pair so that their probability is within the interval of the two related probabilities. ",
            "73": "It enables us to avoid the unilateral decrease of probability, but keep more appropriate to the expectation of the model. ",
            "74": "(2)",
            "75": "The effects of span matching and probability generation are shown in Figure 4.",
            "76": "Experiment",
            "77": "In this section, we introduce information about the experiments, including datasets, evaluation metrics, baselines, experimental results, and ablation study.",
            "78": "Datasets",
            "79": "We evaluate the model performance on ASTE-Data-v1 (Peng et al., 2020) and ASTE-Data-v2 , which are popular benchmark datasets for ASTE task. ",
            "80": "They are derived from Laptop14, Rest14, Rest15, and Rest16 of SemEval shared challenges (Pontiki et al., 2014(Pontiki et al., , 2015(Pontiki et al., , 2016.",
            "81": "Results",
            "82": "We focus on the ASTE task. ",
            "83": "We conducted many experiments on the ASTE-Data-v1 and ASTE-Data-v2 datasets. ",
            "84": "The experimental results are shown in Tables 1, 2 respectively. ",
            "85": "In order to make a fair comparison with baselines, our F1 scores appeared at least three times in the experiments.",
            "86": "It is worth noting that we have achieved state-ofthe-art performances on the ASTE-Data-v1 and ASTE-Data-v2 datasets, indicating that our improvement further improves the performance of BMRC in dealing with ASTE task. ",
            "87": "In the Lap-top14, Rest14, Rest15, and Rest16 datasets of ASTE-Data-v1, the F1 scores of our improved model are increased by 2.97, 4.20, 5.61 and 5.52 respectively compared with the original BMRC, indicating that our improvement is very effective. ",
            "88": "On Table 3: The performance of our four improvements on the ASTE-Data-v2  dataset. ",
            "89": "the Laptop14, Rest14, Rest15, and Rest16 datasets of ASTE-Data-v2, we also increased the F1 scores of the Strong baseline Span-ASTE (Xu et al., 2021) by 2.74, 0.77, 2.36 and 2.90 respectively. ",
            "90": "This indicates that our improvement is very significant.",
            "91": "Ablation Study",
            "92": "Firstly, we experiment the model without improvement on the ASTE-Data-v2. ",
            "93": "The model is a reproduction based on BMRC, and then gradually superimposes the four improvements of word segmentation, exclusive classifiers, span matching and probability generation to conduct ablation experiment. ",
            "94": "This arrangement corresponds to the sequence before and after they contact the data, that is, the data will first pass through word segmentation and enter the model, the prediction value is obtained from the exclusive classifiers, and then span matching is carried out according to it. ",
            "95": "Finally, the probability generation is used to generate probabilistic representations of aspects, opinions and pairs. ",
            "96": "The datasets and various parameters of the five experiments are the same. ",
            "97": "In order to make a fair comparison with baselines, our F1 scores appear at least three times in the experiments. ",
            "98": "The ablation experimental results are shown in Table 3. ",
            "99": "Each improvement advances the performance of the model, demonstrating their advantages and effectiveness.",
            "100": "Conclusion",
            "101": "In this paper, we propose several improvements on the basis of BMRC for ASTE task, which can effectively deal with the complex correspondence among aspect, opinion and sentiment. ",
            "102": "In order to deal with the problems of the original BMRC, we add exclusive classifiers and three strategies, including word segmentation, span matching and probability generation. ",
            "103": "The proposed method is expected to handle complex ASTE task more efficiently. ",
            "104": "Extensive experiments are conducted to demonstrate the advantages of our improvements."
        }
    },
    "198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce": {
        "doc1": {
            "0": "paper_summary:",
            "1": "This paper proposes add copy probability in a transformer-based model to improve the generation quality on question answering task.",
            "2": "Specifically, the attention distribution from the last encoder-decoder attention layer is used as the copy probability.",
            "3": "In addition, the word embedding and the output representation are used together to compute generation probability.",
            "4": "The experiment results show the proposal method outperforms baseline model on two benchmark datasets.",
            "5": "summary_of_strengths:",
            "6": "1. The paper is well organized, and it explains the proposed solution clearly.",
            "7": "summary_of_weaknesses:",
            "8": "1. The idea is incremental.",
            "9": "Applying the copy mechanism in the point generator paper on transformer model has been studied in many papers. [1] Deaton, Jon, et al. \"Transformers and pointer-generator networks for abstractive summarization.\" ( 2019). [2] Prabhu, Nikhil, and Katharina Kann. \" Making a Point: Pointer-Generator Transformers for Disjoint Vocabularies.\"",
            "10": "2. The experimental results are not convincing enough. Only compared to the base version of FiD/FiD-KD model, and lack of result on a large version.",
            "11": "comments,_suggestions_and_typos:",
            "12": "1. What\u2019s the performance of the proposed method on the Yes/No answer which cannot be extracted from passage?"
        },
        "doc2": {
            "0": "A Copy-Augmented Generative Model for Open-Domain Question Answering",
            "1": "Abstract",
            "2": "Open-domain question answering is a challenging task with a wide variety of practical applications. ",
            "3": "Existing modern approaches mostly follow a standard two-stage paradigm: retriever then reader. ",
            "4": "In this article, we focus on improving the effectiveness of the reader module and propose a novel copy-augmented generative approach that integrates the merits of both extractive and generative readers. ",
            "5": "In particular, our model is built upon the powerful generative model FiD (Izacard and Grave, 2020b). ",
            "6": "We enhance the original generative reader by incorporating a pointer network to encourage the model to directly copy words from the retrieved passages. ",
            "7": "We conduct experiments on the two benchmark datasets, Natural Questions and TriviaQA, and the empirical results demonstrate the performance gains of our proposed approach.",
            "8": "Introduction",
            "9": "Open-domain question answering (ODQA) focuses on providing highly precise answers to natural language questions from a large collection of unstructured text data (Voorhees, 1999). ",
            "10": "With the pioneering work of DrQA (Chen et al., 2017), modern approaches to ODQA commonly adopt a simple two-stage retriever-reader pipeline, that firstly retrieve a relatively small number of support passages (Karpukhin et al., 2020;Yamada et al., 2021;Min et al., 2021b), followed by the reader identifying the answer.",
            "11": "The reader models can be broadly categorized into two classes: extractive (Chen et al., 2017;Asai et al., 2019;Karpukhin et al., 2020) and generative (Lewis et al., 2020a;Izacard and Grave, 2020b;Wu et al., 2021). ",
            "12": "Recently, benefiting from the powerful ability of large-scale pre-trained encoder-decoder language models (Raffel et al., 2019;Lewis et al., 2019) and the capability of aggregating information from multiple passages (Izacard and Grave, 2020b), generative approaches  have achieved in general better performance than extractive methods.",
            "13": "Compared to extractive models, generative models generate text more freely, which makes it often suffer from the problem of producing hallucinated text that is inconsistent to the input or factual inaccuracy. ",
            "14": "This problem has been addressed in tasks like text summarization and machine translation (Maynez et al., 2020;Zhou et al., 2021). ",
            "15": "We found that the phenomenon also happens in ODQA. ",
            "16": "As shown in Table 1, the answer \"Dubai in Germany\" produced by the generative model FiD (Izacard and Grave, 2020b) is factual incorrect and the answer \"33\" in the second example is not coherent to the question. ",
            "17": "While in both cases, the ground-truth answers are present in the retrieved passages. ",
            "18": "Thus, we hypothesize that if we could put a constraint on the produced words to the input text, the generated answer will be more faithful.",
            "19": "Inspired by the work of See et al. (2017), we enhance the generative model with a pointer network (Vinyals et al., 2017), that enables the model Figure 1: The overall architecture of our proposed model. ",
            "20": "We add a linear layer to calculate the generation probability, which decides the weights of generating words from vocabulary or copying from source passages.",
            "21": "to directly copy text from the retrieved passages while retains the ability of generating new words when the true answers are not explicitly present in the input. ",
            "22": "To be more specific, our model fusionin-decoder pointer-generator network (FiD-PGN) is built upon the state-of-the-art model FiD. We reuse the encoder-decoder attention scores as the copy distribution to reduce the computational cost. ",
            "23": "Compared to FiD, we achieve comparative or even better accuracy on the Natural Questions (NQ) (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017) benchmarks, with less passages used in training. ",
            "24": "Our experiments results show the effectiveness and efficiency of our model.",
            "25": "Related Work",
            "26": "Open-Domain Question Answering",
            "27": "In this era of data explosion, ODQA offers a way to rapidly and accurately fulfill user's information needs, and hence has recently received significant attention from both industry and academia (Min et al., 2021a). ",
            "28": "Following the work of DrQA (Chen et al., 2017), most recent works build a two-stage retriever-reader system to tackle the problem. ",
            "29": "The retriever aims at retrieving supportive passages to the given question from a large document corpus. ",
            "30": "The reader intends to find answer of the question from the first stage retrieved passages. ",
            "31": "Early work of Chen et al. (2017) adapts a BiLSTM architecture with various lexical and semantic features from the question and passages as inputs. ",
            "32": "Later, with the emergence of large-scale pre-trained language models, readers based on pre-trained models such as BERT and T5 Raffel et al., 2019) have become a common approach (Yang et al., 2019;Karpukhin et al., 2020;Izacard and Grave, 2020b).",
            "33": "Generative Readers",
            "34": "Compared to extractive models which extract existing words from the retrieved passages, generative models are able to produce new words out of the retrieved passages, and thus provide a more flexible modeling framework. ",
            "35": " and Lewis et al. (2020a) concatenate the given question with top retrieved passages and feed the concatenation to the BART model (Lewis et al., 2019). ",
            "36": "Izacard and Grave (2020b) separately encodes the question with each top retrieved passage, then takes the concatenation of the encoder outputs as input to the decoder. ",
            "37": "Their method provide a way to better aggregate evidence from multiple passages and improve the performance significantly. ",
            "38": "FiD-KD (Izacard and Grave, 2020a) is an extension of FiD model that increases the accuracy of passage retrieval by training the dense retriever with the guidance of the FiD reader iteratively.",
            "39": "Pointer-Generator Network",
            "40": "Pointer-Generator Network (See et al., 2017) is an extension of the sequence-to-sequence model by integrating a copy mechanism (Vinyals et al., 2017) into the generator. ",
            "41": "At each decoding stage, the model is able to either directly copy a word from the input or generate one with certain probability, and thus can be viewed as a combination of extractive and generative approaches. ",
            "42": "It has been frequently used in natural language tasks like summarization (Gu et al., 2016;See et al.,  Gehrmann et al., 2018) and neural machine translation (Luong et al., 2014;Gu et al., 2017), but its application to ODQA has been less explored.",
            "43": "Method",
            "44": "Our model follows the standard two-stage retrieverreader framework with a focus on the enhancement of the reader module built upon the FiD model. ",
            "45": "We adopt the retriever results of FiD-KD, where a dense retriever similar to DPR (Karpukhin et al., 2020) is used. ",
            "46": "A pointer network is integrated into the FiD reader to facilitate copying words from the retrieved passages. ",
            "47": "The overall reader architecture is depicted in Figure 1.",
            "48": "Reader Encoder. ",
            "49": "The reader encoder of our model is identical to the one of FiD reader. ",
            "50": "We firstly concatenate the given question q with each retrieved passage p i as x i = [q; p i ]. ",
            "51": "Next, we pass each x i individually to the reader encoder, i.e., the encoder of T5 or BART model, and obtain the hidden representations h i = h i,1 , h i,2 , . . . , h i,n of the question-passage pair where h i,j \u2208 R d and d is the model dimension. ",
            "52": "Finally, we concatenate all the hidden representations {h 1 , . . . , h k } as input to the decoder.",
            "53": "Reader Decoder. ",
            "54": "Our approach mainly differs from FiD reader in the decoder module by adding a pointer network. ",
            "55": "Specifically, at each decoding step t, let e t \u2208 R d be the embedding vector of the input token at this step, and denote s L t \u2208 R d as the output representation of the last layer L of transformer decoder, then the probability of generation is given as follows,",
            "56": "where w e \u2208 R d , w s \u2208 R d and b \u2208 R are all learnable parameters and \u03c3(\u2022) represents the sigmoid function. ",
            "57": "In addition, the probability of copying is 1 \u2212 p gen .",
            "58": "Next, let V denote the vocabulary containing words for the generative model and |V| be the size of the vocabulary. ",
            "59": "Then at step t, the probability distribution of words generation over the vocabulary is computed as,",
            "60": "where W E \u2208 R |V |\u00d7d is a learnable weight matrix.",
            "61": "Benefiting from the encoder-decoder attention layer in transformer architecture, we directly utilize the cross-attention score \u03b1 L t of the last decoder layer L over the source tokens for the target token y t as copy distribution. ",
            "62": "Then the probability of selecting y t in source sequence is calculated as,",
            "63": "where x 1:k denotes the concatenation of the top-k retrieved passages, x 1:k,j is the j-th token of x 1:k , and \u03b1 L t,j is the j-th element of \u03b1 L t . ",
            "64": "If y t is not present in the top-k retrieved passages, the P ctx (y t ) will be zero.",
            "65": "Finally, put all the above together, the target token y t could both be generated from vocabulary with probability p gen , and copy from the source passages. ",
            "66": "The final prediction probability is defined as",
            "67": "Experiments",
            "68": "Datasets",
            "69": "We evaluate the performance of our approach on two standard ODQA datasets, NQ and TriviaQA. ",
            "70": "The NQ dataset comprises real queries that user issued on Google search engine along with answers. ",
            "71": "The TriviaQA dataset consists of question-answer pairs collected from trivia and quiz-league websites. ",
            "72": "The details of data statistics are listed at Appendix A. We use the data released on the repository of FiD 1 , containing question-answer pairs and top-100 passages retrieved by FiD-KD.",
            "73": "Implementation Details",
            "74": "We follow the experimental settings as in FiD. Our model is initialized with a pre-trained T5-base model, and trained using AdamW (Loshchilov and Hutter, 2017) algorithm with a learning rate of 10 \u22124 , linear scheduling with 15k total steps and 1k warm-up steps. ",
            "75": "Moreover, we train our model using the top-25 retrieved passages for each question and set the batch size as 64 due to computational limitation. ",
            "76": "All experiments are run on eight Nvidia V100 32GB GPUs.",
            "77": "Results",
            "78": "Table 2 shows the experimental results of our model and other approaches on the test sets, evaluated with the standard exact match (EM) score (Rajpurkar et al., 2016). ",
            "79": "For a fair comparison, we retrained the FiD reader on the top-25 retrieved passages to match our experimental settings. ",
            "80": "We show the results of different number of passages in Appendix B.",
            "81": "As shown in Table 2, our model outperforms FiD-KD on both NQ and TriviaQA datasets under the same setting. ",
            "82": "This demonstrates that the pointer network could help to generate answers more accurately. ",
            "83": "It is worth noting that, compared with FiD-KD trained with the top-100 retrieved passages, our model achieves comparative or even better results with only 1/4 of the input data and without introducing many parameters (only 1537 extra parameters are added), indicating the efficiency of our model.",
            "84": "Analysis",
            "85": "Generation Probability. ",
            "86": "We explore the proba-  bility of generation during training to further investigate the effects of the pointer module. ",
            "87": "As shown in Figure 2, the generation probability p gen in TriviaQA is always higher than the one in NQ.",
            "88": "Note that a higher generation probability means that more tokens are produced from the vocabulary instead of copying from the input. ",
            "89": "We conjecture that this phenomenon is caused by the different question types. ",
            "90": "As stated in Rogers et al. (2021), Trivia questions are more like probing questions.",
            "91": "Compared to the information-seeking questions in NQ, probing questions tend to need more complex reasoning, and thus it is difficult to directly extract relevant tokens from input texts. ",
            "92": "Moreover, this observation is also consistent with the results that the improvements of our model over FiD reader is smaller in TriviaQA than the one in NQ (0.9 vs. 2.9 EM for TriviaQA and NQ, respectively).",
            "93": "Test-Train Overlap Evaluation. ",
            "94": "The study of test-train overlap (Lewis et al., 2020b) provides valuable insights into the model's question answering behavior. ",
            "95": "We evaluate our model on the same test data splits as in Lewis et al. (2020b). ",
            "96": "Table 3 reports the results with respect to three kinds of test-train overlaps. ",
            "97": "It can be seen that our approach improves most over FiD reader on \"No Overlap\" category, the most challenging setting, indicating a better generalization ability to question answering.",
            "98": "Conclusion",
            "99": "In this article, we propose a novel FiD-PGN approach for the reader module of ODQA under the standard retriever-reader framework. ",
            "100": "Specifically, we integrate a pointer network into the FiD reader to allow the model to directly select words from the retrieved passages. ",
            "101": "Experimental results show that our model outperforms FiD-KD on two benchmark datasets under the same setting, demonstrating the advantages of our method.",
            "102": "Figure 3 shows the performance of our model and FiD reader with regard to different number of retrieved training passages. ",
            "103": "We train both models with top-k passages (k \u2208 {1, 5, 10, 25}) and evaluate on the development sets with the same number of passages. ",
            "104": "We can observe that the matching scores of both models increase with respect to the number of passages used in training, consistent with the findings in Izacard and Grave (2020b) that sequence-to-sequence model is capable of gathering information across multiple retrieved passages. ",
            "105": "Moreover, the two models show comparative performance when the number of training passages is small, but when more passages included, our model outperforms FiD, especially on the NQ dataset."
        }
    },
    "1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc": {
        "doc1": {
            "0": "paper_summary:",
            "1": "In this paper, authors propose a new variation of slot accuracy metric, called as relative slot accuracy (RSA), for dialog state tracking (DST) evaluation.",
            "2": "Unlike the slot accuracy that tracks all the pre-defined slots based on ontology, RSA only uses the slots that either appear in the predicted or the gold states in the given turn.",
            "3": "The metric is very intuitive and makes perfect sense.",
            "4": "summary_of_strengths:",
            "5": "- Evaluating DST appropriately is still challenging and joint goal accuracy (JGA) heavily penalizes a system for even one mistake.",
            "6": "Thus, JGA underestimates the performance of system, while another commonly used slot accuracy metric overestimates the performance.",
            "7": "The proposed modification to slot accuracy metric is very intuitive and can be used to better compare DST systems.",
            "8": "summary_of_weaknesses:",
            "9": "- A major concern is the lack of comparison against the newly proposed average goal accuracy (AGA) metric.",
            "10": "Please refer to the question.",
            "11": "It is important to understand how relative slot accuracy metric differs from the existing AGA metric, and what are the benefit of using this new metric along with (or in place of) the AGA metric.",
            "12": "comments,_suggestions_and_typos:",
            "13": "In Eq. 3, does T* include all the slots-values in gold and predicted belief states or just the predicted and gold slots-values in current turn?",
            "14": "How does this metric correlate with the Average Goal Metric proposed in Rastogi et al. (2020b)?",
            "15": "If you are using entire belief/predicted states for calculating T*, your metric is almost identical to the AGA.",
            "16": "I think you should include evaluation results with the AGA metric."
        },
        "doc2": {
            "0": "Mismatch between Multi-turn Dialogue and its Evaluation Metric in Dialogue State Tracking",
            "1": "Abstract",
            "2": "Dialogue state tracking (DST) aims to extract essential information from multi-turn dialog situations and take appropriate actions. ",
            "3": "A belief state, one of the core pieces of information, refers to the subject and its specific content, and appears in the form of domain-slot-value. ",
            "4": "The trained model predicts \"accumulated\" belief states in every turn, and joint goal accuracy and slot accuracy are mainly used to evaluate the prediction; however, we specify that the current evaluation metrics have a critical limitation when evaluating belief states accumulated as the dialogue proceeds, especially in the most used MultiWOZ dataset. ",
            "5": "Additionally, we propose relative slot accuracy to complement existing metrics. ",
            "6": "Relative slot accuracy does not depend on the number of predefined slots, and allows intuitive evaluation by assigning relative scores according to the turn of each dialog. ",
            "7": "This study also encourages not solely the reporting of joint goal accuracy, but also various complementary metrics in DST tasks for the sake of a realistic evaluation.",
            "8": "Introduction",
            "9": "The dialogue state tracking (DST) module structures the belief state that appears during the conversation in the form of domain-slot-value, to provide an appropriate response to the user. ",
            "10": "Recently, multi-turn DST datasets have been constructed using the Wizard-of-Oz method to reflect more realistic dialog situations (Wen et al., 2017;Mrk\u0161i\u0107 et al., 2017;Budzianowski et al., 2018). ",
            "11": "The characteristic of these datasets is that belief states are \"accumulated\" and recorded every turn. ",
            "12": "That is, the belief states of the previous turns are included in the current turn. ",
            "13": "It confirms whether the DST model tracks essential information that has appeared up to the present point.",
            "14": "Joint goal accuracy and slot accuracy are utilized in most cases to evaluate the prediction of accumu-lated belief states. ",
            "15": "Joint goal accuracy strictly determines whether every predicted state is identical to the gold state, whereas slot accuracy measures the ratio of correct predictions. ",
            "16": "However, we determined that these two metrics solely focus on \"penalizing states that fail to predict,\" not considering \"reward for well-predicted states.\" Accordingly, as also pointed out in Rastogi et al. (2020a), joint goal accuracy underestimates the model prediction because of its error accumulation attribute, while slot accuracy overestimates it because of its dependency on predefined slots.",
            "17": "However, there is a lack of discussion on the metric for evaluating the most used MultiWOZ dataset, despite a recently published dataset (Rastogi et al., 2020b) proposing some metrics. ",
            "18": "To address the above challenge, we propose reporting the relative slot accuracy along with the existing metrics in MultiWOZ. ",
            "19": "Because slot accuracy has the challenge of overestimation by always considering all predefined slots in every turn, relative slot accuracy does not depend on predefined slots, and calculates a score that is affected solely by slots that appear in the current dialog. ",
            "20": "Therefore, relative slot accuracy enables a realistic evaluation by rewarding the model's correct predictions, a complementary approach that joint goal and slot accuracies cannot fully cover. ",
            "21": "It is expected that the proposed metric can be adopted to evaluate model performance more intuitively.",
            "22": "Current Evaluation Metrics",
            "23": "Joint Goal Accuracy",
            "24": "Joint goal accuracy, developed from Henderson et al. (2014b) and Zhong et al. (2018), can be said to be an ideal metric, in that it verifies that the predicted belief states perfectly match the gold label. ",
            "25": "Equation 1 expresses how to calculate the joint goal accuracy, depending on whether the slot values match each turn. ",
            "26": "JGA = 1 if predicted state = gold state 0 otherwise (1) However, the joint goal accuracy underestimates the accumulated states because it scores the performances of later turn to zero if the model mispredicts even once in a particular turn, regardless of the model prediction quality at later turns. ",
            "27": "As illustrated in Figure 1, we measured the relative position of the turn causing this phenomenon for the dialog. ",
            "28": "We used MultiWOZ 2.1 (Eric et al., 2019), and analyzed 642 samples from a total of 999 test sets in which the joint goal accuracy of the last turn is zero. ",
            "29": "The DST model selected for primary verification is the SOM-DST (Kim et al., 2020), which is one of the latest DST models. ",
            "30": "Accordingly, the relative position where joint goal accuracy first became zero was mainly at the beginning of the dialog 1 . ",
            "31": "This means that the joint goal accuracy after the beginning of the dialog is unconditionally measured as zero because of the initial misprediction, although the model may correctly predict new belief states at later turns. ",
            "32": "Failure to measure the performance of the latter part means that it cannot consider various dialog situations provided in the dataset, which is a critical issue in building a realistic DST model.",
            "33": "Slot Accuracy",
            "34": "Slot accuracy can compensate for situations where joint goal accuracy does not fully evaluate the dialog situation. ",
            "35": "Equation 2 expresses how to calculate 0 1 2 3 4 5 6 7 8 9  the slot accuracy. ",
            "36": "T indicates the total number of predefined slots for all the domains. ",
            "37": "M denotes the number of missed slots that the model does not accurately predict among the slots included in the gold state, and W denotes the number of wrongly predicted slots among the slots that do not exist in the gold state.",
            "38": "Figure 2 illustrates the total number of annotated slots in MultiWOZ 2.1 to figure out the limitation of slot accuracy. ",
            "39": "Each value of x-axis in Figure 2 indicates the \"maximum\" number of slots that appear in a single dialog, and we confirmed that approximately 85% of the test set utilized solely less than 12 of the 30 predefined slots in the experiment. ",
            "40": "Because the number of belief states appearing in the early and middle turns of the dialog are smaller, and even fewer states make false predictions, calculating slot accuracy using Equation 2 reduces the influence of M and W , and the final score is dominated by the total slot number T . ",
            "41": "Accordingly, several previous studies still report the model performance using solely joint goal accuracy because slot accuracy excessively depends on the number of predefined slots, making the performance deviation among models trivial (refer to Table A5).",
            "42": "Furthermore, according to Table A6, we determined that slot accuracy tends to be too high. ",
            "43": "The slot accuracies of turns 0 and 1 show approximately 96% accuracy, despite the model not correctly predicting states at all. ",
            "44": "It becomes difficult to compare various models in detail, if each model shows a high performance, even though nothing is adequately predicted. ",
            "45": "In addition, as the turn progresses, there are no rewards for a situation in which the model tracks the belief state without any challenges. ",
            "46": "The case correctly predicting two out of three in turn 4, and the case correctly predicting three out of four in turn 5 exhibit the same slot accuracy. ",
            "47": "Therefore, the slot accuracy measured according to Equation 2 differs from our intuition.",
            "48": "Relative Slot Accuracy",
            "49": "As can be observed in Equation 2, slot accuracy has the characteristic that the larger the number of predefined slots (T ), the smaller the deviation between the prediction results. ",
            "50": "The deviation among DST models will be even more minor when constructing datasets with various dialog situations, because the number of predefined slots will continually increase. ",
            "51": "It is not presumed to be an appropriate metric in terms of scalability. ",
            "52": "Therefore, we propose relative slot accuracy, that is not affected by predefined slots, and is evaluated with adequate rewards and penalties that fit human intuition in every turn. ",
            "53": "Equation 3 expresses how to calculate the relative slot accuracy, and T * denotes the number of unique slots appearing in the predicted and gold states in a particular turn.",
            "54": "Relative slot accuracy rewards well-predicted belief states by measuring the scores in accumulating turns. ",
            "55": "Further discussions on the relative score will be discussed in Section 4.1.",
            "56": "Experiments",
            "57": "We measured MultiWOZ 2.1, an improved version of MultiWOZ 2.0 (Budzianowski et al., 2018), which has been adopted in several studies, according to Table A5. ",
            "58": "Five domains (i.e., hotel, train, restaurant, attraction, and taxi) are adopted in the experiment, following Wu et al. ( 2019), and there are a total of 30 domain-slot pairs. ",
            "59": "We selected the DST models in Table A5 that perform the Mul-tiWOZ experiment with the original authors' reproducible code 2 . ",
            "60": "Additionally, we reported the F1 score, which can be calculated using the current predicted and gold states. ",
            "61": " values. ",
            "62": "Furthermore, the correlation with joint goal accuracy, a mainly adopted metric, and relative slot accuracy with respect to each turn is lower than the correlation with joint goal accuracy and slot accuracy, as illustrated in Figure 3. ",
            "63": "Specifically, it can be compared with a different perspective when using the proposed reward-considering evaluation metric.",
            "64": "Results and Discussion",
            "65": "We reported the joint goal, slot, and relative slot accuracies per domain utilizing the SOM-DST model in Table 2. ",
            "66": "Relative slot accuracy derives a specific score in the turn configuration and prediction ratio of each domain by excluding slots that do not appear in the conversation. ",
            "67": "For example, the taxi domain shows a low score, meaning that it has relatively several cases of incorrect predictions, compared to the number of times slots belonging to the taxi domain appear. ",
            "68": "Because slot accuracy cannot distinguish the above trend, the score of the hotel domain is lower than that of the taxi domain. ",
            "69": "In summary, relative slot accuracy enables relative comparison according to the distribution of the domain in a dialog.",
            "70": "Dependency on predefined slots As discussed in Section 2.2, slot accuracy requiring total predefined slots is not a scalable method for evaluating the current dialog dataset that contain a few domains in each dialog. ",
            "71": "For example, when evaluating a dialog sample that solely deals with the restaurant domain, even domains that never appear at all (i.e., hotel, train, attraction, and taxi) are involved in measuring performance, making deviations among different models trivial. ",
            "72": "However, relative slot accuracy can evaluate the model's predictive score without being affected by slots never seen in the current dialog, which is a more realistic way, considering that each dialog contains its own turn and slot composition. ",
            "73": "Figure 4 illustrates the mean and standard deviations of the model performance in Table 1. ",
            "74": "As can be observed from the results, the relative slot accuracy has a higher deviation than the slot accuracy, enabling a detailed comparison among the methodologies.",
            "75": "Reward & penalty on relative dialog turn Relative slot accuracy is able to reward the model's correct prediction by measuring the accuracy on a relative basis for each turn. ",
            "76": "Table A6 compares the slot and relative slot accuracies. ",
            "77": "The relative slot accuracy from turns 0 -3 is measured as 0 because it calculates the score based on the unique state of the current turn according to Equation 3. ",
            "78": "In addition, regarding slot accuracy in turns 4, 5, and 6, there is no score improvement for the additional wellpredicted state by the model, whereas the score increases when the newly added state is matched in the case of relative slot accuracy. ",
            "79": "Therefore, relative slot accuracy can provide an intuitive evaluation reflecting the current belief state recording method, in which the number of slots accumulates incrementally as the conversation progresses.",
            "80": "Conclusion",
            "81": "This paper points out the challenge that the existing joint goal and slot accuracies cannot fully evaluate the accumulating belief state of each turn in the MultiWOZ dataset. ",
            "82": "Accordingly, the relative slot accuracy is proposed. ",
            "83": "This metric is not affected by unseen slots in the current dialog situation, and compensates for the model's correct prediction. ",
            "84": "When the DST task is scaled up to deal with more diverse conversational situations, a realistic model evaluation will be possible using relative slot accuracy. ",
            "85": "Moreover, we suggest reporting various evaluation metrics to complement the limitations of each metric in future studies, not solely reporting the joint goal accuracy.",
            "86": "Our findings show that if the model makes an incorrect prediction, the error accumulates until the end of the dialog, and the joint goal accuracy remains at zero. ",
            "87": "In this section, we discuss a few cases of 59 dialogs that do not show the trend among 642 dialogs selected in Section 2.1; however, it is important to note that these few cases have negligible effect on the trend in Figure 1, solely changing the position where the joint goal accuracy first becomes zero. ",
            "88": "We sampled dialogs of the MultiWOZ 2.1 test set in Table A1 and Table A2, and marked values appearing in the dialog in bold. ",
            "89": "Table A3 and Table A4 indicate the corresponding belief states of each dialogue. ",
            "90": "In the first dialog presented in Table A1, the joint goal accuracy is measured as 1 at turn 2. ",
            "91": "In this case, the model incorrectly predicted the restaurant-pricerange slot at turns 0 and 1, and then the utterance about the slot appeared by chance. ",
            "92": "In a general case, the wrong prediction of the restaurant-pricerange slot at turn 0 will accumulate to the last turn. ",
            "93": "However, in this case, another incorrect prediction at turn 3 will cause error accumulation in this dialogue.",
            "94": "The second dialog presented in Table A2, reports the incorrect prediction according to the interpretation of annotations at turn 4. ",
            "95": "In other words, because the dialog about the hotel-internet slot appears over turns 4 and 5, it is solely an error depending on the prediction timing of the model. ",
            "96": "Because the correct belief state was predicted right from turn 5, it cannot be said to be an error accumulation phenomenon; however, the model did not predict the hotel-pricerange slot at turn 6, which is the last turn in this case.",
            "97": "In conclusion, it can be determined that the model does not seem to accumulate erroneous predictions because of an accidental situation or interpretation of annotations, but this does not negate the error accumulation phenomenon. ",
            "98": "Furthermore, the fact that the starting point of making the joint goal accuracy of subsequent turns to 0 mainly occurs at the beginning of the dialog does not change.",
            "99": "Turn # Dialogue History 0 System: \" \" User: \"can you help me find a nice restaurant ?\" 1 System: \"sure ! what kind of food do you like ?\" User: \"i was thinking some indian food would be great .\"",
            "100": "2 System: \"i have 22 indian restaurant -s do you have a preference for area of town ?\" User: \"no , i do not care where it is . i am thinking i would like an expensive restaurant .\"",
            "101": "3 System: \"would you like to try curry garden ?\" User: \"that is fine book me a table for 6 on sat at 17:30 . i also need a train for the same and should leave leicester for cambridge\" 4 System: \"alright , i have made your requested booking at curry garden , and the reference number is hk9ycl6z . ",
            "102": "as for the train , what time would you like to depart ?\" User: \"it does not matter as long as i am there by 13:45 leaving leicester going to cambridge , i'll need the reference number too please\" 5 System: \"i have found tr6210 leaving leicester at 11:09 on saturday and arriving in cambridge at 12:54 . ",
            "103": "i have booked 6 seats . ",
            "104": "reference number is lr5i1rzv . ",
            "105": "anything else i can do ?\" User: \"that will be all for today . ",
            "106": "thanks so much for all your help . goodbye\""
        }
    },
    "e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833": {
        "doc1": {
            "0": "paper_summary:",
            "1": "The authors propose and evaluate an approach for automatic question generation in an educational context, specifically looking at the role of summarization (whether it be human or automatic) in the process.",
            "2": "Their approach focuses on a context in which there is no need to explicitly identify a region corresponding to the answer for a generated question (so the task is \u201canswer-unaware\u201d).",
            "3": "summary_of_strengths:",
            "4": "The paper contains insightful empirical results showing the high value of using human summaries in the process,  but also shows that machine generated summaries can also be useful.",
            "5": "It also contains well organized related literature.",
            "6": "Overall, the paper is well written, and easy to understand for a large audience, with good use of examples.",
            "7": "summary_of_weaknesses:",
            "8": "The methodology section could be strengthened by further justification of some of the design decisions; for example, what models were considered in addition to T5 (line 148-152).",
            "9": "Were there other tasks that were considered beyond the three examined (line 153-154).",
            "10": "The evaluation section could benefit from further motivation for the three experiments shown; were other considered?",
            "11": "Were there other datasets that were considered?",
            "12": "comments,_suggestions_and_typos:",
            "13": "How did you determine what questions you were going to ask the annotators?",
            "14": "How is it related to what has been done in related research?",
            "15": "What was the process for identifying annotators?",
            "16": "Why only three annotators?",
            "17": "Glad to see that the \"full annotation data\" will be available, and I'm assuming that any other required data will also be available to other researchers as well.",
            "18": "ethical_concerns:",
            "19": "No ethical concerns."
        },
        "doc2": {
            "0": "A Feasibility Study of Answer-Unaware Question Generation for Education",
            "1": "Abstract",
            "2": "We conduct a feasibility study into the applicability of answer-unaware question generation models to textbook passages. ",
            "3": "We show that a significant portion of errors in such systems arise from asking irrelevant or un-interpretable questions and that such errors can be ameliorated by providing summarized input. ",
            "4": "We find that giving these models human-written summaries instead of the original text results in a significant increase in acceptability of generated questions (33% -> 83%) as determined by expert annotators. ",
            "5": "We also find that, in the absence of human-written summaries, automatic summarization can serve as a good middle ground.",
            "6": "Introduction",
            "7": "Writing good questions that target salient concepts is difficult and time consuming. ",
            "8": "Automatic Question Generation (QG) is a powerful tool that could be used to significantly lessen the amount of time it takes to write such questions. ",
            "9": "A QG system that automatically generates relevant questions from textbooks would help professors write quizzes faster and help students spend more time reviewing flashcards rather than writing them.",
            "10": "Previous work on QG has focused primarily on answer-aware QG models. ",
            "11": "These models require the explicit selection of an answer span in the input context, typically through the usage of highlight tokens. ",
            "12": "This adds significant overhead to the question generation process and is undesirable in cases where clear lists of salient key terms are unavailable. ",
            "13": "We conduct a feasibility study on the application of answer-unaware question generation models (ones which do not require manual selection of answer spans) to an educational context. ",
            "14": "Our contributions are as follows:",
            "15": "\u2022 We show that the primary way answerunaware QG models fail is by generating irrelevant or un-interpretable questions.",
            "16": "Input: The perplexity of a language model on a test set is the inverse probability of the test set, normalized by the number of words. ",
            "17": "For a test set W = w1w2\u2026wN we can use the chain rule to expand the probability of W.  \u2022 We show that giving answer-unaware QG models human-written summaries instead of the original text results in a significant increase in acceptability of generated questions (33% -> 83%).",
            "18": "\u2022 We show that, in the absence of humanwritten summaries, providing automatically generated summaries as input is a good alternative.",
            "19": "Related Work & Background",
            "20": "Early attempts to use QG for educational applications involved generating gap-fill or \"cloze\" questions 1 (Taylor, 1953) from textbooks (Agarwal and Mannem, 2011). ",
            "21": "One may optionally choose to generate distractors to make these questions multiple choice (Narendra et al., 2013;Correia et al., 2012). ",
            "22": "This procedure has been shown to be effective in classroom settings (Zavala and Mendoza, 2018) and students' scores on this style of generated question correlate positively with their scores on human-written questions (Guo et al., 2016). ",
            "23": "However, there are many situations where gap-fill questions are not effective, as they are only able to ask about specific unambiguous key terms.",
            "24": "In recent years, with the advent of large crowdsourced datasets for extractive question answering (QA) such as SQuAD (Rajpurkar et al., 2018), neural models have become the primary methods of choice for generating traditional interrogative style questions (Kurdi et al., 2019). ",
            "25": "A common task formulation for neural QG is to phrase the task as answer-aware, that is, given a context passage C = {c 0 , ..., c n } and an answer span within this context A = {c k , ..., c k+l } such that k \u2265 0 and k + l \u2264 n, train a model to maximize P (Q|A, C) where Q = {q 0 , ..., q m } are the tokens in the question. ",
            "26": "These models are typically evaluated using n-gram overlap metrics such as BLEU/ROUGE/METEOR (Papineni et al., 2002;Lin, 2004;Banerjee and Lavie, 2005) with the reference being the original human-authored question as provided by the extractive QA dataset.",
            "27": "The feasibility of using answer-aware neural QG in an educational setting was investigated by Wang et al. (2018), who used a BiLSTM encoder (Zhang et al., 2015) to encode C and A and a unidirectional LSTM decoder to generate Q. They trained on the SQuAD dataset (Rajpurkar et al., 2018) and evaluated on textbooks from various domains (history, sociology, biology). ",
            "28": "They showed that generated questions were largely grammatical, relevant, and had high n-gram overlap with humanauthored questions. ",
            "29": "However, given that we may not always have a convenient list of key terms to use as answer spans for an input passage, there is a desire to move past answer-aware QG models and evaluate the feasibility of answer-unaware models for use in education. ",
            "30": " are most likely to be used as answer targets for questions. ",
            "31": "We can then take these extracted answer spans and give them as input to an answer-aware QG model P (Q|A, C). ",
            "32": "This modeling choice allows for more controllable QG and more direct modeling of term salience. ",
            "33": "Previous work done by Subramanian et al. ( 2018) trained a BiLSTM Pointer Network (Vinyals et al., 2015) for answer extraction and showed that it outperformed an entity-based baseline when predicting answer spans from SQuAD passages. ",
            "34": "However, their human evaluation centered around question correctness and fluency rather than relevance of answer selection. ",
            "35": "Similar follow-up studies also fail to explicitly ask human annotators whether or not the extracted answers, and subsequent generated questions, were relevant to the broader topic of the context passage (Willis et al., 2019;Cui et al., 2021;Wang et al., 2019;Du and Cardie, 2018;Alberti et al., 2019;Back et al., 2021).",
            "36": "In our study, we explicitly ask annotators to determine whether or not a generated question is relevant to the topic of the textbook chapter from which it is generated. ",
            "37": "In addition, we show that models trained for answer extraction on SQuAD frequently select irrelevant or ambiguous answers when applied to textbook material. ",
            "38": "We show that summaries of input passages can be used instead of the original text to aid in the modeling of topic salience and that questions generated from humanwritten and automatically-generated summaries are more relevant, interpretable, and acceptable.",
            "39": "Methodology",
            "40": "To perform answer-unaware QG, we take inspiration from work done by Dong et al. (2019) and Bao et al. (2020)  when fine-tuned for both QA and QG, perform better than models tuned for only one of those tasks.",
            "41": "We assume that answer extraction will help both QA and QG and therefore use a model that was fine-tuned on all three. ",
            "42": "We chose a version of the T5 language model (Raffel et al., 2020) fine-tuned on SQuAD due to the clean separation between tasks afforded by T5's task-specific prefixes such as \"generate question:\" and \"extract answer:\". ",
            "43": "2 The three fine-tuning tasks that were used to train our model are illustrated in Figure 2. ",
            "44": "For question generation, the model is trained to perform answer-aware question generation by modeling P (Q|A, C). ",
            "45": "For question answering, the model is trained to perform extractive QA by modeling P (A|C, Q). ",
            "46": "Finally, for answer extraction, instead of directly modeling P (A|C), a new context C \u2032 = {c 0 , ..., c s , ..., c e , ..., c n+2 } is generated where c s and c e are highlight tokens that denote the start (s) and end (e) of the sub-sequence within which we want to extract an answer span. ",
            "47": "The answer extraction fine-tuning task thus becomes modeling P (A|C \u2032 ) where A = {c k , ..., c k+l } such that k \u2265 s and k + l \u2264 e.",
            "48": "Because T5 has a fixed maximum context length of 512 tokens, input passages that contain n > 512 tokens must be split up into smaller sub-passages. ",
            "49": "We perform this splitting such that no sentences are divided between sub-passages and all sub-passages have a roughly equal number of sentences. ",
            "50": "3 Finally, to generate questions, we iteratively choose the start and end of each sentence in a given subpassage as our c s and c e and extract at most one answer span per sentence. ",
            "51": "4 We then generate one question per extracted answer span using the same model in an answer-aware fashion.",
            "52": "2 https://huggingface.co/valhalla/t5-base-qa-qg-hl 3 Sentence boundaries are determined by NLTK 4 If the generated answer span tokens are not sequentially present in the highlighted sentence, the answer is discarded",
            "53": "Experiments",
            "54": "Our first experiment evaluates the performance of the model on the original text extracted from Jurafsky and Martin (2009)'s textbook \"Speech and Language Processing 3rd Edition\". ",
            "55": "5 To ensure proper comparison, we manually extracted the text from our three chapters of interest (Chapters 2, 3, and 4). ",
            "56": "When extracting text, all figures, tables, and equations were omitted and all references to them were either replaced with appropriate parenthetical citations or removed when possible. ",
            "57": "In total, we generated 1208 question-answer pairs from the original text.",
            "58": "Our second experiment evaluates the performance of the model on human-written summaries. ",
            "59": "We asked three research assistants (RAs) to write summaries for each subsection of the same three chapters (2-4) of the textbook. ",
            "60": "These RAs were encouraged to make these summaries easily readable by humans rather than to be easily understandable by machines. ",
            "61": "From these 3 sets of summaries we generated a total of 667 question-answer pairs.",
            "62": "Our final experiment evaluates the performance of the model on automatically generated summaries. ",
            "63": "To perform this automatic summarization we used a BART (Lewis et al., 2019) language model which was fine-tuned for summarization on the CNN/DailyMail dataset (Nallapati et al., 2016). ",
            "64": "6 The same chunking procedure as described in Section 3 was performed on input passages that were larger than 512 tokens. ",
            "65": "The summarized output sub-passages were then concatenated together before running question generation. ",
            "66": "In total we generated 318 question-answer pairs from our automatic summaries.",
            "67": "Evaluation",
            "68": "For evaluation, we randomly sampled 100 questionanswer pairs from each of the three experiments to construct our evaluation set of 300 questions. ",
            "69": "We recruited three expert annotators, all undergraduates in computer science, to evaluate the quality of the question-answer pairs. ",
            "70": "All 300 pairs were given to all three annotators. ",
            "71": "We asked the annotators to answer the following yes/no questions: a) Would you directly use this question as a flashcard?, b) Is this question grammatical?, c) Does this question make sense out of context?, d) Is this question relevant? and e) Is the answer to this question correct? ",
            "72": "We report these in our tables as \"Acceptable?\", \"Grammatical?\", \"Interpretable?\", \"Relevant?\", and \"Correct?\" respectively. ",
            "73": "We provided many annotation examples to our annotators and wrote clear guidelines about each category to ensure high agreement. ",
            "74": "Our full annotator guidelines can be found in Appendix A.",
            "75": "In Figure 3 we report the results of our evaluation across the three experiments. ",
            "76": "We note that a majority of observed errors in the original text questions stem from them being either irrelevant or un-interpretable out of context. ",
            "77": "We also see that generating questions directly from human-written summaries significantly improves relevance and incontext interpretability, resulting in over 80% being labeled as acceptable by annotators. ",
            "78": "Finally, in the case of automatic summaries, we see that relevance and in-context interpretability are somewhat improved as compared to the original text questions while grammaticality suffers slightly.",
            "79": "In Table 1 we report the distribution of scores across chapters. ",
            "80": "We note that scores are largely consistent across the three chapters, with lower average relevance for Chapter 2 questions likely owing to the source material containing many worked examples of regular expressions and application-specific details.",
            "81": "In Table 2 we report the per-annotator statistics as well as the pairwise inter-annotator agreement (IAA). ",
            "82": "While at first glance it may seem that agreement is low for grammaticality and correctness, this is somewhat expected for highly unbalanced classes (Artstein and Poesio, 2008). ",
            "83": "For the other three categories we see an average pairwise agree- Table 2: Comparison between our three annotators (A1, A2, A3) on all 300 questions across all categories. ",
            "84": "Numbers represent percentages. ",
            "85": "Pairwise Inter-Annotator Agreement is calculated by Cohen \u03ba and is reported in the order (A1-A2, A2-A3, A3-A1).",
            "86": "ment of approximately 0.4 which suggests a fairly large degree of agreement for such a seemingly amorphous and ambiguous category. ",
            "87": "Examples of questions for each category on which there was significant disagreement are listed in Appendix B.",
            "88": "Conclusion and Future Work",
            "89": "In this work we show that answer-unaware QG models have difficulty both choosing relevant topics to ask about and generating questions that are interpretable out of context. ",
            "90": "We show that asking questions on summarized text ameliorates this in large part and that these gains can be approximated by the use of automatic summarization. ",
            "91": "Future work should seek to further explore the relationship between summarization and QG. ",
            "92": "Work done concurrently to ours by Lyu et al. (2021) already has promising results in this direction, showing that training a QG model on synthetic data from summarized text improves performance on downstream QA.",
            "93": "Additionally, future work should focus on further refining and standardizing the metrics used for both automatic and human evaluation of QG. ",
            "94": "As noted by Nema and Khapra (2018) n-gram overlap metrics correlate poorly with in-context interpretability and evaluation on downstream QA fails to address the relevance of generated questions.",
            "95": "In Table 3 we report the annotation guidelines given to our annotators. ",
            "96": "In the original document, under each category, 3 or more example annotations were given, each containing an explanation as to why the selection was made. ",
            "97": "Categories such as grammaticality had upwards of 10 or more examples given to ensure maximum possible agreement between annotators. ",
            "98": "Several discussion sessions were held between the authors and annotators to ensure that these guidelines were well understood and that they were sensible for the task. ",
            "99": "During annotation, annotators were not given the original source text from which the question was generated. ",
            "100": "Instead, they were given the original textbook chapters to use as reference material for relevance and were allowed to use online search engines to check for grammaticality and correctness.",
            "101": "In Table 4 we list questions for which there was at least one dissenting annotator for the given category.",
            "102": "We see that for categories such as \"Relevant?\" and \"Interpretable?\", annotations are often dependent on the level of granularity with which the topic is being discussed. ",
            "103": "For example, a question such as \"Who named the minimum edit distance algorithm?\" may or may not be relevant depending ",
            "104": "on how granular of a class the student is taking.",
            "105": "For categories such as \"Correct?\" or \"Acceptable?\" certain particularities about otherwise good questions can easily disqualify them from receiving a positive annotation. ",
            "106": "In the case of \"What NLP algorithms require algorithms for word segmentation?\", keen-eyed annotators would notice that the question is non-sensical, however others may note that both Japanese and Thai do, in fact, require word segmentation. ",
            "107": "Particularities such as these make this task very difficult, even for expert annotators.",
            "108": "We provide our full annotation data in CSV form in the supplementary material for further inspection."
        }
    },
    "5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22": {
        "doc1": {
            "0": "paper_summary:",
            "1": "This paper investigates the degree of knowledge that pre-trained LM, with only access to a vocabulary of subword tokens, have about the character composition of these tokens, and if enriching these models with orthographic information about the tokens can improve them.",
            "2": "It proposes to use a probe they name SpellingBee: a generative character-based LM that takes as input an uncontextualized word embedding from a model, and tries to predict the correct character sequence.",
            "3": "It is trained on part of the model's vocabulary, and tested on the other: if it manages to succesfully generalize, the embedding must contain orthographic information.",
            "4": "The probe is tested on 4 models: Roberta-base, and 3 others showing change in a particular aspect: Roberta-large for size, AraBert for language, and GPT2-Medium for an autoregressive model.",
            "5": "The probe's capacity to predict the character sequence is evaluated by counting the exact matches, and with a finer-grained metric measuring overlap.",
            "6": "Compared to a control experiment where the probe is not trained and only randomly initialized, the probe is able to better rebuild character sequences when fed with embeddings from the LMs (up to 30-40% from 0 for exact matches); however, it's performance is weakened when the training part of the vocabulary is filtered (removing token too similar to those in the testing part, or with the same lemma).",
            "7": "However, using a probe trained on the full vocabulary as a way to initialize a LM does not seem to be useful, as the LM reaches the same training loss as a control one rather quickly.",
            "8": "summary_of_strengths:",
            "9": "- This paper poses a research question of great interest, and answers it while carefully considering many possible factors (models, filtering the training vocabulary).",
            "10": "- It investigates a potential application of this answer.",
            "11": "- The paper is very clearly written, and easy to follow.",
            "12": "summary_of_weaknesses:",
            "13": "- The results given by the probe are a little difficult to interpret; as, while there is a control experiment where the probe has no information, it would be very useful to have an idea of what the probe can do when fed with embeddings that we know contain orthographic information.",
            "14": "If testing the probe on static embeddings (word2vec, glove), fasttext embeddings could work; in this setting, I believe uncontextualized embeddings from CharacterBERT (El Boukkouri et al, 2020) could work.",
            "15": "comments,_suggestions_and_typos:",
            "16": "- The samples of errors shown in Table 3 seem to often have the first, or few first characters right. Did you at some point try to filter by prefix rather than lemmas ?"
        },
        "doc2": {
            "0": "Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens",
            "1": "Abstract",
            "2": "Standard pretrained language models operate on sequences of subword tokens without direct access to the characters that compose each token's string representation. ",
            "3": "We probe the embedding layer of pretrained language models and show that models learn the internal character composition of whole word and subword tokens to a surprising extent, without ever seeing the characters coupled with the tokens. ",
            "4": "Our results show that the embedding layer of RoBERTa holds enough information to accurately spell up to a third of the vocabulary and reach high average character ngram overlap on all token types. ",
            "5": "We further test whether enriching subword models with additional character information can improve language modeling, and observe that this method has a near-identical learning curve as training without spelling-based enrichment. ",
            "6": "Overall, our results suggest that language modeling objectives incentivize the model to implicitly learn some notion of spelling, and that explicitly teaching the model how to spell does not enhance its performance on such tasks.",
            "7": "Introduction",
            "8": "Contemporary subword tokenization algorithms such as BPE (Sennrich et al., 2016) partition a string into contiguous spans of characters. ",
            "9": "Each span represents a frequent character ngram, from individual characters (a), through prefixes (uni) and suffixes (tion), and even complete words (cats). ",
            "10": "The tokenizer then converts each such span into a discrete symbol (a token) with no internal structure, effectively discarding the token's orthographic information. ",
            "11": "Therefore, a model operating over sequences of subword tokens should be oblivious to the spelling of each token. ",
            "12": "In this work, we show that despite having no direct access to the subwords' internal character composition, pretrained language models do learn some notion of spelling.",
            "13": "To examine what pretrained language models learn about spelling, we present the SpellingBee probe. ",
            "14": "SpellingBee is a generative language model that predicts the character composition of a token given only its (uncontextualized) vector representation from the pretrained model's embeddings matrix. ",
            "15": "SpellingBee is trained on part of the model's vocabulary, and then tested by spelling unseen token types. ",
            "16": "If the probe can successfully reconstruct the correct character sequence from an unseen token's embedding, then there must be significant orthographic information encoded in the vector.",
            "17": "We find that the embedding layers of several pretrained language models contain surprising amounts of character information. ",
            "18": "SpellingBee accurately spells 31.8% of the held-out vocabulary for RoBERTa-Large (Liu et al., 2019), 32.9% for GPT2-Medium (Radford et al., 2019), and 40.9% for the Arabic language model AraBERT-Large (Antoun et al., 2020). ",
            "19": "A softer metric that is sensitive to partially-correct spellings (chrF) (Popovi\u0107, 2015) shows a similar trend, with 48.7 for RoBERTa-Large and 62.3 for AraBERT-Large. ",
            "20": "These results are much higher than the baseline of applying SpellingBee to randomly-initialized vectors, which fails to spell a single token.",
            "21": "Given that subword models learn some notion of character composition to fulfill language modeling objectives, could they perhaps benefit from knowing the exact spelling of each token a priori? ",
            "22": "To that end, we reverse SpellingBee's role and use it to pretrain the embedding layer of a randomlyinitialized model, thus imbuing each token representation with its orthographic information before training the whole model on the masked language modeling objective. ",
            "23": "We compare the pretraining process of the character-infused model to that of an identical model whose embedding layer is randomly initialized (and not pretrained), and find that both learning curves converge to virtually identi-cal values within the first 1000 gradient updates, a fraction of the total optimization process. ",
            "24": "This experiment suggests that while language models may need to learn some notion of spelling to optimize their objectives, they can quickly acquire all the character-level information they need without directly observing the composition of each token.",
            "25": "Spelling Bee",
            "26": "To measure how much a model knows the character composition of its tokens, we introduce Spelling-Bee, a generative probe that tries to spell out a token character-by-character. ",
            "27": "Specifically, Spelling-Bee probes the original model's embedding matrix, since spelling is a property of token types, invariant to context. ",
            "28": "For example, given the embedding of the token cats, SpellingBee will try to generate the sequence [c, a, t, s]. ",
            "29": "We do so by modeling SpellingBee as a character-based language model, where the first token is a vector representation of the vocabulary item. ",
            "30": "1 Training We split the vocabulary to train and test sets, 2 and use teacher forcing to train SpellingBee. ",
            "31": "In the example of cats, SpellingBee will compute the following probabilities:",
            "32": ". . .",
            "33": "All of SpellingBee's parameters are randomly initialized. ",
            "34": "The only parameters that are pretrained are the token embeddings (e.g. the representation of cats or a), which are taken from the original pretrained language model we intend to probe, and treated as constants; i.e. kept frozen during SpellingBee's training.",
            "35": "Inference & Evaluation Once SpellingBee is trained, we apply it to the test set using greedy decoding. ",
            "36": "For each vocabulary item w in the test set, SpellingBee is given only the corresponding embedding vector e w , and is expected to generate the character sequence w 1 , . . . , w n that defines w. We measure success on the test set using two metrics:",
            "37": "exact match (EM), and character ngram overlap score using chrF (Popovi\u0107, 2015). ",
            "38": "While EM is strict, chrF allows us to measure partial success. ",
            "39": "We also report edit distance using Levenshtein distance ratio in Appendix A.",
            "40": "While we mainly use SpellingBee as a probe, a variation of our method could potentially imbue the embedding layer with character information before training a language model. ",
            "41": "We could train a probe with randomly-initialized embeddings (instead of pretrained embeddings from another model) to predict the spelling of all vocabulary items, and use these trained probe embeddings to initialize any target model's embedding layer (instead of random initialization). ",
            "42": "We experiment with this method in Section 5, but find that it does not have any significant impact on the convergence of language models.",
            "43": "Experiment Setup",
            "44": "We begin with a series of probing experiments, where we apply SpellingBee to the embedding layer of various pretrained models. ",
            "45": "3",
            "46": "Pretrained Models We probe four pretrained models: RoBERTa-Base and Large (Liu et al., 2019), GPT2-Medium (Radford et al., 2019), and AraBERT-Large (Antoun et al., 2020). ",
            "47": "This set introduces some diversity in vocabulary, objective, and scale: the first three models are trained on English corpora, while AraBERT is trained on text in Arabic; GPT2 is an autoregressive language model, while the rest are masked language models; RoBERTa-Base consists of 125M parameters (with 768 dimensions per embedding), while the other models have approximately 350M parameters (with 1024 dimensions per embedding).",
            "48": "Control Since SpellingBee is a trained probe, we wish to establish the probe's baseline performance when provided with inputs with no orthographic information. ",
            "49": "As an empirical control, we train and test SpellingBee on randomly-initialized vectors, in addition to the main experiments where we utilize the pretrained embedding layers.",
            "50": "Training & Testing Data We split the vocabulary into training and testing data using the following protocol. ",
            "51": "First, we randomly sample 1000 token types as test. ",
            "52": "We then filter the remaining  4 The lemma filter always applies the similarity filter first, providing an even more adversarial approach for splitting the data.",
            "53": "To control for variance, we create 10 such splits for each model and filter, and report the averaged evaluation metrics over all 10 test sets.",
            "54": "Results",
            "55": "Main Result Table 1 shows how well Spelling-Bee can spell a vocabulary token using only its frozen pretrained embedding. ",
            "56": "We observe that SpellingBee is able to accurately recover the spelling of up to 40.9% of the test set, while the control is unable to spell even a single word correctly. ",
            "57": "A similar trend can be seen when considering the finer character ngram metric (chrF but does not affect EM. ",
            "58": "These results are persistent across different models and filters, strongly indicating that the embedding layer of pretrained models contain significant amounts of information about each token's character composition.",
            "59": "One may suggest that training SpellingBee over 32000 examples may leak information from the test set. ",
            "60": "For example, if dog was seen during training, then spelling out dogs might be easy. ",
            "61": "We thus consider the similarity and lemma filters, which remove such near-neighbors from the training set. ",
            "62": "While results are indeed lower (and probably do account for some level of information leakage), they are still considerably higher than the control, both in terms of EM and chrF. Results using the similarity and lemma filters are rather similar, suggesting that embedding-space similarity captures some information about each token's lemma.",
            "63": "Finally, we find that the properties of pretrained models also seem to have a significant effect on the amount of spelling information SpellingBee can extract. ",
            "64": "Larger models tend to score higher in the probe, and the model trained on text in Arabic appears to have substantially higher EM and chrF scores than those trained on English corpora. ",
            "65": "One possibility is that Arabic's rich morphology incentivizes the model to store more information about each token's character composition, however it is also possible that AraBERT's different vocabulary, which allocates shorter character sequences to each token type, might explain this difference (we discuss the link between sequence length and accuracy later in this section). ",
            "66": "Overall, our probing experiments show that even though subword-based language models do not have direct access to spelling, they can and do learn a surprising amount of information about the character composition of each vocabulary token.",
            "67": "We further examine whether SpellingBee can extract information when trained on less examples. ",
            "68": "Figure 1 shows how well SpellingBee can spell RoBERTa-Large's vocabulary when trained on varying amounts of data, across all filters. ",
            "69": "We find that more data makes for a better probe, but that even a few thousand examples are enough to train SpellingBee to extract significant character information from the embeddings, which cannot be extracted from randomized vectors (the control). ",
            "70": "5",
            "71": "Pretraining Language Models to Spell",
            "72": "Our probing experiments reveal that language models learn some partial notion of spelling, despite the lack of direct access to characters. ",
            "73": "Therefore, we hypothesize that learning to spell is beneficial for language models, and propose pretraining the embedding layer using a variant of the SpellingBee probe described in Section 2. ",
            "74": "Here, the goal is to imbue each embedding with enough information for SpellingBee to accurately generate its surface form, and then initialize the language model with the pretrained embeddings before it starts training on the language modeling objective. ",
            "75": "5 We provide additional analysis on spelling accuracy by subword frequency and length in Appendices B and C.",
            "76": "We apply this process to RoBERTa-Large, training the model's embedding layer with SpellingBee using the same hyperparameter settings from Appendix E, with the key difference being that the embeddings are now tunable parameters (not frozen). ",
            "77": "6 We train RoBERTa-Large on English Wikipedia using the hyperparameter configuration of 24hBERT (Izsak et al., 2021), and cease training after 24 hours (\u223c16000 steps). ",
            "78": "For comparison, we train exactly the same model with a randomly-initialized embedding layer.",
            "79": "Figure 2 shows the masked language modeling loss with and without pretrained embeddings. ",
            "80": "We see that the curves quickly converge into one. ",
            "81": "After only 1000 training steps, the difference between the validation losses never exceeds 0.01. ",
            "82": "This result indicates that the model does not utilize the character information injected into the tokens' embeddings. ",
            "83": "Along with the results from Section 4, we conjecture that the model learns an implicit notion of spelling during pretraining, which is sufficient for masked language modeling, and does not benefit from explicitly adding orthographic information.",
            "84": "Conclusion",
            "85": "This work reveals that pretrained language models learn, to some extent, the character composition of subword tokens. ",
            "86": "We show that our Spelling-Bee probe can spell many vocabulary items using their uncontextualized embedding-layer representations alone. ",
            "87": "Trying to explicitly infuse character information into the model appears to have a minimal effect on the model's ability to optimize its language modeling objective, suggesting that the model can independently learn all the characterlevel information it needs for the task.",
            "88": "Levenshtein distance (Levenshtein et al., 1966) is an edit distance metric that, given two strings, calculates the minimal number of changes needed to be done in order to make the two strings identical. ",
            "89": "Levenshtein distance ratio is the length-normalized version, which is computed by adding the sum of lengths of both strings to the edit distance and dividing by the same sum of lengths. ",
            "90": "We report the main experiment's results using this ratio in Table 2.",
            "91": "We test whether pretrained models tend to store more spelling-related information in higherfrequency token types. ",
            "92": "We focus on RoBERTa-Large, and assign each token in the test set to its frequency quintile according to the number of times it appeared in the pretraining corpus -from the 10000 most frequent token types (top 20%) to those ranked 40000-50000 in the vocabulary (bottom 20%) -and measure the average performance of SpellingBee within each quintile. ",
            "93": "Figures 3 and  4 shows the results with and without the similarity filter. ",
            "94": "We observe that SpellingBee is indeed able to extract more information from higher-frequency token types, suggesting that the pretrained model has more information about their character composition.",
            "95": "We analyze the effect of token length on the probe's ability to spell. ",
            "96": "A priori, it is reasonable to assume that it is easier for the probe to spell shorter tokens, since less information needs to be extracted from the embedding and there are less discrete decisions to be made while decoding. ",
            "97": "Indeed, Figure 5 shows that with the none filter most vocabulary tokens with 2-4 characters can be accurately reproduced from their vector representations, while longer tokens are harder to replicate. ",
            "98": "This trend is particularly sharp when the similarity filter is applied, as the probe is hardly able to spell tokens with 6 or more characters accurately; having said that, the probe is able to generate many partially correct spellings, as measured by chrF (Figure 6). ",
            "99": "Perhaps a less intuitive result is the probe's failure to spell single-character tokens. ",
            "100": "A closer look reveals that many of these examples are rare or non-alphanumeric characters (e.g. c \u00b8and $), which are probably very difficult for the probe to generate if it had not seen them during training. ",
            "101": "While these results show strong trends with respect to length, token length is also highly correlated with frequency, and it is not necessarily clear which of the two factors has a stronger impact on the amount and resolution of character-level information stored in the embedding layer of pretrained models.",
            "102": "We manually analyze 100 random tokens that SpellingBee spelled incorrectly with the lemma filter to understand the nature of the spelling mistakes.",
            "103": "Out of those 100 we display 20 mistakes in Table 3 alongside the spelling prediction of the control baseline. ",
            "104": "SpellingBee's mistakes vary from singlecharacter typos to completely different words. ",
            "105": "Having said that, the vast majority of mistakes have significant overlap with the correct spelling, such as shared prefixes and capitalization.",
            "106": "We implement SpellingBee with a 6-layer encoderdecoder model, with 512 model dimensions.",
            "107": "The model parameters are optimized with Adam (Kingma and Ba, 2015) for 1000 steps with up to 1024 tokens per batch, a learning rate of 5e-4, and a dropout rate of 0.1. ",
            "108": "These are the default hyperpa-  rameters for training a transformer language model in Fairseq ."
        }
    },
    "df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c": {
        "doc1": {
            "0": "paper_summary:",
            "1": "The paper presents NAKDIMON, a character-LSTM Hebrew diacritizer that does not use a morphological analyzer/dictionary.",
            "2": "It compares its performance to SOTA on the task; and presents a new test set for the task that is more diverse that previous sets.",
            "3": "summary_of_strengths:",
            "4": "The paper is well written and clear.",
            "5": "The work is well motivated and enough related work is presented.",
            "6": "The error analysis is clear and helpful.",
            "7": "This is a good short paper with a negative result.",
            "8": "summary_of_weaknesses:",
            "9": "The paper presents inconclusive negative results: it is unclear what the results would look like had millions of additional words that are *automatically diacritized* using Dicta are added in Nakdimon's training. The authors added 1.3M such words.",
            "10": "I think an additional experiment that uses more words (automatically diacritized) or just generated word forms from a morphological dictionary can make the result more convincing (negative or positive it may be).",
            "11": "comments,_suggestions_and_typos:",
            "12": "- it would help to report OOV rates; and performance on in-vocabulary vs OOV for your system.",
            "13": "- the discussion of Arabic use of diacritization is not accurate.",
            "14": "Arabic \"dots\" are not optional in common use of Arabic; diacritical marks (vowels, nunation, gemination) are. Check out https://aclanthology.org/N07-2014.pdf https://aclanthology.org/2007.mtsummit-papers.20.pdf",
            "15": "- I am puzzled by a difference between the Dicta test set and the new test set: the difference between CHA and WOR for the Dicta test set is much bigger that the respective difference in the new test set (between 1.4 and 2.4 times bigger). Any thoughts on that?",
            "16": "ethical_concerns:",
            "17": "None"
        },
        "doc2": {
            "0": "Restoring Hebrew Diacritics Without a Dictionary",
            "1": "Abstract",
            "2": "We demonstrate that it is feasible to diacritize Hebrew script without any human-curated resources other than plain diacritized text. ",
            "3": "We present NAKDIMON, a two-layer characterlevel LSTM, that performs on par with much more complicated curation-dependent systems, across a diverse array of modern Hebrew sources. ",
            "4": "Its lightweight architecture allows it to be integrated into end-to-end Hebrew processing systems with little overhead, and with the capacity for further tuning on downstream tasks.",
            "5": "Introduction",
            "6": "The vast majority of modern Hebrew texts are written in a letter-only version of the Hebrew script, one which omits the diacritics present in the full diacritized, or dotted variant. ",
            "7": "1 Since most vowels are encoded via diacritics, the pronunciation of words in the text is left underspecified, and a considerable mass of tokens becomes ambiguous. ",
            "8": "This ambiguity forces readers and learners to infer the intended reading using syntactic and semantic context, as well as common sense (Bentin and Frost, 1987;Abu-Rabia, 2001). ",
            "9": "In NLP systems, recovering such signals is difficult, and indeed their performance on Hebrew tasks is adversely affected by the presence of undotted text (Shacham and Wintner, 2007;Goldberg and Elhadad, 2010;Tsarfaty et al., 2019). ",
            "10": "As an example, the sentence in Table 1 (a) will be resolved by a typical reader as (b) in most reasonable contexts, knowing that the word \"softly\" may characterize landings. ",
            "11": "In contrast, an automatic system processing Hebrew text may not be as sensitive to this kind of grammatical knowledge and instead interpret the undotted token as the more frequent word in (c), harming downstream performance. ",
            "12": "'The plane landed congratulations' Table 1: An example of an undotted Hebrew text (a) (written right to left) which can be interpreted in at least two different ways (b,c), dotted and pronounced differently, but only (b) makes grammatical sense.",
            "13": "One possible way to overcome this problem is by adding diacritics to undotted text, or dotting, implemented using data-driven algorithms trained on dotted text. ",
            "14": "Obtaining such data is not trivial, even given correct pronunciation: the standard Tiberian diacritic system contains several sets of identicallyvocalized forms, so while most Hebrew speakers easily read dotted text, they are unable to produce it. ",
            "15": "Moreover, the process of manually adding diacritics in either handwritten script or through digital input devices is mechanically cumbersome. ",
            "16": "Thus, the overwhelming majority of modern Hebrew text is undotted, and manually dotting it requires expertise. ",
            "17": "The resulting scarcity of available dotted text in modern Hebrew contrasts with Biblical and Rabbinical texts which, while dotted, manifest a very different language register. ",
            "18": "This state of affairs allows individuals and companies to offer dotting as paid services, either by experts or automatically, e.g. the Morfix engine by Melingo. ",
            "19": "2 Such usage practices also force a disconnect in the NLP pipeline, requiring an API call into an external service whose parameters cannot be updated.",
            "20": "Existing computational approaches to dotting are manifested as complex, multi-resourced systems which perform morphological analysis on the undotted text and look undotted words up in handcrafted dictionaries as part of the dotting process. ",
            "21": "Dicta's Nakdan (Shmidman et al., 2020), the current state-of-the-art, applies such methods in addition to applying multiple neural networks over different levels of the text, requiring manual annotation not only for dotting but also for morphology. ",
            "22": "Among the resources it uses are a diacritized corpus of 3M tokens and a POS-tagged corpus of 300K tokens. ",
            "23": "Training the model takes several weeks. ",
            "24": "3 In this work, we set out to simplify the dotting task as much as possible to standard modules. ",
            "25": "Our system, NAKDIMON, accepts the undotted character sequence as its input, consults no external resources or lexical components, and produces diacritics for each character, resulting in dotted text whose quality is comparable to that of the commercial Morfix, on both character-level and word-level accuracy. ",
            "26": "Our model is easy to integrate within larger systems that perform end-to-end Hebrew processing tasks, as opposed to the existing proprietary dotters. ",
            "27": "To our knowledge, this is the first attempt at a \"light\" model for Hebrew dotting since early HMM-based systems (Kontorovich, 2001;Gal, 2002). ",
            "28": "We introduce a novel test set for Modern Hebrew dotting, derived from larger and more diverse sources than existing datasets. ",
            "29": "In experiments over our novel dataset, we show that our system is particularly useful in the main use case of modern dotting, which is to convey the desired pronunciation to a reader, and that the errors it makes should be more easily detectable by non-professionals than Dicta's. ",
            "30": "4",
            "31": "Dotting as Sequence Labeling",
            "32": "The input to the dotting task consists of a sequence of characters. ",
            "33": "Each of the characters is assigned three values, from three separate diacritic categories: one category for the dot distinguishing shin ( \u202b)\ufb2a\u202c from sin ( \u202b,)\ufb2b\u202c two consonants sharing a base character \u202b;\u05e9\u202c another for the presence of dagesh/mappiq, a central dot affecting pronunciation of some consonants, e.g. \u202b\ufb44\u202c /p/ from \u202b\u05e4\u202c /f/, but also present elsewhere; and one for all other diacritic marks, which mostly determine vocalization, e.g. \u202b\u05d3\u202c /da/ vs. \u202b\u05d3\u202c /de/. Diacritics of different categories may co-occur on single letters, e.g. , or may be absent altogether.",
            "34": "Full script Hebrew script written without intention of dotting typically employs a compensatory variant known colloquially as full script (ktiv male, \u202b\u05de\u05dc\u05d0\u202c \u202b,)\u05db\u05ea\u05d9\u05d1\u202c which adds instances of the letters \u202b\u05d9\u202c and \u202b\u05d5\u202c in some places where they can aid pronunciation, but are incompatible with the rules for dotted script. ",
            "35": "In our formulation of dotting as a sequence tagging problem, and in collecting our test set from raw text, these added letters may conflict with the dotting standard. ",
            "36": "For the sake of input integrity, and unlike some other systems, we opt not to remove these characters, but instead employ a dotting policy consistent with full script. ",
            "37": "See Appendix A for further details.",
            "38": "New test set Shmidman et al. (2020) ",
            "39": "provide a benchmark dataset for dotting modern Hebrew documents. ",
            "40": "However, it is relatively small and nondiverse: all 22 documents in the dataset originate in a single source, namely Hebrew Wikipedia articles. ",
            "41": "Therefore, we created a new test set 5 from a larger variety of texts, including high-quality Wikipedia articles and edited news stories, as well as usergenerated blog posts. ",
            "42": "This set consists of ten documents from each of eleven sources (5x Dicta's test set), and totals 20,796 Hebrew tokens, roughly 3.5x Dicta's.",
            "43": "Character-LSTM Dotter",
            "44": "NAKDIMON embeds the input characters and passes them through a two-layer Bi-LSTM (Hochreiter and Schmidhuber, 1997). ",
            "45": "The LSTM output is fed into a single linear layer, which then feeds three linear layers, one for each diacritic category (see \u00a72). ",
            "46": "Each character then receives a prediction for each category independently, and decoding is performed greedily. ",
            "47": "In training, we sum the cross-entropy loss from all categories. ",
            "48": "Trivial decisions, such as the label for the shin/sin diacritic for any non-\u202b\u05e9\u202c letter, are masked.",
            "49": "Training corpora Dotted modern Hebrew text is scarce, since speakers usually read and write undotted text, with the occasional diacritic added for disambiguation when context does not suffice. ",
            "50": "As we are unaware of legally-obtainable dotted modern corpora, we use a combination of dotted pre-modern texts and semi-automatically dotted modern sources to train NAKDIMON:",
            "51": "The PRE-MODERN portion is obtained from two main sources: A combination of late pre-modern text from Project Ben-Yehuda, mostly texts from the late 19th century and the early 20th century; 6 and rabbinical texts from the medieval period, the most important of which is Mishneh Torah. ",
            "52": "7 This portion contains roughly 2.6M Hebrew tokens, most of which are dotted, with a varying level of accuracy, varying dotting styles, and varying degree of similarity to Modern Hebrew.",
            "53": "The AUTOMATIC portion contains 547 short stories taken from the short story project. ",
            "54": "8 The stories are dotted using Dicta without manual validation. ",
            "55": "The corpus contains roughly 1.3M Hebrew tokens.",
            "56": "Lastly, the MODERN portion contains manually collected text in Modern Hebrew, mostly from undotted sources, which we dot using Dicta and follow up by manually fixing errors, either using Dicta's API or via automated scripts which catch common mistakes. ",
            "57": "We use the same technique and style for dotting this corpus as we do for our test corpus ( \u00a72), but the documents were collected in different ways. ",
            "58": "We made an effort to collect a diverse set of sources: news, opinion columns, paragraphs from books, short stories, Wikipedia articles, governmental publications, blog posts and forums expressing various domains and voices, and more. ",
            "59": "Our MODERN corpus contains roughly 300,000 Hebrew tokens, and is much more consistent and similar to the expectation of a native Hebrew speaker than the PRE-MODERN or the AU-TOMATIC corpora, and more accurately dotted than the AUTOMATIC corpus. ",
            "60": "The sources and statistics of this dataset are presented in Table 2.",
            "61": "Experiments",
            "62": "We compare the performance of NAKDIMON against Dicta (retrieved 2022-1-9), Snopi, 9 and Morfix (Kamir et al., 2002), on our new test set ( \u00a72). ",
            "63": "10 We report the following metrics: decision accuracy (DEC) is computed over the entire set of individual possible decisions: dagesh/mappiq for letters that allow it, sin/shin dot for the letter \u202b,\u05e9\u202c and all other diacritics for letters that allow them; char-6 Obtained from the Ben-Yehuda Project www.benyehuda.org.",
            "64": "7 Obtained from Project Mamre www.mechon-mamre. ",
            "65": "org.",
            "66": "8 www.shortstoryproject.com/he/ 9 http://www.nakdan.com/Nakdan.aspx 10 Results on Dicta's test set (Shmidman et al., 2020)   We train NAKDIMON over PRE-MODERN for a single epoch, followed by two epochs over the AU-TOMATIC corpus, and then by three epochs over the MODERN corpus. ",
            "67": "We pre-process the input by removing all but Hebrew characters, spaces and punctuation; digits are converted to a dedicated symbol, as are Latin characters. ",
            "68": "We strip all existing diacritic marks. ",
            "69": "We split the documents into chunks of at most 80 characters 12 bounded at whitespace, ignoring sentence boundaries. ",
            "70": "We optimize using Adam (Kingma and Ba, 2014)",
            "71": "Results",
            "72": "We provide document-level macro-averaged accuracy percentage results for a single run over our test set in Table 3. ",
            "73": "NAKDIMON outperforms Morfix on character-level metrics but not on word-level metrics, mostly since Morfix ignores certain words altogether, incurring errors on multiple characters.",
            "74": "We note the substantial improvement our model achieves on the VOC metric compared to the WOR metric: 18.43% of word-level errors are attributable to vocalization-agnostic dotting, compared to 13.80% for Dicta and 10.41% for Snopi (but 20.91% for Morfix). ",
            "75": "Considering that the central use case for dotting modern Hebrew text is to facilitate pronunciation to learners and for reading, and that undotted homograph ambiguity typically comes with pronunciation differences, we believe this measure to be no less important than WOR.",
            "76": "Error analysis",
            "77": "In Table 4 we present examples of words dotted incorrectly, or correctly, only by NAKDIMON, compared with Morfix and Dicta. ",
            "78": "The largest category for NAKDIMON-only errors (\u223c18% of 90 sampled) are ones where a fused preposition+determiner character is dotted to only include the preposition, perhaps due to its inability to detect the explicit determiner clitic \u202b\u05d4\u202c in neighboring words, on which the complex systems apply morphological segmentation. ",
            "79": "In other cases (\u223c15%), NAKDIMON creates unreadable vocalization sequences, as it has no lexical component and is decoded greedily. ",
            "80": "These types of errors are more friendly to the typical use cases of a dotting system, as they are likely to stand out to a reader. ",
            "81": "In contrast, a large portion of cases where NAKDIMON was exclusively correct (\u223c13% of 152) are foreign names and terms. ",
            "82": "This may be the result of such words not yet appearing in dictionaries, or not being easily separable from an adjoining clitic, while character-level information can capture pronunciation patterns from similar words (e.g. \u202b\u05d8\u05dc\u05e4\u05d5\u202c 'telephone', for the example).",
            "83": "Related Work",
            "84": "Existing work on diacritizing Hebrew is not common, and all efforts build on word-level features. ",
            "85": "In Arabic, diacritization serves a comparable purpose to that in Hebrew, but not exclusively: most diacritic marks differentiate letters from each other (which only the sin/shin dot does in Hebrew), while vocalization marks are in a one-to-one relationship with their phonetic realizations. ",
            "86": "Dictionary-less Arabic diacritization has been attempted using a 3layer Bi-LSTM (Belinkov and Glass, 2015). ",
            "87": "Abandah et al. (2015) use a Bi-LSTM where characters are assigned either one or more diacritic symbols. ",
            "88": "Our system differs from theirs by virtue of separating the diacritization categories. ",
            "89": "Mubarak et al. (2019) tackled Arabic diacritization as a sequenceto-sequence problem, tasking the model with reproducing the characters as well as the marks. ",
            "90": "Zalmout and Habash (2017) have made the case against RNN-only systems, arguing for the importance of morphological analyzers in Arabic NLP systems. ",
            "91": "We concede that well-curated systems perform better than ours, noting that they are difficult to train for individual use-cases and are burdensome to deploy as services.",
            "92": "Conclusion",
            "93": "Learning directly from plain diacritized text can go a long way, even with relatively limited resources. ",
            "94": "NAKDIMON demonstrates that a simple architecture for diacritizing Hebrew text as a sequence tagging problem can achieve performance on par with much more complex systems. ",
            "95": "We also introduce and release a corpus of dotted Hebrew text, as well as a source-balanced test set. ",
            "96": "In the future, we wish to evaluate the utility of dotting as a feature for downstream tasks such as question answering and speech generation, taking advantage of the fact that our simplified model can be easily integrated in an end-to-end Hebrew processing system. ",
            "97": " We cannot report our full NAKDIMON's performance on the former, as we use the test set for parts of its training.",
            "98": "We apply the following resolution tactics for added letters in undotted text: (a) We almost never remove or add letters to the original text (unless it is completely undiacritizable). ",
            "99": "(b) We keep dagesh in letters that follow a shuruk which replaces a kubuts, and similarly for yod (hirik male replacing hirik haser). ",
            "100": "(c) When we have double vav or double yod, the second letter is usually left undotted, except when it is impossible to have the correct vocalization this way.",
            "101": "Resolving ktiv haser discrepancies from Morfix outputs is done by adding missing vowel letters, or removing superfluous vowel letters, in such a way that would not count as an error if it is correct according to Academy regulations.",
            "102": "While developing NAKDIMON, we performed several evaluations over a held-out validation set of 40 documents with 27,681 tokens, on which Dicta performs at 91.56% WOR accuracy. ",
            "103": "Figure 1 shows the favorable effect of training NAKDIMON over an increasing amount of MODERN text.",
            "104": "We tried to further improve NAKDIMON by initializing its parameters from a language model trained to predict masked characters in a large undotted Wikipedia corpus (440MB, 30% mask rate), but were only able to achieve an improvement of 0.07%. ",
            "105": "Attempted architectural modifications, including substituting a Transformer (Vaswani et al., 2017) for the LSTM; adding a CRF layer to the decoding process; and adding a residual connection between the character LSTM layers, yielded no substantial benefits in these experiments. ",
            "106": "Similarly, varying the number of LSTM layers between 2 and 5 (keeping the total number of parameters roughly constant, close to the 5, 313, 223 parameters of our final model) has little to no impact on the accuracy",
            "107": "We present results for the Dicta test set in Table 5. ",
            "108": "In order to provide fair comparison and to preempt overfitting on this test data, we ran this test in a preliminary setup on a variant of NAKDIMON which was not tuned or otherwise unfairly trained. ",
            "109": "This system, NAKDIMON 0 , differs from our final variant in three main aspects: it is not trained on the Dicta portion of our training corpus ( \u00a73), it is not trained on the AUTOMATIC corpus, and it employs a residual connection between the two character Bi-LSTM layers. ",
            "110": "Testing on the Dicta test set required some minimal evaluation adaptations resulting from encoding constraints (for example, we do not distinguish between kamatz katan and kamatz gadol). ",
            "111": "Thus, we copy the results reported in Shmidman et al. (2020) as well as our replication.",
            "112": "We see that the untuned NAKDIMON 0 performs on par with the proprietary Morfix, which uses word-level dictionary data, consistent with our main results on our novel test set."
        }
    },
    "cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3": {
        "doc1": {
            "0": "paper_summary:",
            "1": "This paper extends the worst-case-aware curriculum learning work (Zhang et al., 2020) to zero-shot dependency parsing.",
            "2": "The key idea is to control the sampling to sample more from languages with higher losses.",
            "3": "Empirical analyses show that the proposed method improves zero-shot accuracy on unseen languages, outperforming uniform sampling or sampling proportionally to the size of the treebanks.",
            "4": "summary_of_strengths:",
            "5": "- Zero-shot dependency parsing is a difficult but valuable task in both academia and industry.",
            "6": "- The proposed method is intuitive.",
            "7": "- The empirical analyses were conducted on a large set of languages, which makes the results more convincible.",
            "8": "summary_of_weaknesses:",
            "9": "- Based on the empirical analyses (Table 2), it is not clear if pure worst-case-aware sampling ($\\phi=0$) is consistently better than pure loss-proportional sampling ($\\phi=1$).",
            "10": "It seems the results on mBERT and XLM-R give different conclusions.",
            "11": "The relatively small differences between $\\phi=0$ and $\\phi=1$ in the mBERT and XLM-R settings also make me wonder if the differences are statistically significant.",
            "12": "- The proposed work is an extension of previous Zhang et al., 2020. So the contribution seems a bit incremental.",
            "13": "comments,_suggestions_and_typos:",
            "14": "- Line 183: it would be good to show what languages are used in the PLM pretraining and if there is a difference of the proposed method between the seen and unseen languages.",
            "15": "- Table 2: there are only three values of $\\phi$ listed, and there are no difference between $\\phi=0.5$ and $\\phi=1$. It would be good to show more details on how $\\phi$ affects the final performance.",
            "16": "- Table 3: which PLM is used in this experiment?",
            "17": "What is the $\\phi$?"
        },
        "doc2": {
            "0": "Zero-Shot Dependency Parsing with Worst-Case Aware Automated Curriculum Learning",
            "1": "Abstract",
            "2": "Large multilingual pretrained language models such as mBERT and XLM-RoBERTa have been found to be surprisingly effective for cross-lingual transfer of syntactic parsing models (Wu and Dredze, 2019), but only between related languages. ",
            "3": "However, source and training languages are rarely related, when parsing truly low-resource languages. ",
            "4": "To close this gap, we adopt a method from multi-task learning, which relies on automated curriculum learning, to dynamically optimize for parsing performance on outlier languages. ",
            "5": "We show that this approach is significantly better than uniform and size-proportional sampling in the zero-shot setting.",
            "6": "Introduction",
            "7": "The field of multilingual NLP is booming (Agirre, 2020). ",
            "8": "This is due in no small part to large multilingual pretrained language models (PLMs) such as mBERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020), which have been found to have surprising cross-lingual transfer capabilities in spite of receiving no cross-lingual supervision. ",
            "9": "1 Wu and Dredze (2019), for example, found mBERT to perform well in a zero-shot setting when finetuned for five different NLP tasks in different languages. ",
            "10": "There is, however, a sharp divide between languages that benefit from this transfer and languages that do not, and there is ample evidence that transfer works best between typologically similar languages (Pires et al., 2019). ",
            "11": "This means that 1 In the early days, cross-lingual transfer for dependency parsing relied on projection across word alignments (Spreyer and Kuhn, 2009;Agi\u0107 et al., 2016) or delexicalized transfer of abstract syntactic features (Zeman and Resnik, 2008;McDonald et al., 2011;S\u00f8gaard, 2011;Cohen et al., 2011). ",
            "12": "Delexicalized transfer was later 're-lexicalized' by word clusters (T\u00e4ckstr\u00f6m et al., 2012) and word embeddings (Duong et al., 2015), but with the introduction of multilingual contextualized language models, transfer models no longer rely on abstract syntactic features, removing an important bottleneck for transfer approaches to scale to truly low-resource languages.",
            "13": "the majority of world languages that are truly lowresource are still left behind and inequalities in access to language technology are increasing.",
            "14": "Large multilingual PLMs are typically fine-tuned using training data from a sample of languages that is supposed to be representative of the languages that the models are later applied to. ",
            "15": "However, this is difficult to achieve in practice, as multilingual datasets are not well balanced for typological diversity and contain a skewed distribution of typological features (Ponti et al., 2021). ",
            "16": "This problem can be mitigated by using methods that sample from skewed distributions in a way that is robust to outliers. ",
            "17": "Zhang et al. (2020) recently developed such a method. ",
            "18": "It uses curriculum learning with a worst-case-aware loss for multi-task learning. ",
            "19": "They trained their model on a subset of the GLUE benchmark (Wang et al., 2018) and tested on outlier tasks. ",
            "20": "This led to improved zero-shot performance on these outlier tasks. ",
            "21": "This method can be applied to multilingual NLP where different languages are considered different tasks. ",
            "22": "This is what we do in this work, for the case of multilingual dependency parsing. ",
            "23": "Multilingual dependency parsing is an ideal test case for this method, as the Universal Dependency treebanks (Nivre et al., 2020) are currently the manually annotated dataset that covers the most typological diversity (Ponti et al., 2021). ",
            "24": "Our research question can be formulated as such: Can worst-case aware automated curriculum learning improve zero-shot dependency parsing?",
            "25": "2 Worst-Case-Aware Curriculum Learning",
            "26": "In multi-task learning, the total loss is generally the average of losses of different tasks:",
            "27": "where l i is the loss of task i. The architecture we use in this paper is adapted from Zhang et al. (2020), which is an automated curriculum learning (Graves et al., 2017) framework to learn a worstcase-aware loss in a multi-task learning scenario.",
            "28": "The architecture consists of a sampler, a buffer, a trainer and a multilingual dependency parsing model. ",
            "29": "The two main components are the sampler, which adopts a curriculum sampling strategy to dynamically sample data batches, and the trainer which uses worst-case-aware strategy to train the model. ",
            "30": "The framework repeats the following steps:",
            "31": "(1) the sampler samples data batches of different languages to the buffer; (2) the trainer uses a worstcase strategy to train the model; (3) the automated curriculum learning strategy of the sampler is updated.",
            "32": "Sampling data batches We view multilingual dependency parsing as multi-task learning where parsing in each individual language is considered a task. ",
            "33": "This means that the target of the sampler at each step is to choose a data batch from one language. ",
            "34": "This is a typical multi-arm bandit problem (Even-Dar et al., 2002). ",
            "35": "The sampler should choose bandits that have higher rewards, and in our scenario, data batches that have a higher loss on the model are more likely to be selected by the sampler and therefore, in a later stage, used by the trainer. ",
            "36": "Automated curriculum learning is adopted to push a batch with its loss into the buffer at each time step. ",
            "37": "The buffer consists of n first-in-first-out queues, and each queue corresponds to a task (in our case, a language). ",
            "38": "The procedure repeats k times and, at each round, k data batches are pushed into the buffer.",
            "39": "Worst-case-aware risk minimization In multilingual and multi-task learning scenarios, in which we jointly minimize our risk across n languages or tasks, we are confronted with the question of how to summarize n losses. ",
            "40": "In other words, the question is how to compare two loss vectors \u03b1 and \u03b2 containing losses for all tasks l i , . . . ",
            "41": "l n :",
            "42": "The most obvious thing to do is to minimize the mean of the n losses, asking whether \u2208\u03b1 < \u2208\u03b2 . ",
            "43": "We could also, motivated by robustness (S\u00f8gaard, 2013) and fairness (Williamson and Menon, 2019), minimize the maximum (supremum) of the n losses, asking whether max \u2208\u03b1 < max \u2208\u03b2 . ",
            "44": "Mehta et al. (2012) observed that these two loss summarizations are extremes that can be generalized by a family of multi-task loss functions that summarize the loss of n tasks as the L p norm of the n-dimensional loss vector. ",
            "45": "Minimizing the average loss then corresponds to computing the L 1 norm, i.e., asking whether |\u03b1| 1 < |\u03b2| 1 , and minimizing the worst-case loss corresponds to computing the L \u221e (supremum) norm, i.e., asking whether |\u03b1| \u221e < |\u03b2| \u221e .",
            "46": "Zhang et al. ( 2020) present a stochastic generalization of the L \u221e loss summarization and a practical approach to minimizing this family of losses through automated curriculum learning (Graves et al., 2017): The core idea behind their generalization is to optimize the worst-case loss with a certain probability, otherwise optimize the average (loss-proportional) loss with the remaining probability. ",
            "47": "The hyperparameter \u03c6 is introduced by the worst-case-aware risk minimization to trade off the balance between the worst-case and the lossproportional losses. ",
            "48": "The loss family is formally defined as:",
            "49": "where p \u2208 [0, 1] is a random generated rational number, and P = i j\u2264n j is the normalized probability distribution of task losses. ",
            "50": "If p < \u03c6 the model choose the maximum loss among all tasks, otherwise, it randomly chooses one loss according to the loss distribution. ",
            "51": "If the hyperparameter \u03c6 equals 1, the trainer updates the model with respect to the worst-case loss. ",
            "52": "On the contrary, if \u03c6 = 0, the trainer loss-proportionally samples one loss.",
            "53": "The model updates its parameters with respect to the loss chosen by the trainer. ",
            "54": "After that, the sampler updates its policy according to the behavior of the trainer. ",
            "55": "At each round, the policy of the task that is selected by the trainer receives positive rewards and the policy of all other tasks that have been selected by the sampler receive negative rewards.",
            "56": "We use a standard biaffine graph-based dependency parser (Dozat and Manning, 2017). ",
            "57": "The model takes token representations of words from a contextualized language model (mBERT or XLM-R) as input and classifies head and dependency relations between words in the sentence. ",
            "58": "The Chu-Liu-Edmonds algorithm (Chu and Liu, 1965;Edmonds, 1967) is then used to decode the score matrix into a tree. ",
            "59": "All languages share the same encoder and decoder in order to learn features from different languages, and more importantly to perform zero-shot transfer to unseen languages.",
            "60": "Experiments",
            "61": "We base our experimental design on \u00dcst\u00fcn et al.",
            "62": "(2020), a recent paper doing zero-shot dependency parsing with good performance on a large number of languages. ",
            "63": "They fine-tune mBERT for dependency parsing using training data from a sample of 13 typologically diverse languages from Universal Dependencies (UD; Nivre et al., 2020), listed in Table 1. ",
            "64": "For testing, they use 30 test sets from treebanks whose language has not been seen at finetuning time. ",
            "65": "We use the same training and test sets and experiment both with mBERT and XLM-R as PLMs. ",
            "66": "It is important to note that not all of the test languages have been seen by the PLMs. ",
            "67": "We test worst-case aware learning with different values of \u03c6 and compare this to three main baselines: size-proportional samples batches proportionally to the data sizes of the training treebanks, uniform samples from different treebanks with equal probability, thereby effectively reducing the size of the training data, and smooth-sampling uses the smooth sampling method developed in van der Goot et al. ( 2021) which samples from multiple languages using a multinomial distribution. ",
            "68": "These baselines are competitive with the state-ofthe-art when using mBERT, they are within 0.2 to 0.4 LAS points from the baseline of \u00dcst\u00fcn et al. (2020) on the same test sets. ",
            "69": "When using XLM-R, they are largely above the state-of-the-art. ",
            "70": "We implement all models using MaChAmp (van der Goot et al., 2021), a library for multi-task learning based on AllenNLP (Gardner et al., 2018). ",
            "71": "The library uses transformers from HuggingFace (Wolf et al., 2020). ",
            "72": "We make our code publicly available.",
            "73": "Our main results are in Table 2 where we report average scores across test sets, for space reasons. ",
            "74": "Tables with results broken down by test treebank can be found in Appendix A. We can see that worstcase-aware training outperforms all of our baselines in the zero-shot setting, highlighting the effectiveness of this method. ",
            "75": "This answers positively our research question Can worst-case aware automated curriculum learning improve zero-shot dependency parsing?",
            "76": "Our results using mBERT are more than 1 LAS point above the corresponding baselines. ",
            "77": "Our best model with mBERT comes close to Udapter (36.5 LAS on the same test sets) while being a lot simpler and not using external resources such as typological features, which are not always available for truly low-resource languages.",
            "78": "The results with XLM-R are much higher in general 2 but the trends are similar: all our models outperform all of our baselines albeit with smaller differences (there is only a 0.4 LAS difference between our best model and the best baseline). ",
            "79": "This highlights the robustness of the XLM-R model itself. ",
            "80": "Our results with XLM-R outperform Udapter by close to 7 LAS points. ",
            "81": " (de Lhoneux et al., 2017;Schluter and Agi\u0107, 2017;de Lhoneux, 2019). ",
            "82": "We can, however, easily construct samples that are not representative, for example, by taking a sample of related languages. ",
            "83": "We expect worst-case aware learning to lead to larger improvements in cases where some language types are underrepresented in the sample. ",
            "84": "We can construct an extreme case of underrepresentation by selecting a sample of training languages that has one or more clear outliers. ",
            "85": "For example we can construct a sample of related languages, add a single unrelated language in the mix, and then evaluate on other unrelated languages. ",
            "86": "We also expect that with a typologically diverse set of training languages, worst-case aware learning should lead to larger relative improvements than with a homogeneous sample, but perhaps slightly smaller improvements than with a very skewed sample.",
            "87": "We test these hypotheses by constructing seven samples of training languages in addition to the one used so far (13LANG). ",
            "88": "We construct three different homogeneous samples using treebanks from three different genera: GERMANIC, ROMANCE and SLAVIC. ",
            "89": "We construct four skewed samples using the sample of romance languages and a language from a different language family, an outlier language: Basque (eu), Arabic (ar), Turkish (tr) and Chinese (zh). ",
            "90": "Since we keep the sample of test sets constant, we do not include training data from languages that are in the test sets. ",
            "91": "The details of which treebanks are used for each of these samples can be found in Table 5 in Appendix B.",
            "92": "We can see first that, as expected, our typologically diverse sample performs best overall. ",
            "93": "This indicates that it is a good sample. ",
            "94": "We can also see that, as expected, the method works best with a skewed sample: the largest gains from using worst-case learning, both in terms of absolute LAS difference and relative error reduction, are seen for a skewed sample (ROM+EU). ",
            "95": "However, contrary to expectations, the lowest gains are obtained for another skewed sample (ROM+AR). ",
            "96": "The gains are also low for ROM+TR, ROM+ZH and for GER-MANIC. ",
            "97": "Additionally, there are slightly more gains from using worst-case aware learning with the SLAVIC sample than for our typologically diverse sample. ",
            "98": "These results could be due to the different scripts of the languages involved both in training and testing.",
            "99": "Looking at results of the different models on individual test languages (see Figure 1 in Appendix C), we find no clear pattern of the settings in which this method works best. ",
            "100": "We do note that the method always hurts Belarusian, which is perhaps unsurprising given that it is the test treebank for which the baseline is highest. ",
            "101": "Worst-case aware learning hurts Belarusian the least when using the SLAVIC sample, indicating that, when using the other samples, the languages related to Belarusian are likely downsampled in favour of languages unrelated to it. ",
            "102": "Worst-case learning consistently helps Breton and Swiss German, indicating that the method might work best for languages that are underrepresented within their language family but not necessarily outside of it. ",
            "103": "For Swiss German, worst-case learning helps least when using the GERMANIC sample where it is less of an outlier.",
            "104": "Conclusion",
            "105": "In this work, we have adopted a method from multitask learning which relies on automated curriculum learning to the case of multilingual dependency parsing. ",
            "106": "This method allows to dynamically optimize for parsing performance on outlier languages. ",
            "107": "We found this method to improve dependency parsing on a sample of 30 test languages in the zeroshot setting, compared to sampling data uniformly across treebanks from different languages, or proportionally to the size of the treebanks. ",
            "108": "We investigated the impact of varying the homogeneity of the sample of training treebanks on the usefulness of the method and found conflicting evidence with different samples. ",
            "109": "This leaves open questions about the relationship between the languages used for training and the ones used for testing. ",
            "110": "mBERT XLM-R iso \u03c6=0 \u03c6=0.5 \u03c6=1 S-P S-S U \u03c6=0 \u03c6=0. ",
            "111": " 2020) using mBERT and XLM-R. S-P=size-proportional, S-S = smooth-sampling, U=uniform. ",
            "112": "Bold indicates the best performance across models using the same PLM."
        }
    },
    "48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0": {
        "doc1": {
            "0": "paper_summary:",
            "1": "This paper proposed how to adapt the graph-based semantic parser PERIN to the task of structured sentiment analysis, aiming to analyze a polar expression, an optional holder, an optional sentiment target, and sentiment polarity.",
            "2": "Based on PERIN, the weighted bipartite graph between all queries and nodes is applied to indicate the prior ordering of the graph.",
            "3": "The proposed method advances the-state-of-the art method on 4 out of 5 standard benchmark sets.",
            "4": "summary_of_strengths:",
            "5": "1) This work applies the graph-based semantic parser PERIN for structured sentiment analysis with task-specific adaption, which refines the parallel queries process and gold nodes mapping.",
            "6": "2) The paper is clearly written and well structured.",
            "7": "Given the page limitation of a short paper, this paper still provides abundant information.",
            "8": "3) The results of performance evaluation are quite convincing for the comparison with state-of-the-art methods and complete experimental settings.",
            "9": "summary_of_weaknesses:",
            "10": "1) Lacking innovation is the main weakness of this paper, though proven the usefulness of PERIN and the refinement.",
            "11": "2) Due to the page limitation, this work lacks detailed analysis on Node-centric encoding, Labeled-edge encoding and Opinion-tuple encoding, which are the essential design towards SSA.",
            "12": "comments,_suggestions_and_typos:",
            "13": "1) In Figure 2, please indicate STEP 1,2,3,4 in the corresponding places in the diagram for better illustration.",
            "14": "2) It would be better to present a few case studies for readers."
        },
        "doc2": {
            "0": "Direct parsing to sentiment graphs",
            "1": "Abstract",
            "2": "This paper demonstrates how a graph-based semantic parser can be applied to the task of structured sentiment analysis, directly predicting sentiment graphs from text. ",
            "3": "We advance the state of the art on 4 out of 5 standard benchmark sets. ",
            "4": "We release the source code, models and predictions with the camera-ready version.",
            "5": "Introduction",
            "6": "The task of structured sentiment analysis (SSA) is aimed at locating all opinion tuples within a sentence, where a single opinion contains a) a polar expression, b) an optional holder, c) an optional sentiment target, and d) a positive, negative or neutral polarity. ",
            "7": "An example is provided in Figure 1. ",
            "8": "While there have been sentiment corpora annotated with this type of information for decades (Wiebe et al., 2005;Toprak et al., 2010), there have so far been few attempts at modeling the full representation, rather focusing on various subcomponents, such as the polar expressions and targets without explicitly expressing the relations (Peng et al., 2019;Xu et al., 2020) or the polarity (Yang and Cardie, 2013;Katiyar and Cardie, 2016).",
            "9": "Dependency parsing approaches have recently shown promising results for SSA (Barnes et al., 2021;Peng et al., 2021). ",
            "10": "Here we present a novel sentiment parser which, unlike previous attempts, predicts sentiment graphs directly from text without reliance on heuristic lossy conversions to intermediate dependency representations. ",
            "11": "The model takes inspiration from successful work in meaning representation parsing, and in particular the permutation-invariant graph-based parser of Samuel and Straka (2020) called PERIN.",
            "12": "Experimenting with several different graph encodings, we evaluate our approach on five datasets from four different languages, and find that it compares favorably to dependency-based baselines across all datasets; most significantly on the more structurally complex ones -NoReC and MPQA. . \"",
            "13": "Figure 1: A sentiment graph for the phrase \"I actually enjoyed the bad acting\", which contains an example of nesting of two opposing opinions.",
            "14": "Related work",
            "15": "Proposing a dependency parsing approach to the full task of SSA, Barnes et al. (2021) show that it leads to strong improvements over state-of-the-art baselines. ",
            "16": "Peng et al. (2021) propose a sparse fuzzy attention mechanism to deal with the sparseness of dependency arcs in the models from Barnes et al. (2021) and show further improvements. ",
            "17": "However, in order to apply the parsing algorithm of Dozat and Manning (2018), both of these approaches have to rely on a lossy conversion to bi-lexical dependencies with ad-hoc internal head choices for the nodes of the abstract sentiment graph. ",
            "18": "This lossy behaviour is caused by nested text spans in the sentiment graphs, as illustrated by Figure 1, which are ambiguous in their bi-lexical dependency encoding (see Section A in the Appendix). ",
            "19": "More generally, decoding structured graph information from text has sparked a lot of interest in recent years, especially for parsing meaning representation graphs (Oepen et al., 2020). ",
            "20": "There has been tremendous progress in developing complex transition-based and graph-based parsers (Hershcovich et al., 2017;McDonald and Pereira, 2006;Dozat and Manning, 2018). ",
            "21": "In this paper, we adopt PERIN (Samuel and Straka, 2020), a state-of-theart graph-based parser capable of modeling a superset of graph features needed for our task.",
            "22": "PERIN model",
            "23": "PERIN is a general permutation-invariant text-tograph parser. ",
            "24": "We briefly describe our modified SSA version, please consult the original work for more details (Samuel and Straka, 2020).",
            "25": "Architecture",
            "26": "PERIN processes the input text in four steps, illustrated in Figure 2: 1) To encode the input, PERIN uses contextualized embeddings from XLM-R (base size; Conneau et al., 2020) and combines them with learned character-level embeddings; 2) each token is mapped onto latent queries by a linear transformation; 3) a stack of Transformer layers (Vaswani et al., 2017) optionally models the inter-query dependencies; and 4) classification heads select and label queries onto nodes, establish anchoring from nodes to tokens, and predict the node-to-node edges.",
            "27": "Permutation-invariant query-to-node matching",
            "28": "Traditional graph-based parsers are trained as autoregressive sequence-to-sequence models. ",
            "29": "PERIN does not assume any prior ordering of the graph nodes. ",
            "30": "Instead, it processes all queries in parallel and then dynamically maps them to gold nodes.",
            "31": "Based on the predicted probabilities of labels and anchors, we create a weighted bipartite graph between all queries and nodes. ",
            "32": "Our goal is to find the most probable matching, which can be done efficiently in polynomial time by using the Hungarian algorithm. ",
            "33": "Finally, every node is assigned to a query and we can backpropagate through standard cross-entropy losses to update the model weights.",
            "34": "Graph encodings",
            "35": "PERIN defines an overall framework for general graph parsing, it can cater to specific graph encodings by changing the subset of its classification heads. ",
            "36": "In parsing the abstract sentiment structures, there are several possible lossless graph encodings depending on the positioning of the polarity information and the sentiment node type (see Figure 3):",
            "37": "1. ",
            "38": "Node-centric encoding, with labeled nodes and directed unlabeled arcs. ",
            "39": "Each node corresponds to a target, holder or sentiment expression; edges form their relationships. ",
            "40": "The parser uses a multi-class node head, an anchor head and a binary edge classification head. \"Norbert loves flowers\" finetuned base XLM-R q 1,1 q 1,2 q 2,1 q 2,2 q 3,1 q 3,2 2. ",
            "41": "Labeled-edge encoding, with deduplicated unlabeled nodes and labeled arcs. ",
            "42": "Each node corresponds to a unique text span from some sentiment graph, while edge labels denote their relationships and functions. ",
            "43": "The model has a binary node classifier, an anchor classifier and a binary and multi-class edge head. ",
            "44": "3. ",
            "45": "Opinion-tuple encoding, which represents the structured sentiment information as a sequence of opinion four-tuples. ",
            "46": "This encoding is the most restrictive, having the lowest degrees of freedom. ",
            "47": "The parser utilizes a multiclass node head and three anchor classifiers, it does not need an edge classifier.",
            "48": "Experiments",
            "49": "Following Barnes et al. (2021) we perform experiments on five structured sentiment datasets in four languages, the statistics of which are shown in Table 1. ",
            "50": "The largest dataset is the NoReC f ine dataset (\u00d8vrelid et al., 2020), a multi-domain dataset of professional reviews in Norwegian. ",
            "51": "EU and CA (Barnes et al., 2018) contain hotel reviews in Basque and Catalan, respectively. ",
            "52": "MPQA (Wiebe et al., 2005) annotates news wire text in English. ",
            "53": "Finally, DSU (Toprak et al., 2010) annotates English reviews of online universities. ",
            "54": "We use the SemEval 2022 releases of MPQA and DSU.",
            "55": "Evaluation",
            "56": "Following Barnes et al. (2021), we evaluate our models using both token-level F 1 for extraction of Holders, Targets, and polar Expressions, as well as the graph-level metrics Non-polar Sentiment Graph F 1 (NSF 1 ) and Sentiment Graph F 1 (SF 1 ), weighing the overlap in predicted and gold spans for each entity, averaged across all three spans. ",
            "57": "SF 1 , which also includes polarity, is considered the primary metric for the full SSA task.",
            "58": "Models",
            "59": "We compare our models to the head-final dependency graph parsers from Barnes et al. (2021) as well as the second-order Sparse Fuzzy Attention parser of Peng et al. (2021). ",
            "60": "For all models, we perform 5 runs with 5 different random seeds and report the mean and standard deviation. ",
            "61": "Results on development splits are provided in Appendix D, training details are in Appendix E.",
            "62": "Results",
            "63": "Table 2 shows the main results. ",
            "64": "Our models outperform both dependency graph models on SF 1 , although the results are mixed for span extraction.",
            "65": "The opinion-tuple encoding gives the best performance on SF 1 (an average of 6.2 percentage points (pp.) better than Peng et al. ( 2021)), followed by the labeled edge encoding (3.0) and finally the node-centric encoding (2.1).",
            "66": "For extracting spans, the opinion tuple encoding also achieves the the best results on NoReC, either labeled-edge or node centric on CA and MPQA, while Peng et al. ( 2021) is best on EU and DSU. ",
            "67": "This suggests that the main benefit of PERIN is at the structural level, rather than local extraction.",
            "68": "Analysis",
            "69": "There are a number of architectural differences between the dependency parsing approaches compared above. ",
            "70": "In this section, we aim to isolate the effect of predicting intermediate dependency graphs vs. directly predicting sentiment graphs by creating more comparable dependency 2 and PERIN models. ",
            "71": "We adapt the dependency model from Barnes et al. (2021) by removing the token, lemma, and POS embeddings and replacing mBERT (Devlin et al., 2019) with XLM-R (Conneau et al., 2020). ",
            "72": "The 'XLM-R dependency' model thus has character LSTM embeddings and token-level XLM-R features. ",
            "73": "Since these are not updated during training, for the opinion-tuple 'Frozen PERIN' model, we fix the XLM-R weights to make it comparable.",
            "74": "As shown in Table 3, predicting the sentiment graph directly leads to an average gain of 3.7 pp. ",
            "75": "on the Sentiment Graph F 1 metric. ",
            "76": "For extracting the spans of holder, target, and polar expressions, the 58.9 \u00b11.1 63.5 \u00b11.5 73.9 \u00b10.6 59.8 \u00b10.7 58.6 \u00b10.7 PERIN -labeled edge 57.6 \u00b12.5 64.9 \u00b10.8 72.5 \u00b11.9 60.0 \u00b11.4 58.8 \u00b11.3 PERIN -opinion-tuple 64.2 \u00b12.5 67.4 \u00b10.8 73.2 \u00b11.2 62.5 benefit is less clear. ",
            "77": "Here, the PERIN model only outperforms the XLM-R dependency model 5 of 15 times, which seems to confirm that its benefit is at the graph level. ",
            "78": "This is further supported by the fact that the highest gains are found on the datasets with the most nested sentiment expressions and dependency arcs lost due to overlap, which are difficult to encode in bi-lexical graphs (see Appendix A).",
            "79": "Conclusion",
            "80": "Previous work cast the task of structured sentiment analysis (SSA) as dependency parsing, converting the sentiment graphs into lossy dependency graphs. ",
            "81": "We present a novel sentiment parser which, unlike previous attempts, predicts sentiment graphs directly from text without reliance on lossy dependency representations. ",
            "82": "We adapted a state-ofthe-art meaning representation parser to SSA and experimentally evaluated three candidate graph encodings of the sentiment structures. ",
            "83": "The results suggest that our approach to SSA has clear performance benefits, advancing the state of the art on four out of five commonly used benchmarks. ",
            "84": "Specifically, the most direct opinion-tuple encoding provides the highest performance gains. ",
            "85": "More detailed analysis of the results shows that the benefits stem from better extraction of global structures, rather than local span prediction. ",
            "86": "We will release the source code, models and predictions in the camera-ready version of this paper at https://github.com/censored/for-review.",
            "87": "As briefly mentioned in the main text, previous dependency parsing approaches have relied on a lossy bi-lexical conversion. ",
            "88": "We use this appendix to describe this problem in more detail. ",
            "89": "There is an inherent ambiguity in the encoding of two nested text spans with the same head (defined as either the first or the last token in (Barnes et al., 2021)). ",
            "90": "To be concrete, we can use the running example \"I actually enjoyed the bad acting\", which has two opinions with nested targets \"the bad acting\" and \"acting\". ",
            "91": "As shown in Figure 4, both expressiontarget edges correctly lead to the word \"acting\" but it is impossible to disambiguate the prefix of both targets in the bi-lexical encoding. ",
            "92": "For that, we need a more abstract graph encoding, such as the ones suggested in the main text. ",
            "93": "Table 4 shows that the amount of nesting in the SSA datasets is not negligible. ",
            "94": "This is especially true for NoReC and MPQA, two datasets experiencing significant performance gains from our  : Ambiguous targets when encoding the sentence \"I actually enjoyed the bad acting\" as a headfinal bi-lexical dependency graph (Barnes et al., 2021). ",
            "95": " proposed graph encoding. ",
            "96": "Table 5 further shows the amount of dependency edges lost because of overlap. ",
            "97": "Finally, Table 6 shows the SF 1 score when converting the gold sentiment graphs to bi-lexical dependency graphs and back -an inherent upper bound for any dependency parser.",
            "98": "We found out that the official data published at https://competitions.codalab.org/ competitions/33556 was slightly changed from the data used in previous related work. ",
            "99": "Specifically the MPQA and DSU datasets had removed a number of errors resulting from the annotation and from the conversion scripts used to create the sentiment graph representations. ",
            "100": "We re-run the experiments for the comparable baseline model and show the performance differences in Table 8.",
            "101": "In order to see whether the performance differences for the experiments are significant, we do bootstrap significance testing Berg-Kirkpatrick et al. (2012), combining two variations. ",
            "102": "First, we resample the test sets with replacement from all 5 runs together, b = 1 000 000 times, setting the threshold at p = 0.05. ",
            "103": "Additionally, we test each pair out of the 5 \u00d7 5 combinations for all runs, resampling the test set with replacement b = 100 000 times, setting the threshold again at p = 0.5. ",
            "104": "When one system is significantly better in 15 out of the 25 comparisons, and additionally significantly better in the first joint test, we finally mark it as significantly better.",
            "105": "To make any future comparison of our approach easier, we show the development scores of all reported models in Table 7.",
            "106": "Generally, we follow the training regime described in the original PERIN paper (Samuel and Straka, 2020). ",
            "107": "The trainable parameters are updated with the AdamW optimizer (Loshchilov and Hutter, 2019), and their learning rate is linearly warmed-up for the first 10% of the training to improve stability, and then decayed with a cosine schedule. ",
            "108": "The XLM-R parameters are updated with a lower learning rate and higher weight decay to improve gener-alization; its lower also use an increasingly lower learning rate (Howard and Ruder, 2018). ",
            "109": "Similarly to PERIN, we freeze the embedding parameters for increased efficiency and regularization. ",
            "110": "Following the finding by Zhang et al. (2021), we use small learning rates and fine-tune for a rather long time to increase the training stability. ",
            "111": "Unlike the authors of PERIN, we did not find any benefits from a dynamic scaling of loss weights (Chen et al., 2018), so we simply set all loss weights to constant 1.0. ",
            "112": "We trained our models on a single Nvidia P100 with 16GB RAM, the runtimes are given in Table 7. ",
            "113": "We made five runs from different seeds for each reported value to better estimate the expected error. ",
            "114": "The hyperparameter configurations for all runs follow, please consult the released code for more details and context: https://github.com/ censored/for-review."
        }
    },
    "d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5": {
        "doc1": {
            "0": "paper_summary:",
            "1": "The present paper investigates the Word-in-Context task in a few-shot setting.",
            "2": "Utilising a prompt-based approach, rather than optimising models to predict the expected label (i.e. whether two word senses are different), this paper proposes to predict (embeddings of) words synonymous to those in question and compare the distances of these embeddings.",
            "3": "Models optimised in this way reach the performance of models finetuned on the whole training set and the approach is argued to generalise to some other tasks.",
            "4": "summary_of_strengths:",
            "5": "The method is surprisingly simple, the results are convincing as they significantly improve upon the state-of-the-art on the WiC task in the few-shot setting.",
            "6": "The paper shows that the methodology is applicable to some other few-shot tasks as well.",
            "7": "Claims are substantiated and backed by evidence.",
            "8": "summary_of_weaknesses:",
            "9": "The main weakness of the paper is that it's very terse, partly due to the 4-page limit of a short submission.",
            "10": "This leads to potentially important information being omitted, see detailed comments below.",
            "11": "Some of the mentioned points could be easily alleviated by utilising the additional page that is provided upon acceptance in a venue.",
            "12": "comments,_suggestions_and_typos:",
            "13": "The following are points where i would like to see further clarifications: (NB: I do not have access to the forum/comments of the previous submission so some of my comments might have been addressed earlier.)",
            "14": "- Why are SST-2 and SICK-E chosen as representative tasks to show that the proposed method generalises to other few-shot settings?",
            "15": "Other papers seem to go for the full SuperGLUE suite.",
            "16": "Similarly, for those tasks, why is Autoprompt chosen as the only reference approach to compare against?",
            "17": "Admittedly, the difference in performance to other approaches (at least for the SST-2 task) appears to not differ too much.",
            "18": "- It is great to see that confidence scores were reported for the obtained results, but how exactly are they calculated?",
            "19": "- The high-level description helps to understand the approach intuitively, but a more detailed (e.g. mathematical) formulation, for example in the appendix, would be helpful as well.",
            "20": "Similarly, the figure is supposed to help to understand the problem better, but I find it confusing in two ways: First, the figure is too abstract for me. Maybe having more text labels would help.",
            "21": "Second, depicting sentiment analysis, it does not align well with the main contribution of the paper, improvements on the WiC task.",
            "22": "Maybe reworking the figure to depict the WiC task would help with both problems.",
            "23": "- Qualitative error analysis in the Appendix is great, but the manuscript so far lacks a more detailed analysis of the results.",
            "24": "One could wonder, for the WiC task, are the errors always due to models predicting \"matched\" for \"not matched\" GT? Is this similar to other approaches? For example, a by-class accuracy breakdown could answer some of these questions."
        },
        "doc2": {
            "0": "Exploiting Language Model Prompts Using Similarity Measures: A Case Study on the Word-in-Context Task",
            "1": "Abstract",
            "2": "As a recent development in few-shot learning, prompt-based techniques have demonstrated promising potential in a variety of natural language processing tasks. ",
            "3": "However, despite proving competitive on most tasks in the GLUE and SuperGLUE benchmarks, existing prompt-based techniques fail on the semantic distinction task of the Word-in-Context (WiC) dataset. ",
            "4": "Specifically, none of the existing few-shot approaches (including the incontext learning of GPT-3) can attain a performance that is meaningfully different from the random baseline. ",
            "5": "Trying to fill this gap, we propose a new prompting technique, based on similarity metrics, which boosts few-shot performance to the level of fully supervised methods. ",
            "6": "Our simple adaptation shows that the failure of existing prompt-based techniques in semantic distinction is due to their improper configuration, rather than lack of relevant knowledge in the representations. ",
            "7": "We also show that this approach can be effectively extended to other downstream tasks for which a single prompt is sufficient.",
            "8": "Introduction",
            "9": "Recently, there has been a resurgence of interest in few-shot learning, especially after the introduction of GPT-3 (Brown et al., 2020). ",
            "10": "The current dominant few-shot approach is the so-called promptbased learning which involves a simple reformulation of the target task as a cloze-style (Taylor, 1953) fill-in-the-blank objective. ",
            "11": "The core idea is to extract knowledge by asking the right question from the pre-trained language model (PLM) using a task-specific prompting template which directs the PLM to generate a textual output corresponding to a target class. ",
            "12": "This paradigm has proven its effectiveness in the few-shot setting, even for relatively smaller models, such as BERT (Devlin et al., 2019) and RoBERTA , when combined with ensembling and fine-tuning (Schick and Sch\u00fctze, 2021a). ",
            "13": "From the practical point of view, prompt-based learning is particularly well-suited for massive models, such as GPT-3, since it does not involve parameter tuning.",
            "14": "Prompt-based techniques have shown impressive performance in the few-shot setting, especially when compared to standard fine-tuning on datasets of hundreds of data points (Scao and Rush, 2021). ",
            "15": "However, surprisingly, the Word-in-Context task (Pilehvar and Camacho-Collados, 2019) -one of the tasks in the SuperGLUE benchmark )-is one exception on which these methods fail to stay on par with their fine-tuned counterparts. ",
            "16": "1 While a simple fine-tuned BERT-base model achieves around 69% accuracy on this task , GPT-3, with more than 100 times the number of parameters, performs no better than a random baseline by employing a promptbased approach (Brown et al., 2020). ",
            "17": "The same pattern of failure is also observed in the more recent prompt based attempts (Liu et al., 2021;Schick and Sch\u00fctze, 2021a).",
            "18": "The natural question that arises here is if the failure of few-shot techniques on WiC is due to lack of relevant encoded knowledge in PLMs or the inefficiency of the employed prompt-based methods. ",
            "19": "Two issues could be responsible for the latter case:",
            "20": "(1) improper prompt, or (2) inefficient utilization of PLM's response. ",
            "21": "To address the first issue, there have been proposals to automatically find a suitable prompt template using a search in the discrete token space (Shin et al., 2020) or in the continuous embedding space (Liu et al., 2021). ",
            "22": "However, none of these have shown success on the WiC task.",
            "23": "In this work we investigate the latter issue by introducing a new configuration for prompting. ",
            "24": "Given the comparison-based nature of WiC, we hypothesize that conventional prompting methods fall short since they only utilize a single prompt response. ",
            "25": "Hence, instead of relying on a single response, we make use of the similarity of PLM's response to the combination of a pair of prompts. ",
            "26": "The experimental results on the WiC dataset shows that, with only 16 instances per class, our proposed prompt-based technique can achieve comparable results to the fine-tuned models (with access to full training data of 2700+ instances per class). ",
            "27": "Moreover, we show that with few adjustments, this simple approach can be effectively used for other downstream tasks.",
            "28": "Methodology",
            "29": "Fine-tuning on a specific task can potentially update PLMs on what the task is and how to solve it. ",
            "30": "Assuming that PLMs know how to solve some tasks (to some extent), prompt-based learning focuses on the former, i.e., teaching the model what the task is, without needing to resort to large amounts of data or additional parameters. ",
            "31": "The common approach in prompt-based learning is to reformulate the task as a cloze-style question. ",
            "32": "For instance, to ask about the sentiment of a movie review, one can augment the review with a cloze question like \"this movie was --.\". ",
            "33": "Existing methods often pick a set of one or few word predictions as a representative for each class, utilizing the language model's response in a sub-optimal manner. ",
            "34": "We propose a similarity-based method that not only better exploits the response, but also allows using multiple prompts which paves the way for comparisonbased tasks, such as WiC. In what follows in this section, we describe our similarity-based prompt-ing approach which we will refer to as SP (Similarity Prompting).",
            "35": "As shown in Figure 1, SP consists of three main steps: (1) prompt generation, (2) feature extraction, and (3) prediction. ",
            "36": "Given a task-specific input consisting of one or more text sequences, we first use a template function to generate a prompt-a sequence of tokens containing one [MASK] token-per input sequence. ",
            "37": "For instance, in sentiment analysis, for the movie review \"Just give it a chance.\", a valid template function would generate as output prompt: \"Just give it a chance. ",
            "38": "this movie was --.\". ",
            "39": "The Next step is feature extraction from a PLM. ",
            "40": "This is done by giving the generated prompts to the PLM as input and obtaining its contextualized embedding at the MASK index.",
            "41": "The third step is where SP differs from existing prompt-based approaches. ",
            "42": "Here, we first obtain class-specific centroids by taking the average of the MASK embeddings of our few training examples. ",
            "43": "To classify a new sample at inference time, a simple approach would be to employ a nearest centroid classifier. ",
            "44": "However, this assumes the variance of different classes to be equal in the embedding space. ",
            "45": "To alleviate the problem, we perform a class centroid-based dimension reduction (i.e. by taking the distance to each centroid as a feature), and train a simple linear classifier. ",
            "46": "This linear model is then used at inference time to evaluate SP on test set. ",
            "47": "SP for WiC. The surprising failure of existing prompt-based techniques on the Word-in-Context task (Pilehvar and Camacho-Collados, 2019, WiC), motivated us to focus on filling this gap. ",
            "48": "Given an ambiguous target word in two different contexts, the task in WiC is defined as a simple binary classification problem to identify if the triggered meaning of the target word differs in the two contexts or not.",
            "49": "Previous work has fallen short of designing a single prompt template which make the PLM answer about the target word having the same meaning or not (e.g., with \"yes\" or \"no\"). ",
            "50": "Therefore, we ask PLM about the triggered meaning of the target word, separately for each context, and leave the comparison to similarity measures. ",
            "51": "Having an input sentence and the target word index, we insert \"or --\" after the target word, where \"--\" indicates the MASK token. ",
            "52": "In the first step of SP, we apply this template function to both input sentences which generates a pair of prompts. ",
            "53": "Next the prompts are separately fed to PLM, resulting in a pair of mask embeddings as PLM's response. ",
            "54": "Finally, our classification step reduces to that of directly comparing our pair of embedding vectors using a similarity function, to produce a single similarity score for each instance. ",
            "55": "We then train the same linear model as before on the similarity scores of the training set examples to find the best discriminating threshold.",
            "56": "Similarity Measures. ",
            "57": "We opted for two similarity metrics: cosine similarity and Spearman's rank correlation. ",
            "58": "The latter is a rank-based comparison measure which is insensitive to the absolute values of individual dimensions (rather checks for their relative rankings).",
            "59": "3 Experiments",
            "60": "Comparison Systems",
            "61": "We compare our results on WiC with three other methods, all of which use 32 examples for their training. ",
            "62": "PET (Schick and Sch\u00fctze, 2021b) prefers ALBERT-xxlarge-v2 (Lan et al., 2019) over RoBERTa (with an average gain of 8 points on a subset of SuperGLUE tasks) and fine-tunes it with manually engineered cloze-style prompts. ",
            "63": "P-tuning (Liu et al., 2021) uses the same PLM as PET, but optimizes a continuous prompt instead of tuning PLM parameters. ",
            "64": "GPT3 (Brown et al., 2020) is different in that it employs the so-called in-context learning which involves no parameter tuning.",
            "65": "Tasks",
            "66": "In addition to WiC, we also carried out experiments on two more tasks. ",
            "67": "The goal of this additional experiment is twofold: first, to show the applicability of SP to other settings, including tasks with single input sequence; and second, to evaluate if SP is effective when using prompt templates from other techniques, including those optimized for specific tasks. ",
            "68": "For this experiment, we compare against AutoPrompt (Shin et al., 2020). ",
            "69": "The approach makes use of full training set to optimize discrete prompts for each specific target task. ",
            "70": "Following AutoPrompt, we report results for the following two task:",
            "71": "SST. ",
            "72": "Stanford Sentiment Treebank (Socher et al., 2013) contains fine-grained sentiment labeled parse trees of sentences from movie reviews. ",
            "73": "Systems are evaluated either on a five-way fine-grained or binary classification task. ",
            "74": "We follow the latter (SST-2) in our experiments. ",
            "75": "For this task we used the automatically-generated template of Auto-Prompt, along with the following manual template: T (sent) = sent + \" this movie was --.\", where sent is the input sentence and \"+\" is concatenation operator. ",
            "76": "This is the same manual prompt used in AutoPrompt.",
            "77": "Knowledge (Marelli et al., 2014) is a collection of sentence pairs annotated with their entailment relationship as well as a quantified measurement of their semantic similarity. ",
            "78": "In our experiments, we only use the former annotations (SICK-E) to compare our results with AutoPrompt, which only reports results for its optimized prompt. ",
            "79": "Thus we define our own manual template function as: T (pre, hyp) = pre + \"? Answer: --, \" + hyp, where pre is the premise and hyp is the hypothesis of an input example.",
            "80": "Setup",
            "81": "To train our models, we only used 16 examples per class. ",
            "82": "As for PLM, we opted for RoBERTAlarge to be able to benchmark our results against AutoPrompt's ( Shin et al., 2020).",
            "83": "Results",
            "84": "WiC. Table 1 summarizes the results on WiC with RoBERTa-Large as SP's PLM. ",
            "85": "The performance of SP in the few-shot setting is in the same ballpark as supervised fine-tuning (with nearly 170 times the data, i.e., 2,714 instances per class). ",
            "86": "This observation suggests that PLMs already encode a certain amount of task-related knowledge and the supervised fine-tuning mainly updates their task description (i.e., what the task is, not how to solve it). ",
            "87": "Therefore, using limited examples in the fewshot setting they are able to reach their maximum fine-tuning potential on WiC. We report SP's performance on WiC for other PLMs in the Appendix which shows our method/observation does not depend on a specific PLM. ",
            "88": "We also include some detailed examples of how SP works for WiC in the Appendix.",
            "89": "SICK and SST-2. ",
            "90": "The results on SST-2 and SICK-E are shown in Table 2. ",
            "91": "We compare SP with AutoPrompt which searches for the best template for each task. ",
            "92": "For SST-2, we observe that SP can exploit a manual prompt template significantly better than AutoPrompt, while being competitive using the best template optimized by AutoPrompt (auto-generated). ",
            "93": "This suggests that it is possible to gain significant improvement by simply exploiting a non-optimized manual prompt template.",
            "94": "To compare our results with AutoPrompt on SICK-E task, we report accuracy score of SP for the standard test set (with neutral majority) and its balanced variant.",
            "95": "Similarity Measures Comparison. ",
            "96": "Notably, the Spearman correlation score, which is less commonly used for comparing embeddings, outperforms the cosine similarity on WiC by a large margin while maintaining the same level performance on other tasks. ",
            "97": "This superiority can be explained by the assumption that cosine similarity is more susceptible to variations in the dominant dimensions. ",
            "98": "To evaluate this hypothesis, we performed an experiment in which the most dominant dimension was set to zero for all the embeddings (the dominant dimension is identical across all vectors). ",
            "99": "The results approve the assumption: pruned cosine similarity gains around 10% absolute performance boost on WiC, filling the gap to Spearman correlation. ",
            "100": "However, the gain in the other two tasks is negligible.",
            "101": "The difference in the gain across tasks can be explained by the difference in their underlying nature. ",
            "102": "In WiC, the MASK embeddings can potentially refer to any word, varying from sample to sample. ",
            "103": "However, in SST and SICK the MASK template embedding is more restricted, often representing a closely related word to one of the class centroid embeddings (e.g., in SST the MASK embedding almost always represents a positive or negative adjective). ",
            "104": "This results in a higher spread on the most dominant dimension in the case of WiC. It is known that the most dominant dimensions in PLMs often encode irrelevant information, such as word frequency (Gao et al., 2019), therefore hampering performance for sensitive metrics such as cosine similarity. ",
            "105": "To verify our hypothesis, we ran an experiment using 1200 sample MASK embeddings for each of our three tasks. ",
            "106": "Figure 2 (Appendix) illustrates the distribution of values for the most dominant dimension. ",
            "107": "The ratio of variance is 6.5 times for WiC compared to SST and 27.3 times compared to SICK. ",
            "108": "This further supports the sensitivity of cosine similarity for WiC to the noisy variations along the most dominant dimension compared to the other two tasks.",
            "109": "Conclusion",
            "110": "We proposed an adaptation of prompt-based learning which addresses the common failure of existing techniques on the WiC dataset. ",
            "111": "In this work we showed that similarity based approach to promptbased learning is capable of achieving comparable results to purely fine-tuning based methods on Word-in-Context task, in which previous few-shot attempts have failed. ",
            "112": "We also showed that Spearman's ranking correlation is a more robust choice of similarity measure compared to cosine similarity in this setting. ",
            "113": "We hope that our positive results inspire other prompting strategies to better exploit the encoded knowledge in PLMs. ",
            "114": "As future work, one interesting direction could be to perform further analysis on the behaviour of Spearman's correlation compared to cosine similarity anywhere it is applicable as a similarity measure."
        }
    },
    "f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2": {
        "doc1": {
            "0": "paper_summary:",
            "1": "This work proposes a multi-modal contrastive learning objective to learn text representation based on both visual and text information.",
            "2": "Extensive experiments have been provided to show the improvement of the proposed method on seven semantic textual similarity tasks.",
            "3": "Besides, additional analyses show that the proposed method leads to better semantical alignment of the learned sentence representation.",
            "4": "summary_of_strengths:",
            "5": "- The paper is well-written and clearly presented;",
            "6": "- The idea of using visually grounded corpus to help learn sentence representation is well-motivated and novel.",
            "7": "Extensive experiments (with random seeds) have been provided to show the significance of the proposed methods.",
            "8": "Detailed ablation studies and analyses have also been provided to show the impact of dataset, pre-trained model and sub-tasks;",
            "9": "- The idea can be potentially scaled up to many noisy image-text pairs on the web.",
            "10": "summary_of_weaknesses:",
            "11": "- in Table 1, it seems using Flickr textual corpus only can bring improvement for SimCSE on downstream tasks but not for using COCO, is there a detailed explanation for this? ( also, will adding additional textual data, e.g. more Wikipedia texts bring similar improvement)?",
            "12": "- Tan and Bansal (2020) shows the significant difference between the image-text corpus and wiki corpus (small sentence length, usually less than 77 and so on.)",
            "13": "But the dataset statistics have not been discussed here, wondering will become a problem or not for sentence representation learning.",
            "14": "comments,_suggestions_and_typos:",
            "15": "See above"
        },
        "doc2": {
            "0": "MCSE: Multimodal Contrastive Learning of Sentence Embeddings",
            "1": "Abstract",
            "2": "Learning semantically meaningful sentence embeddings is an open problem in natural language processing. ",
            "3": "In this work, we propose a multimodal contrastive learning approach that exploits both visual and textual information for learning sentence representations. ",
            "4": "Through experiments on a variety of semantic textual similarity tasks, we demonstrate that our approach consistently improves the performance across various datasets and pre-trained encoders. ",
            "5": "In particular, combing a small amount of multimodal data with a large text-only corpus, we improve the state-of-the-art average Spearman's correlation by 1.7%. ",
            "6": "By analyzing the properties of the textual embedding space, we show that our model excels in aligning semantically similar sentences, providing an explanation for its improved performance.",
            "7": "Introduction",
            "8": "Sentence embedding learning, i.e., encoding sentences into fixed-length vectors that faithfully reflect the semantic relatedness among sentences, is a fundamental challenge in natural language processing (NLP). ",
            "9": "Despite the tremendous success of pre-trained language models (PLMs), such as BERT (Devlin et al., 2019), it has been shown that the sentence embeddings of PLMs without finetuning are even inferior to averaging Glove embeddings (Pennington et al., 2014) in terms of semantic similarity measure (Reimers and Gurevych, 2019). ",
            "10": "Hence, recent research (Li et al., 2020;Zhang et al., 2020;Su et al., 2021) focuses on adjusting the original sentence embeddings derived from PLMs in an unsupervised manner. ",
            "11": "In particular, there has been growing interest in adopting contrastive learning objectives to achieve this goal (Carlsson et al., 2020;Kim et al., 2021;Yan et al., 2021).",
            "12": "Although purely text-based models have led to impressive progress, it remains an open question to what extent they capture the deeper notion of sentence meaning beyond the statistical distribution of texts, which lies outside of the text and is grounded in the real-world (Bender and Koller, 2020;Bisk et al., 2020). ",
            "13": "As a central part of the human perceptual experience, vision has also been shown to be effective in grounding language models and improving performance on various NLP tasks (Zhang et al., 2019;Bordes et al., 2019;Zhao and Titov, 2020). ",
            "14": "We hypothesize that using vision as supplementary semantic information can further promote sentence representation learning.",
            "15": "In this work, we propose MCSE, an approach for multimodal contrastive learning of sentence embeddings. ",
            "16": "To exploit both visual and textual information, we adopt the state-of-the-art (SOTA) contrastive sentence embedding framework Sim-CSE (Gao et al., 2021) and extend it with a multimodal contrastive objective. ",
            "17": "In addition to the textual objective in SimCSE that maximizes agreement between positive sentence pairs, the multimodal objective maximizes agreement between sentences and corresponding images in a shared space. ",
            "18": "We conduct extensive experiments on standard Semantic Textual Similarity (STS) benchmarks and show the effectiveness of MCSE across various datasets and pre-trained encoders. ",
            "19": "We find that, using a small amount of multimodal data in addition to text-only corpus yields significant improvements on STS tasks. ",
            "20": "By analyzing the alignment and uniformity properties of the embedding space (Wang and Isola, 2020), we show that MCSE better aligns the semantically similar sentences while maintaining uniformity, providing an explanation for its superior performance.",
            "21": "Related Work",
            "22": "Sentence Representation Learning. ",
            "23": "Existing works for learning sentence embeddings can be categorized into supervised (Conneau et al., 2017;Cer et al., 2018;Reimers and Gurevych, 2019;Wieting et al., 2020) and unsupervised approaches (Li  Carlsson et al., 2020;Su et al., 2021;Kim et al., 2021;Gao et al., 2021;Yan et al., 2021). ",
            "24": "Supervised approaches mostly utilize supervision from annotated natural language inference data or parallel data. ",
            "25": "By adjusting the training objective to STS tasks, unsupervised approaches are able to make use of the intrinsic semantic information embedded in the natural language text corpus, thereby eliminating the need for a costly annotation process.",
            "26": "In particular, contrastive learning objective (Carlsson et al., 2020;Kim et al., 2021;Gao et al., 2021;Yan et al., 2021) regularizes the embedding space by pulling positive (i.e., semantically similar) sentences closer and pushing apart negatives, showcasing great effectiveness in capturing the semantic similarity among sentences. ",
            "27": "Our approach adopts the contrastive learning framework and is built on top of the current SOTA approach (Gao et al., 2021), further pushing the frontier of STS by leveraging multimodal semantic information.",
            "28": "Visually Grounded Representation Learning. ",
            "29": " et al., 2015) by learning a grounded space that preserves the structure of visual and textual spaces.",
            "30": "Recently, Tan and Bansal (2020) and Tang et al. (2021) train large scale language models with multimodal supervision from scratch, with the goal of improving general language understanding. ",
            "31": "Different from the aforementioned works, we focus on learning grounded sentence embeddings by fine-tuning pre-trained models in a contrastive learning framework.",
            "32": "Method",
            "33": "To exploit both visual and textual information, we adopt SimCSE (Gao et al., 2021) as the textual baseline and extend it with a multimodal contrastive learning objective.",
            "34": "Background: Unsupervised SimCSE",
            "35": "Data augmentation plays a critical role in contrastive self-supervised representation learning (Chen et al., 2020). ",
            "36": "The idea of unsupervised SimCSE is to use dropout noise as a simple yet effective data augmentation strategy. ",
            "37": "Given a collection of sentences {x i } m i=1 , we construct a positive pair for each input x i by encoding it twice using different dropout masks:",
            "38": ", where z and z \u2032 denote different dropout masks 1 , f \u03b8 (\u2022) is a pre-trained language encoder such as BERT, and g \u03d5 (\u2022) is a projection head 2 on top of the [CLS] token. ",
            "39": "The training objective is:",
            "40": "where N is the size of mini-batch, \u03c4 is a temperature parameter and sim(h 1 , h 2 ) is the cosine similarity",
            "41": "After training, the [CLS] token outputs of the language encoder are taken as the sentence embeddings.",
            "42": "Multimodal Contrastive Learning",
            "43": "Beyond the textual objective in SimCSE, we introduce a multimodal objective within the contrastive  Gao et al. (2021). ",
            "44": "All other results are from our implementation and models are trained with 5 random seeds. ",
            "45": "We report the means and standard deviations.",
            "46": "learning framework. ",
            "47": "The overview of our MCSE model is shown in Figure 1. ",
            "48": "Given a collection of sentence-image pairs D = {x i , y i } m i=1 , firstly we map sentence x i and image y i into a shared space:",
            "49": "where f v (\u2022) is a pre-trained image encoder such as ResNet (He et al., 2016), which is fixed during training. ",
            "50": "g \u03d5 1 (\u2022) and g \u03d5 2 (\u2022) are distinct projection heads for text and image modality respectively. ",
            "51": "To pull semantically close image-sentence pairs together and push away non-related pairs, we define the multimodal contrastive learning objective as:",
            "52": "where \u03c4 \u2032 is a temperature parameter. ",
            "53": "Let \u03bb denote the trade-off hyperparameter between two objectives, we formulate the final loss as:",
            "54": "Our method further regularizes the sentence representation in such a way that aligns with the image representation in the grounded space.",
            "55": "Experiments",
            "56": "Setup",
            "57": "Dataset We use Flickr30k (Young et al., 2014) and MS-COCO (Lin et al., 2014)   (Agirre et al., 2012(Agirre et al., , 2013(Agirre et al., , 2014(Agirre et al., , 2015(Agirre et al., , 2016, STS Benchmark (Cer et al., 2017) and SICK-Relatedness (Marelli et al., 2014). ",
            "58": "Each of these datasets consists of a collection of sentence pairs and the goal is to predict a similarity score for each pair. ",
            "59": "Following Gao et al. (2021), we report the Spearman's correlation (\u00d7100) between gold annotations and predicted scores in the \"all\" setting, i.e., for each task, we concatenate all the subsets and report the overall Spearman's correlation.",
            "60": "Results",
            "61": "Augmenting text-only corpus with small scale multimodal data yields significant improvements. ",
            "62": "To further investigate the impact of different datasets, we train models solely on multimodal data and report results in Table 2. ",
            "63": "We observe that, without the large text-only corpus, the performances decrease considerably compared to results in Table 1. ",
            "64": "Still, MCSE models consistently surpass SimCSE models (0.9 -3.8 points improvement). ",
            "65": "Moreover, replacing the paired images with shuffled images before training MCSE leads to 0.8 -5.0 points reduction in terms of average Spearman's correlation, further validating the efficacy of visual semantics. ",
            "66": "We also replace the ResNet encoder with CLIP (Radford et al., 2021)   similar features to semantically similar instances (assuming features have been normalized):",
            "67": "And the uniformity loss prefers a uniform distribution in the hypersphere:",
            "68": "Gao et al. ( 2021) empirically showed that sentence embedding models with both low alignment and uniformity achieve better performance in general. ",
            "69": "Similarly, we calculate the two losses on STS-B 4 and plot them in Figure 2. ",
            "70": "It shows that MCSE models achieve lower alignment scores compared to SimCSE while maintaining uniformity. ",
            "71": "This analysis provides further support that by improving the alignment property of the textual embedding space, visually grounding can enhance sentence representation learning.",
            "72": "Conclusion",
            "73": "In this paper, we propose MCSE, a novel approach for sentence embedding learning that applies a multimodal contrastive objective to align sentences and corresponding images in a grounded space. ",
            "74": "Experiments show that MCSE consistently improves the performance on STS tasks. ",
            "75": "We also highlight the superiority of our method by analyzing the alignment and uniformity properties of the embedding space. ",
            "76": "The multimodal objective is generic and can be potentially incorporated into other sentence embedding methods to boost their performance.",
            "77": "Language Encoder Our implementation is based on the Hugging Face Transformers library 5 (Apache-2.0) (Wolf et al., 2020). ",
            "78": "We start from the checkpoints of bert-base-uncased and roberta-base, and fine-tune the pre-trained models using a contrastive objective function. ",
            "79": "We use the 768-dimensional [CLS] token outputs before the MLP pooler layer as sentence embeddings for evaluation.",
            "80": "Image Encoder We use ResNet-50 and extract 2048-dimensional feature vectors at the last layer. ",
            "81": "The image encoder is not fine-tuned 6 . ",
            "82": "Projection Heads We use distinct projection heads for different modalities and objectives. ",
            "83": "All of them are implemented by single-layer MLPs with Tanh activation. ",
            "84": "We map sentence embeddings to a 768dimensional space before calculating the textual objective. ",
            "85": "We map both sentence embeddings and image feature vectors to a 256-dimensional shared space, and normalize them before calculating the multimodal objective.",
            "86": "Parameter Settings We adopt most of the parameter settings suggested by Gao et al. (2021). ",
            "87": "For BERT-based models, we use batch size of 64 and take 3e-5 as the learning rate. ",
            "88": "For RoBERTa-based models, we use batch size of 128 and take 1e-5 as the learning rate. ",
            "89": "Temperature parameters \u03c4 and \u03c4 \u2032 are set to 0.05. ",
            "90": "We use the dev set of STS-B to tune the trade-off parameter \u03bb and ablation studies are shown in Table 3 and Table 4. ",
            "91": "We explore 5 training settings in the paper: {flickr, coco, wiki, wiki+flickr, wiki+coco}. For wiki+flickr and wiki+coco, we sample a mini-batch for each training step from either Wiki1M or the caption dataset based on the ratio of data scales. ",
            "92": "For flickr and coco, we set \u03bb = 0.05 for BERT-based models and \u03bb = 0.01 for RoBERTa-based models, while for other settings we set \u03bb = 0.01. ",
            "93": "We train 6 epochs for flickr and 3 epochs for other settings. ",
            "94": "We evaluate models every 125 training steps on STS-B dev set and keep the best checkpoint for final evaluation.",
            "95": "To delve into the performance gap between MCSE-BERT and SimCSE-BERT, we calculate the Spearman's correlation for different subsets of each year's STS challenge separately. ",
            "96": "The improvements of MCSE over SimCSE are shown in Figure 3. ",
            "97": "In STS12, \"MSRvid\" subset achieves the largest improvement, which is a corpus of video descriptions. ",
            "98": "\"Image\" subsets in STS14 and STS15 also get considerable improvements. ",
            "99": "Meanwhile, the performance of \"answers-students\" subset in STS15 drops extensively, and none of the subsets in STS16 get noticeable improvement by MCSE.",
            "100": "The results indicate that different topics benefit diversely from the visually grounding.",
            "101": "CLIP as Image Encoder We use CLIP (Radford et al., 2021) as an alternative Image encoder. ",
            "102": "The implementation is based on the Sentence Transformer library 7 (Apache-2.0) (Reimers and Gurevych, 2019) and we use the checkpoint clip-ViT-B-32 to extract 512-dimensional feature vectors. ",
            "103": "As shown in Table 6, different image encoders lead to very similar results, thus we use ResNet as the default image encoder for our experiments.",
            "104": "We adopt the same parameter setting as wiki+flickr and wiki+coco, and train models on the combination of Wiki1M, Flickr30k, and MS-COCO. ",
            "105": "As shown in Table 5, MCSE models achieve 1.9 point and 2.6 point improvements when using BERT and RoBERTa, respectively. ",
            "106": "     Cross-Modal Retrieval We take BERT-based models (same seed) and conduct cross-modal retrieval experiments. ",
            "107": "We use the metric Recall@K, which is calculated based on if the ground truth of the query image or caption appears in the top-K retrieved captions or images. ",
            "108": "As results shown in Table 8, MCSE models also achieve a decent level of retrieval performance as a by-product of multimodal contrastive learning.",
            "109": "In this paper, we propose MCSE that exploits both vision and textual information for sentence embedding learning. ",
            "110": "Despite showing a strong performance on STS benchmarks, it has a few limitations as well. ",
            "111": "Firstly, we take caption datasets as the source of multimodal information, while these datasets are collected and curated with nonnegligible human efforts. ",
            "112": "It will have great practical value if we can properly leverage noisy imagesentence pairs, or even get rid of the explicit alignments between images and sentences. ",
            "113": "Secondly, the definition of \"semantic similarity\" is highly task-dependent. ",
            "114": "Besides STS benchmarks, it is worth exploring the performance gap between Sim-CSE and MCSE on other downstream tasks that can also assess the quality of sentence representations, e.g., paraphrase identification."
        }
    }
}