document_id,source_id,target_id,method,annotator_1,annotator_2,source_sentence,target_sentence,agreement,both_link
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,1,0,random,No Link,Link,"This paper proposes, as a form of data augmentation, to replace the one-hot sequences that are typically consumed by text classification models with an interpoloation between this one-hot distribution and a distribution over word-types obtained by running the sequence through BERT.",Text Smoothing: Enhance Various Data Augmentation Methods on Text Classification Tasks,False,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,1,4,mutual,Link,Link,"This paper proposes, as a form of data augmentation, to replace the one-hot sequences that are typically consumed by text classification models with an interpoloation between this one-hot distribution and a distribution over word-types obtained by running the sequence through BERT.","We propose an efficient data augmentation method, termed text smoothing, by converting a sentence from its one-hot representation to a controllable smoothed representation.",True,True
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,1,20,retriever,Link,Link,"This paper proposes, as a form of data augmentation, to replace the one-hot sequences that are typically consumed by text classification models with an interpoloation between this one-hot distribution and a distribution over word-types obtained by running the sequence through BERT.","Therefore, the use of smoothed representation instead of one-hot representation as the input of the model can be seen as an efficient weighted data augmentation method.",True,True
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,1,34,random,Link,No Link,"This paper proposes, as a form of data augmentation, to replace the one-hot sequences that are typically consumed by text classification models with an interpoloation between this one-hot distribution and a distribution over word-types obtained by running the sequence through BERT.",We combine the two stages as text smoothing: obtaining a smooth representation through MLM and interpolating to constrain the representation more controllable.,False,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,1,58,llm,Link,Link,"This paper proposes, as a form of data augmentation, to replace the one-hot sequences that are typically consumed by text classification models with an interpoloation between this one-hot distribution and a distribution over word-types obtained by running the sequence through BERT.","In text smoothing, the one-hot representation and smoothed representation are derived from the same raw input, their lables are identical and the interpolation operation will not change the label.",True,True
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,1,85,mutual,Link,Link,"This paper proposes, as a form of data augmentation, to replace the one-hot sequences that are typically consumed by text classification models with an interpoloation between this one-hot distribution and a distribution over word-types obtained by running the sequence through BERT.","This article proposes text smoothing, an effective data augmentation method, by converting sentences from their one-hot representations to controllable smoothing representations.",True,True
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,2,7,llm,Link,No Link,The authors show that this form of augmentation helps on its own as well as when combined with other standard data augmentation techniques.,"Moreover, text smoothing can be combined with those data augmentation methods to achieve better performance.",False,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,2,9,retriever,No Link,Link,The authors show that this form of augmentation helps on its own as well as when combined with other standard data augmentation techniques.,"Data augmentation is a widely used technique, especially in the low-resource regime.",False,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,2,19,random,No Link,No Link,The authors show that this form of augmentation helps on its own as well as when combined with other standard data augmentation techniques.,"Multiplying the smooth representation by the word embedding matrix can obtain a weighted summation of the word embeddings of the candidate words, termed smoothed embedding, which is more informative and context-rich than the onehot's embedding obtained through lookup operation.",True,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,2,37,llm,Link,No Link,The authors show that this form of augmentation helps on its own as well as when combined with other standard data augmentation techniques.,"Further, we are pleased to find that text smoothing can be combined with other data augmentation methods to improve the tasks further.",False,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,2,38,retriever,Link,No Link,The authors show that this form of augmentation helps on its own as well as when combined with other standard data augmentation techniques.,"To the best of our knowledge, this is the first method to improve a variety of mainstream data augmentation methods.",False,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,2,59,random,No Link,No Link,The authors show that this form of augmentation helps on its own as well as when combined with other standard data augmentation techniques.,So the mixup operation can be simplified to:,True,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,2,81,llm,Link,Link,The authors show that this form of augmentation helps on its own as well as when combined with other standard data augmentation techniques.,"Moreover, we are pleased to find that text smoothing can be well combined with various data augmentation methods, further improving the baseline data augmentation methods.",True,True
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,4,45,random,No Link,No Link,- The paper obtains good results with a straightforward approach.,"Moreover, we propose to constrain smoothed representation more controllable through interpolation for classification tasks.",True,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,4,66,random,No Link,No Link,- The paper obtains good results with a straightforward approach.,"BARTword, BARTspan (Kumar et al., 2020) conditions BART by prepending class labels to all examples of given class.",True,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,5,28,random,No Link,No Link,- The paper is written fairly clearly.,Such a smoothed representation is hardly a good augmented input for the task.,True,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,5,60,random,No Link,No Link,- The paper is written fairly clearly.,"t i is the one-hot representation, MLM(t i ) is the smoothed representation, t i is the interpolated representation and λ is the balance hyperparameter to control interpolation strength.",True,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,9,3,retriever,No Link,Link,"- One thing I wasn't sure I understood is whether the ""smoothed"" inputs are used on their own to train the model, or if they're used in addition to the standard inputs.","Smoothed representation is the probability of candidate tokens obtained from a pre-trained masked language model, which can be seen as a more informative substitution to the one-hot representation.",False,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,9,20,mutual,Link,Link,"- One thing I wasn't sure I understood is whether the ""smoothed"" inputs are used on their own to train the model, or if they're used in addition to the standard inputs.","Therefore, the use of smoothed representation instead of one-hot representation as the input of the model can be seen as an efficient weighted data augmentation method.",True,True
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,9,28,retriever,Link,Link,"- One thing I wasn't sure I understood is whether the ""smoothed"" inputs are used on their own to train the model, or if they're used in addition to the standard inputs.",Such a smoothed representation is hardly a good augmented input for the task.,True,True
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,9,58,llm,Link,Link,"- One thing I wasn't sure I understood is whether the ""smoothed"" inputs are used on their own to train the model, or if they're used in addition to the standard inputs.","In text smoothing, the one-hot representation and smoothed representation are derived from the same raw input, their lables are identical and the interpolation operation will not change the label.",True,True
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,9,61,random,Link,No Link,"- One thing I wasn't sure I understood is whether the ""smoothed"" inputs are used on their own to train the model, or if they're used in addition to the standard inputs.","In the downstream tasks, we use interpolated representation instead of the original one-hot representation as input.",False,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,9,65,random,No Link,No Link,"- One thing I wasn't sure I understood is whether the ""smoothed"" inputs are used on their own to train the model, or if they're used in addition to the standard inputs.","GPT2context (Kumar et al., 2020) provides a prompt to the pre-trained GPT model and keeping generating until the EOS token.",True,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,9,81,llm,No Link,Link,"- One thing I wasn't sure I understood is whether the ""smoothed"" inputs are used on their own to train the model, or if they're used in addition to the standard inputs.","Moreover, we are pleased to find that text smoothing can be well combined with various data augmentation methods, further improving the baseline data augmentation methods.",False,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,11,15,random,No Link,No Link,"This is fine, but should be emphasized more explicitly.",MLM outputs the probability distribution of the vocabulary size of each mask position.,True,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,11,24,retriever,No Link,No Link,"This is fine, but should be emphasized more explicitly.",This is harmful for downstream tasks such as fine-grained sentiment classification.,True,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,11,28,retriever,No Link,No Link,"This is fine, but should be emphasized more explicitly.",Such a smoothed representation is hardly a good augmented input for the task.,True,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,11,37,llm,No Link,No Link,"This is fine, but should be emphasized more explicitly.","Further, we are pleased to find that text smoothing can be combined with other data augmentation methods to improve the tasks further.",True,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,11,45,retriever,No Link,No Link,"This is fine, but should be emphasized more explicitly.","Moreover, we propose to constrain smoothed representation more controllable through interpolation for classification tasks.",True,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,11,52,random,No Link,No Link,"This is fine, but should be emphasized more explicitly.","where − → t i ∈ R seq_len,emb_size is a 2D dense vector in shape of [sequence_len, embedding_size].",True,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,11,81,llm,No Link,No Link,"This is fine, but should be emphasized more explicitly.","Moreover, we are pleased to find that text smoothing can be well combined with various data augmentation methods, further improving the baseline data augmentation methods.",True,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,11,87,llm,No Link,No Link,"This is fine, but should be emphasized more explicitly.","Furthermore, text smoothing can further be combined with various data augmentation methods to obtain better performance.",True,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,12,12,retriever,No Link,Link,"- Since, as the authors note, they are are merely approximating the token-distribution given by BERT (by not using any MASK tokens), it might be interesting to see whether this approximation is in fact hurting the performance or not.","One most commonly used method is to randomly select tokens in a sentence and replace them with semantically similar tokens to synthesize a new sentence (Wei and Zou, 2019;Kobayashi, 2018;. (Kobayashi, 2018) proposes contextual augmentation to predict the probability distribution of replacement tokens by using the LSTM language model and sampling the replacement tokens according to the probability distribution. ) uses BERT's (Devlin et al., 2018) masked language modeling (MLM) task to extend contextual augmentation by considering deep bi-directional context.",False,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,12,13,llm,Link,Link,"- Since, as the authors note, they are are merely approximating the token-distribution given by BERT (by not using any MASK tokens), it might be interesting to see whether this approximation is in fact hurting the performance or not.","(Kumar et al., 2020) further propose to use different types of transformer based pre-trained models for MLM takes masked sentences as input, and typically 15% of the original tokens in the sentences will be replaced by the [MASK] token.",True,True
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,12,21,llm,Link,No Link,"- Since, as the authors note, they are are merely approximating the token-distribution given by BERT (by not using any MASK tokens), it might be interesting to see whether this approximation is in fact hurting the performance or not.","To get the smoothed representation of all the tokens of the entire sentence with only one forward process in MLM, we do not explicitly mask the input.",False,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,12,83,random,No Link,No Link,"- Since, as the authors note, they are are merely approximating the token-distribution given by BERT (by not using any MASK tokens), it might be interesting to see whether this approximation is in fact hurting the performance or not.","To the best of our knowledge, this is the first method to improve a variety of mainstream data augmentation methods.",True,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,12,54,mutual,Link,Link,"- Since, as the authors note, they are are merely approximating the token-distribution given by BERT (by not using any MASK tokens), it might be interesting to see whether this approximation is in fact hurting the performance or not.","where each row in MLM(t i ) is a probability distribution over the token vocabulary, representing the context-compatible token choices in that position of the input text learned by pre-trained BERT.",True,True
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,12,62,retriever,Link,Link,"- Since, as the authors note, they are are merely approximating the token-distribution given by BERT (by not using any MASK tokens), it might be interesting to see whether this approximation is in fact hurting the performance or not.",CBERT  masks some tokens and predicts their contextual substitutions with pretrained BERT.,True,True
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,12,67,random,Link,No Link,"- Since, as the authors note, they are are merely approximating the token-distribution given by BERT (by not using any MASK tokens), it might be interesting to see whether this approximation is in fact hurting the performance or not.",BARTword masks a single word while BARTspan masks a continuous chunk.,False,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,13,71,random,No Link,No Link,"That is, if we obtain token-level distributions by masking each token in the input in turn, and then use the resulting smoothed representations, is this better or worse for augmentation than the approximation the authors propose?",Data statistics of the three datasets are shown in Table 1.,True,False
026fbeb69144bca95d6df70934fd608871855c30486e1a88714ee1343574663f55216ee356d90a1e5110be0a86ec6b9e1f515149abe73c645caaf05409af7731,13,78,random,Link,Link,"That is, if we obtain token-level distributions by masking each token in the input in turn, and then use the resulting smoothed representations, is this better or worse for augmentation than the approximation the authors propose?","As shown in Table2, text smoothing brings the largest improvement to the model on the three datasets compared with other data augmentation methods.",True,True
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,1,0,random,Link,Link,"This well-written paper presents ""DIBIMT"", a manually annotated and curated benchmark to measure semantic biases in word sense disambiguation in machine translation via 4 newly defined metrics.",DIBIMT:,True,True
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,1,6,mutual,Link,Link,"This well-written paper presents ""DIBIMT"", a manually annotated and curated benchmark to measure semantic biases in word sense disambiguation in machine translation via 4 newly defined metrics.","In this paper, we present DIBIMT, the first entirely manually-curated evaluation benchmark which enables an extensive study of semantic biases in Machine Translation in five different language combinations, namely English and one of the following languages:",True,True
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,1,15,mutual,Link,Link,"This well-written paper presents ""DIBIMT"", a manually annotated and curated benchmark to measure semantic biases in word sense disambiguation in machine translation via 4 newly defined metrics.","The DIBIMT benchmark focuses on detecting Word Sense Disambiguation biases in NMT, i.e., biases of certain words towards some of their more frequent meanings.",True,True
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,1,72,mutual,Link,Link,"This well-written paper presents ""DIBIMT"", a manually annotated and curated benchmark to measure semantic biases in word sense disambiguation in machine translation via 4 newly defined metrics.","In this work, we presented DIBIMT, a novel benchmark for measuring and understanding semantic biases in NMT, which goes beyond simple accuracy metrics and provides novel metrics that summarize how biased NMT models are.",True,True
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,1,78,random,No Link,No Link,"This well-written paper presents ""DIBIMT"", a manually annotated and curated benchmark to measure semantic biases in word sense disambiguation in machine translation via 4 newly defined metrics.","Additionally, since stanza has multi-word expansion tokenization for some of the languages in our list, when available, we try to perform matching on both the list of words (alongside the list of tokens) in the translated sentence.",True,False
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,2,6,llm,Link,Link,"The framework has 5 translation directions (English to: Chinese, German, Italian, Russian and Spanish), and also provides statistical and linguistic analysis for 7 (non-)commerical neural MT systems.","In this paper, we present DIBIMT, the first entirely manually-curated evaluation benchmark which enables an extensive study of semantic biases in Machine Translation in five different language combinations, namely English and one of the following languages:",True,True
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,2,7,llm,Link,Link,"The framework has 5 translation directions (English to: Chinese, German, Italian, Russian and Spanish), and also provides statistical and linguistic analysis for 7 (non-)commerical neural MT systems.","Chinese, German, Italian, Russian and Spanish.",True,True
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,2,8,llm,Link,Link,"The framework has 5 translation directions (English to: Chinese, German, Italian, Russian and Spanish), and also provides statistical and linguistic analysis for 7 (non-)commerical neural MT systems.","Furthermore, we test state-of-the-art Machine Translation systems, both commercial and non-commercial, against our new test bed and provide a thorough statistical and linguistic analysis of the results.",True,True
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,2,11,retriever,No Link,No Link,"The framework has 5 translation directions (English to: Chinese, German, Italian, Russian and Spanish), and also provides statistical and linguistic analysis for 7 (non-)commerical neural MT systems.","More recently, Emelin et al. (2020) introduced a statistical method for the identification of disambiguation errors in neural MT (NMT) and demonstrated that models capture data biases within the training corpora, which leads models to produce incorrect translations.",True,False
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,2,21,random,No Link,No Link,"The framework has 5 translation directions (English to: Chinese, German, Italian, Russian and Spanish), and also provides statistical and linguistic analysis for 7 (non-)commerical neural MT systems.","7 The only step left to construct an initial item is to associate a sense σ with the word w i used in the example s. We perform this association in two phases: first, we try to map the definition d related to the example s to a Babel-Net synset by relying on the mappings available in BabelNet 5 between WordNet and Wiktionary, discarding examples for which this association could not be found; second, we manually validate and correct these successful associations to ensure high quality of our initial items.",True,False
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,2,30,retriever,No Link,No Link,"The framework has 5 translation directions (English to: Chinese, German, Italian, Russian and Spanish), and also provides statistical and linguistic analysis for 7 (non-)commerical neural MT systems.","Given the expertise required to carry out this task, we rely on the work of three highly qualified translators: one for Italian, German and Russian, one for Spanish and one for Chinese.",True,False
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,2,46,retriever,No Link,No Link,"The framework has 5 translation directions (English to: Chinese, German, Italian, Russian and Spanish), and also provides statistical and linguistic analysis for 7 (non-)commerical neural MT systems.","• DeepL Translator 11 , a state-of-the-art com-      In Figure 3(a), we plot the number and percentage of errors made on average by the models, grouping items by µ λ X P (σ X ), where X is a non-MISS analyzed item.",True,False
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,2,61,random,No Link,No Link,"The framework has 5 translation directions (English to: Chinese, German, Italian, Russian and Spanish), and also provides statistical and linguistic analysis for 7 (non-)commerical neural MT systems.",16 We skip item X if either X M L1 or X M L2 is a MISS.,True,False
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,4,6,random,Link,No Link,-- thorough experimentation,"In this paper, we present DIBIMT, the first entirely manually-curated evaluation benchmark which enables an extensive study of semantic biases in Machine Translation in five different language combinations, namely English and one of the following languages:",False,False
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,4,8,llm,Link,Link,-- thorough experimentation,"Furthermore, we test state-of-the-art Machine Translation systems, both commercial and non-commercial, against our new test bed and provide a thorough statistical and linguistic analysis of the results.",True,True
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,4,13,llm,Link,Link,-- thorough experimentation,"Based on the findings and open research questions raised in the aforementioned works, the present paper aims at investigating not only the presence, but also, most importantly, the nature and properties of semantic biases in MT in multiple language combinations, via a novel entirely manually-curated benchmark called DIBIMT and a thorough performance analysis.",True,True
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,4,37,random,No Link,No Link,-- thorough experimentation,We instruct annotators to update the set of good (G L ) and bad (B L ) translated lexicalizations of w i ∈ s such that it does reflect what a human translator would use in the context of that sentence.,True,False
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,4,43,retriever,No Link,No Link,-- thorough experimentation,10 A more detailed description of the analysis procedure is provided in the Appendix.,True,False
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,4,45,llm,Link,Link,-- thorough experimentation,"We test a wide range of models, both commercial and non-commercial, and report their performances on DIBIMT's evaluation metrics:",True,True
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,4,54,retriever,No Link,No Link,-- thorough experimentation,"This experiment mirrors SFII, but groups items by their lemma's polysemy degree δ EN (λ X P ) instead of µ.  5.",True,False
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,4,84,retriever,No Link,No Link,-- thorough experimentation,Everything is detailed in Section 4 of the paper.,True,False
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,5,1,llm,Link,Link,-- interesting benchmark that will be both tough and useful for MT system comparison in terms of their ability to perform word sense disambiguation,A Novel Benchmark for Measuring Word Sense Disambiguation Biases in Machine Translation,True,True
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,5,10,retriever,No Link,Link,-- interesting benchmark that will be both tough and useful for MT system comparison in terms of their ability to perform word sense disambiguation,"The polysemous nature of words poses a longstanding challenge in a wide range of Natural Language Processing (NLP) tasks such as Word Sense Disambiguation (Navigli, 2009)  Further research works investigated the disambiguation capabilities of MT systems by exploring their internal representations (Marvin and Koehn, 2018;Michel et al., 2019) or improving them via context-aware word embeddings (Liu et al., 2017).",False,False
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,5,12,random,No Link,No Link,-- interesting benchmark that will be both tough and useful for MT system comparison in terms of their ability to perform word sense disambiguation,"Although the authors expect their approach to be transferable to other language combinations, they only focus on German → English.",True,False
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,5,13,mutual,Link,Link,-- interesting benchmark that will be both tough and useful for MT system comparison in terms of their ability to perform word sense disambiguation,"Based on the findings and open research questions raised in the aforementioned works, the present paper aims at investigating not only the presence, but also, most importantly, the nature and properties of semantic biases in MT in multiple language combinations, via a novel entirely manually-curated benchmark called DIBIMT and a thorough performance analysis.",True,True
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,5,15,mutual,Link,Link,-- interesting benchmark that will be both tough and useful for MT system comparison in terms of their ability to perform word sense disambiguation,"The DIBIMT benchmark focuses on detecting Word Sense Disambiguation biases in NMT, i.e., biases of certain words towards some of their more frequent meanings.",True,True
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,5,50,random,No Link,No Link,-- interesting benchmark that will be both tough and useful for MT system comparison in terms of their ability to perform word sense disambiguation,Values are reported in Table 5: Frequency Analysis: MFS represents the average percentage of time the model mistakenly translated the target word to a lexicalization belonging to the Most Frequent Sense associated with λ P .,True,False
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,6,10,retriever,No Link,No Link,-- especially liked the inclusion of 5 (diverse) languages and the comparison against a dedicated word sense disambiguation system,"The polysemous nature of words poses a longstanding challenge in a wide range of Natural Language Processing (NLP) tasks such as Word Sense Disambiguation (Navigli, 2009)  Further research works investigated the disambiguation capabilities of MT systems by exploring their internal representations (Marvin and Koehn, 2018;Michel et al., 2019) or improving them via context-aware word embeddings (Liu et al., 2017).",True,False
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,6,85,random,No Link,No Link,-- especially liked the inclusion of 5 (diverse) languages and the comparison against a dedicated word sense disambiguation system,• DeepL,True,False
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,6,75,llm,Link,No Link,-- especially liked the inclusion of 5 (diverse) languages and the comparison against a dedicated word sense disambiguation system,"In the future, we plan to improve DIBIMT's handling of MISS, widen language coverage and expand the annotations.",False,False
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,6,82,random,Link,No Link,-- especially liked the inclusion of 5 (diverse) languages and the comparison against a dedicated word sense disambiguation system,We include model-specific analyses with perlanguage breakdown of the scores achieved on our benchmark.,False,False
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,8,14,random,Link,No Link,"-- none really, maybe only the sheer amount of material that is almost too much for a conference paper format (given the long appendix, as well)",Building DIBIMT,False,False
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,8,43,llm,Link,Link,"-- none really, maybe only the sheer amount of material that is almost too much for a conference paper format (given the long appendix, as well)",10 A more detailed description of the analysis procedure is provided in the Appendix.,True,True
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,8,50,random,Link,No Link,"-- none really, maybe only the sheer amount of material that is almost too much for a conference paper format (given the long appendix, as well)",Values are reported in Table 5: Frequency Analysis: MFS represents the average percentage of time the model mistakenly translated the target word to a lexicalization belonging to the Most Frequent Sense associated with λ P .,False,False
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,8,84,retriever,Link,No Link,"-- none really, maybe only the sheer amount of material that is almost too much for a conference paper format (given the long appendix, as well)",Everything is detailed in Section 4 of the paper.,False,False
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,9,0,random,Link,No Link,-- ...and maybe that only few annotators have been working on the task without the ability to check for agreements or majority votes,DIBIMT:,False,False
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,9,26,random,No Link,No Link,-- ...and maybe that only few annotators have been working on the task without the ability to check for agreements or majority votes,"If the same holds true for all languages, the synset passes the test and thus the sentence is retained.",True,False
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,9,27,llm,Link,Link,-- ...and maybe that only few annotators have been working on the task without the ability to check for agreements or majority votes,Annotating the Dataset,True,True
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,9,39,retriever,Link,Link,-- ...and maybe that only few annotators have been working on the task without the ability to check for agreements or majority votes,We also instruct annotators to mark sentences as,True,True
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,11,86,random,No Link,No Link,-- some of the formal notations introduced in chapter 3 would probably be easier to understand if put in words,• OPUS,True,False
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,11,87,random,No Link,No Link,-- some of the formal notations introduced in chapter 3 would probably be easier to understand if put in words,• M2M100,True,False
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,12,35,random,Link,Link,-- but I realize that that would make the paper yet longer again,we produce an automatically populated version of our annotated items.,True,True
2451bf980a82f881856251ecbbc2a588b7d2e748af28bcd71cf24c1fd0d54ecc0a2d3d8c7f92afe1ed068e709aad5e98a94f10cb52153f5b9e24c44ef715fb01,12,52,random,No Link,No Link,-- but I realize that that would make the paper yet longer again,Lower is better.,True,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,1,0,random,Link,Link,"This paper propose a two-stage training for the complete many-to-many Multilingual NMT, where the model is first pretrained on the complete multilingual dataset, then finetuned only with the same target language data.",Building Multilingual Machine Translation Systems That Serve Arbitrary XY Translations,True,True
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,1,5,retriever,Link,Link,"This paper propose a two-stage training for the complete many-to-many Multilingual NMT, where the model is first pretrained on the complete multilingual dataset, then finetuned only with the same target language data.","To address the issue, this paper discusses how to practically build MNMT systems that serve arbitrary XY translation directions while leveraging multilinguality with the two-stage training strategy of pretraining and finetuning.",True,True
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,1,15,mutual,Link,Link,"This paper propose a two-stage training for the complete many-to-many Multilingual NMT, where the model is first pretrained on the complete multilingual dataset, then finetuned only with the same target language data.","In this paper, we propose a two-stage training for complete MNMT systems that serve arbitrary XY translations by 1) pretraining a complete multilingual many-to-many model and 2) finetuning the model to effectively transferring knowledge from the complete multilingual training.",True,True
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,1,54,retriever,Link,No Link,"This paper propose a two-stage training for the complete many-to-many Multilingual NMT, where the model is first pretrained on the complete multilingual dataset, then finetuned only with the same target language data.","We validate our proposed approach in an extremely large-scale setting, while briefly touching the following three topics of 1) multi-way multilingual data collection, 2) English-centric vs. multi-centric pretraining for better XY, and 3) a lighter MNMT model that addresses the trade-off issue between performance and latency.",False,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,1,81,random,No Link,Link,"This paper propose a two-stage training for the complete many-to-many Multilingual NMT, where the model is first pretrained on the complete multilingual dataset, then finetuned only with the same target language data.",Conclusion,False,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,1,82,llm,Link,Link,"This paper propose a two-stage training for the complete many-to-many Multilingual NMT, where the model is first pretrained on the complete multilingual dataset, then finetuned only with the same target language data.",This paper proposes a simple but effective twostage training strategy for MNMT systems that serve arbitrary XY translations.,True,True
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,1,83,llm,Link,Link,"This paper propose a two-stage training for the complete many-to-many Multilingual NMT, where the model is first pretrained on the complete multilingual dataset, then finetuned only with the same target language data.","To support translations across languages, we first pretrain a complete multilingual many-to-many model, then transfer the representations via finetuning the model in a many-to-one multilingual fashion.",True,True
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,2,6,llm,Link,Link,"With the proposed training schedule, author reports a significant improvement over both bilingual and pivot-based baselines.","Experimenting in the WMT'21 multilingual translation task, we demonstrate that our systems outperform the conventional baselines of direct bilingual models and pivot translation models for most directions, averagely giving +6.0 and +4.1 BLEU, without the need for architecture change or extra data collection.",True,True
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,2,22,mutual,Link,Link,"With the proposed training schedule, author reports a significant improvement over both bilingual and pivot-based baselines.","We assume that  (top) and Task 2 (bottom), with the respective improvement of (Base, BaseFT, Big, BigFT) = (+3.6, +4.7, +5.0, +6.0) and (+2.0, +2.9, +9.2 +3.2, +4.1) against the bilingual baseline (""Bi"") and the pivot translation baslines (""Pivot""). ""Base / Big"" denote our two settings, with the suffix ""FT"" for finetuned systems.",True,True
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,2,44,retriever,Link,Link,"With the proposed training schedule, author reports a significant improvement over both bilingual and pivot-based baselines.","Baselines For system comparison, we build two different baselines: 1) a direct bilingual system and 2) a pivot translation system via English (only applicable for non-English XY evaluation).",True,True
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,2,56,random,No Link,No Link,"With the proposed training schedule, author reports a significant improvement over both bilingual and pivot-based baselines.","From these English-centric data, we extract a multi-way multilingual XY data, by aligning EX and EY data via pivoting English.",True,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,2,83,random,No Link,No Link,"With the proposed training schedule, author reports a significant improvement over both bilingual and pivot-based baselines.","To support translations across languages, we first pretrain a complete multilingual many-to-many model, then transfer the representations via finetuning the model in a many-to-one multilingual fashion.",True,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,2,84,mutual,Link,Link,"With the proposed training schedule, author reports a significant improvement over both bilingual and pivot-based baselines.","In the WMT'21 translation task, we experimentally showed that the proposed approach substantially improve translation accuracy for most XY directions against the strong conventional baselines of bilingual systems and pivot translation systems.",True,True
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,4,15,mutual,Link,No Link,1. The proposed two-stage training is novel and highly effective.,"In this paper, we propose a two-stage training for complete MNMT systems that serve arbitrary XY translations by 1) pretraining a complete multilingual many-to-many model and 2) finetuning the model to effectively transferring knowledge from the complete multilingual training.",False,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,4,20,retriever,Link,Link,1. The proposed two-stage training is novel and highly effective.,Two-Stage Training for MNMT Models,True,True
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,4,32,random,No Link,No Link,1. The proposed two-stage training is novel and highly effective.,"The data size per direction varies in a range 1 https://www.statmt.org/wmt21/largescale-multilingual-translation-task.html 2 The data provided among English (en), 5 Central/East European languages of {Croatian (hr), Hungarian (hu), Estonian (et), Serbian (sr), Macedonian (mk)} for the task 1, and 5 Southeast Asian languages of {Javanese (jv), Indonesian (id), Malay (ms), Tagalog (tl), Tamil (ta)} for the task 2.",True,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,4,36,random,No Link,No Link,1. The proposed two-stage training is novel and highly effective.,"We tokenize the data with the SentencePiece (Kudo and Richardson, 2018) and build a shared vocabulary with 64k tokens.",True,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,4,82,llm,Link,Link,1. The proposed two-stage training is novel and highly effective.,This paper proposes a simple but effective twostage training strategy for MNMT systems that serve arbitrary XY translations.,True,True
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,5,7,retriever,Link,Link,"Demonstrated on two large-scale datasets, this approach is able to achieve significant improvement over the baselines.","Moreover, we also examine our proposed approach in an extremely large-scale data setting to accommodate practical deployment scenarios.",True,True
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,5,28,llm,Link,Link,"Demonstrated on two large-scale datasets, this approach is able to achieve significant improvement over the baselines.","We experiment with our proposed approach in two different settings using 1) WMT'21 large-scale multilingual translation data with 487M training data, and 2) our in-house production-scale dataset with 3.7B training data.",True,True
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,5,37,random,No Link,No Link,"Demonstrated on two large-scale datasets, this approach is able to achieve significant improvement over the baselines.","We train Transformer models Base and Big (Vaswani et al., 2017) in a complete multilingual many-to-many fashion, respectively.",True,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,5,48,random,No Link,No Link,"Demonstrated on two large-scale datasets, this approach is able to achieve significant improvement over the baselines.","All results on the test sets are displayed in Figure 1, where we report the case-sensitive sacreBLEU score (Post, 2018) for translation accuracy.",True,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,5,52,retriever,Link,Link,"Demonstrated on two large-scale datasets, this approach is able to achieve significant improvement over the baselines.","In the following section, we validate our proposed approach in an extremely large-scale data setting and also discuss how we can build lighter MNMT models without the performance loss.",True,True
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,5,72,llm,Link,Link,"Demonstrated on two large-scale datasets, this approach is able to achieve significant improvement over the baselines.","Over- all, the finetuned multi-centric models achieved the best, largely outperforming the English pivotbased baselines by +2.6 and +2.8 points.",True,True
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,5,75,llm,Link,Link,"Demonstrated on two large-scale datasets, this approach is able to achieve significant improvement over the baselines.","For the xx-{es,it,pl} directions 4 , each finetuned system gains similar accuracy improvement, significantly outperforming the conventional baselines.",True,True
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,5,85,retriever,No Link,No Link,"Demonstrated on two large-scale datasets, this approach is able to achieve significant improvement over the baselines.","We also examined the proposed approach in the extremely large-scale setting, while addressing the practical questions such as multi-way parallel data collection, the usefulness of multilinguality during the pretraining and finetuning, and how to save the decoding cost, achieving the better XY quality.",True,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,6,5,llm,Link,No Link,2. Their idea is simple and clearly conveyed.,"To address the issue, this paper discusses how to practically build MNMT systems that serve arbitrary XY translation directions while leveraging multilinguality with the two-stage training strategy of pretraining and finetuning.",False,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,6,15,llm,No Link,No Link,2. Their idea is simple and clearly conveyed.,"In this paper, we propose a two-stage training for complete MNMT systems that serve arbitrary XY translations by 1) pretraining a complete multilingual many-to-many model and 2) finetuning the model to effectively transferring knowledge from the complete multilingual training.",True,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,6,53,random,No Link,No Link,2. Their idea is simple and clearly conveyed.,In-house Extremely Large-Scale Setting,True,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,6,57,random,No Link,No Link,2. Their idea is simple and clearly conveyed.,"Specifically, we extracted {de, fr, es, it, pl}-centric data and concatenate them to the existing direct XY data, providing 24M-192M per direction.",True,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,6,58,retriever,No Link,No Link,2. Their idea is simple and clearly conveyed.,"Similarly as in Section 2.1, we build a shared SentencePiece vocabulary with 128k tokens to address the largescale setting.",True,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,6,79,retriever,No Link,No Link,2. Their idea is simple and clearly conveyed.,"For all xx-{de,fr,es,it,pl} directions, our proposed models show the best performance in both metrics.",True,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,6,82,mutual,Link,Link,2. Their idea is simple and clearly conveyed.,This paper proposes a simple but effective twostage training strategy for MNMT systems that serve arbitrary XY translations.,True,True
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,7,5,random,No Link,No Link,"With the experimental success, it is well-suited for a short paper.","To address the issue, this paper discusses how to practically build MNMT systems that serve arbitrary XY translation directions while leveraging multilinguality with the two-stage training strategy of pretraining and finetuning.",True,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,7,40,random,No Link,No Link,"With the experimental success, it is well-suited for a short paper.","After pretraining, the models are finetuned on a subset of XL training data.",True,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,9,10,random,No Link,No Link,"1. The ""baseline"" may not be appropriate.","Because the multilingual capability hugely reduces the deployment cost at training and inference, the MNMT has actively been employed as a machine translation system backbone in recent years (Johnson et al., 2017;Hassan et al., 2018).",True,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,9,44,mutual,Link,Link,"1. The ""baseline"" may not be appropriate.","Baselines For system comparison, we build two different baselines: 1) a direct bilingual system and 2) a pivot translation system via English (only applicable for non-English XY evaluation).",True,True
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,9,49,retriever,Link,Link,"1. The ""baseline"" may not be appropriate.","Our best systems (""BigFT"") are significantly better by ≥ +0.5 sacreBLEU for 83% and 88% directions against the bilingual baselines and the pivot translation baselines, respectively.",True,True
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,9,67,llm,Link,No Link,"1. The ""baseline"" may not be appropriate.","To examine the light MNMT model architecture, we train the Transformer Base architecture modified with 9-3 layers (E9D3) in a bilingual setting and compare it with a standard Transformer Base model, with 6-6 layers (E6D6), as a baseline.",False,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,9,84,random,Link,Link,"1. The ""baseline"" may not be appropriate.","In the WMT'21 translation task, we experimentally showed that the proposed approach substantially improve translation accuracy for most XY directions against the strong conventional baselines of bilingual systems and pivot translation systems.",True,True
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,10,14,retriever,Link,No Link,Their approach should be compared with the baseline setup where N many-to-one models are trained with only their corresponding target languages.,"The model suffers from handling many diverse languages, in contrast to successful manyto-one translation (Johnson et al., 2017).",False,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,10,15,llm,No Link,No Link,Their approach should be compared with the baseline setup where N many-to-one models are trained with only their corresponding target languages.,"In this paper, we propose a two-stage training for complete MNMT systems that serve arbitrary XY translations by 1) pretraining a complete multilingual many-to-many model and 2) finetuning the model to effectively transferring knowledge from the complete multilingual training.",True,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,10,17,llm,Link,No Link,Their approach should be compared with the baseline setup where N many-to-one models are trained with only their corresponding target languages.,"We transfer the representations to a specifically targeted task via manyto-one multilingual finetuning, and eventually build multiple many-to-one MNMT models that cover all XY directions.",False,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,10,38,retriever,No Link,No Link,Their approach should be compared with the baseline setup where N many-to-one models are trained with only their corresponding target languages.,"The model parameters are optimized by using RAdam (Liu et al., 2020) with an initial learning rate of 0.025, and warm-up steps of 10k and 30k for the Base and Big model training, respectively.",True,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,10,41,random,No Link,No Link,Their approach should be compared with the baseline setup where N many-to-one models are trained with only their corresponding target languages.,"We tune the model parameters gently on 8 V100 GPUs with the same mini-batch size, graduation accumulations, and the same optimizer with different learning rate scheduling of (init_lr, warm-up steps)=({1e-4, 1e-5, 1e-6}, 8k).",True,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,10,42,random,No Link,No Link,Their approach should be compared with the baseline setup where N many-to-one models are trained with only their corresponding target languages.,The best checkpoints are selected based on development loss.,True,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,10,44,mutual,Link,Link,Their approach should be compared with the baseline setup where N many-to-one models are trained with only their corresponding target languages.,"Baselines For system comparison, we build two different baselines: 1) a direct bilingual system and 2) a pivot translation system via English (only applicable for non-English XY evaluation).",True,True
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,11,14,random,Link,No Link,"The approach in the paper ends up with N models, thus it's unfair to compare against the Many-to-Many baseline (one model) nor the bilingual baseline (one dataset).","The model suffers from handling many diverse languages, in contrast to successful manyto-one translation (Johnson et al., 2017).",False,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,11,43,random,No Link,No Link,"The approach in the paper ends up with N models, thus it's unfair to compare against the Many-to-Many baseline (one model) nor the bilingual baseline (one dataset).","The translations are obtained by a beam search decoding with a beam size of 4, unless otherwise stated.",True,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,11,44,mutual,Link,No Link,"The approach in the paper ends up with N models, thus it's unfair to compare against the Many-to-Many baseline (one model) nor the bilingual baseline (one dataset).","Baselines For system comparison, we build two different baselines: 1) a direct bilingual system and 2) a pivot translation system via English (only applicable for non-English XY evaluation).",False,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,12,14,retriever,Link,No Link,"2. Since their approach ends up solving a Many-to-Many translation problem with N Many-to-One models, it greatly drops the deployment efficiency provided by Multilingual NMT, and scales linearly to the number of languages.","The model suffers from handling many diverse languages, in contrast to successful manyto-one translation (Johnson et al., 2017).",False,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,12,17,mutual,Link,Link,"2. Since their approach ends up solving a Many-to-Many translation problem with N Many-to-One models, it greatly drops the deployment efficiency provided by Multilingual NMT, and scales linearly to the number of languages.","We transfer the representations to a specifically targeted task via manyto-one multilingual finetuning, and eventually build multiple many-to-one MNMT models that cover all XY directions.",True,True
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,12,27,llm,Link,Link,"2. Since their approach ends up solving a Many-to-Many translation problem with N Many-to-One models, it greatly drops the deployment efficiency provided by Multilingual NMT, and scales linearly to the number of languages.","As a result, we obtain |L| multilingual many-to-one systems to serve all XY translation directions.",True,True
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,12,60,random,No Link,No Link,"2. Since their approach ends up solving a Many-to-Many translation problem with N Many-to-One models, it greatly drops the deployment efficiency provided by Multilingual NMT, and scales linearly to the number of languages.","Considering the dominant text data is usually English, e.g., 70% tasks are English-centric in the WMT'21 News translation task, the model supervised on English-centric corpora might learn representations enough to transfer for XY translations.",True,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,12,74,random,No Link,No Link,"2. Since their approach ends up solving a Many-to-Many translation problem with N Many-to-One models, it greatly drops the deployment efficiency provided by Multilingual NMT, and scales linearly to the number of languages.","This suggests that, by pretraining a model on more multi-centric data, the model learns better multilinguality to transfer.",True,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,14,34,random,No Link,No Link,4. paper writing needs improvement (see below),"To balance the data distribution across languages (Kudugunta et al., 2019), we upsample the low-resource languages with tempera-ture=5.",True,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,14,76,random,No Link,No Link,4. paper writing needs improvement (see below),"Figure 2 shows the effectiveness of our light NMT model architecture for 5 EX directions, reporting the translation performance in sacreBLEU scores and the latency measured on CPUs.",True,False
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,17,11,llm,Link,Link,"I suggest changing all XY, EX, XE notations to X-Y, En-X, X-En, since they could be confusing.","Most MNMT systems are trained with multiple English-centric data for both directions (e.g. for English → {French, Chinese} (EX) and {French, Chi-nese} → English (XE)).",True,True
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,17,56,random,Link,Link,"I suggest changing all XY, EX, XE notations to X-Y, En-X, X-En, since they could be confusing.","From these English-centric data, we extract a multi-way multilingual XY data, by aligning EX and EY data via pivoting English.",True,True
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,17,62,retriever,Link,Link,"I suggest changing all XY, EX, XE notations to X-Y, En-X, X-En, since they could be confusing.","After pretraining, we apply the multilingual many-to-one finetuning with a subset of the training data and evaluate each system for the fully supervised XY directions, i.e. xx-{en,de,fr}, and the partially supervised XY directions, i.e. xx-{es,it,pl}. We followed the same training and finetuning settings as described in Section 2.1, unless otherwise stated.",True,True
6666ff9aa9f0e507dde28de61fe9d051463456cab0a8a74228c85f766696594713932397fdc416011b2a091d2b8af6a5dfed284d270849bbb449d997fbfa7f5a,17,82,random,No Link,Link,"I suggest changing all XY, EX, XE notations to X-Y, En-X, X-En, since they could be confusing.",This paper proposes a simple but effective twostage training strategy for MNMT systems that serve arbitrary XY translations.,False,False
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,1,6,mutual,Link,Link,"This paper proposes NoisyTune, which is a matrix-wise perturbing method by adding different uniform noises according to the standard deviations of different parameter matrices.","More specifically, we propose a matrix-wise perturbing method by adding different uniform noises according to the standard deviations of different parameter matrices, which can consider the varied characteristics of different types of parameters in PLMs.",True,True
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,1,24,random,Link,Link,"This paper proposes NoisyTune, which is a matrix-wise perturbing method by adding different uniform noises according to the standard deviations of different parameter matrices.","In this section, we introduce our proposed Noisy-Tune approach that adds noise to perturb PLMs for more effective finetuning.",True,True
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,1,34,mutual,Link,Link,"This paper proposes NoisyTune, which is a matrix-wise perturbing method by adding different uniform noises according to the standard deviations of different parameter matrices.","To solve this challenge, we propose a matrix-wise perturbing method that adds noise with different intensities according to the variance of parameter matrices.",True,True
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,1,43,random,No Link,No Link,"This paper proposes NoisyTune, which is a matrix-wise perturbing method by adding different uniform noises according to the standard deviations of different parameter matrices.",We conduct extensive experiments on two widely used benchmarks for PLM evaluation.,True,False
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,1,93,mutual,Link,Link,"This paper proposes NoisyTune, which is a matrix-wise perturbing method by adding different uniform noises according to the standard deviations of different parameter matrices.","In NoisyTune, we propose a matrix-wise perturbing method that adds noise with different intensities according to the variance of different parameter matrices in PLMs, which can consider the varied characteristics of different types of parameters.",True,True
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,3,5,llm,Link,Link,A simple method,"In this paper, we propose a very simple yet effective method named NoisyTune which can help better finetune PLMs in downstream tasks by adding some noise to the parameters of PLMs before finetuning.",True,True
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,3,18,mutual,Link,Link,A simple method,"In this paper, we propose a very simple yet effective method named NoisyTune, which can help better finetune PLMs for downstream tasks.",True,True
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,3,25,random,No Link,No Link,A simple method,"Since the parameters of PLMs are well tuned in the pretraining tasks and may overfit self-supervision signals, it may be difficult for them to adapt to downstream tasks especially when labeled data in downstream tasks are rather limited.",True,False
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,3,41,random,No Link,No Link,A simple method,3 Experiments,True,False
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,3,66,retriever,No Link,No Link,A simple method,"We compare five methods, including (1) basic method without noise;",True,False
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,3,92,mutual,Link,Link,A simple method,"In this paper, we propose a very simple but effective method named NoisyTune, which adds a little noise to PLMs before finetuning for better transferability from pretraining tasks to downstream tasks.",True,True
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,5,3,llm,No Link,No Link,There are some questions:,"However, PLMs may have risks in overfitting pretraining signals, and there are some gaps between downstream tasks and the pretraining tasks.",True,False
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,5,31,random,No Link,No Link,There are some questions:,"However, different parameter matrices in the PLM have very different characteristics.",True,False
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,5,46,retriever,No Link,No Link,There are some questions:,"It covers 40 languages and contains four groups of tasks, including sentence classification, structured prediction, sentence retrieval and question answering.",True,False
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,5,58,random,No Link,No Link,There are some questions:,"The results on the two benchmarks are shown in Tables 1 and 2, respectively.",True,False
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,6,11,random,No Link,No Link,"Q1. Even simple, I doubt its effectiveness.","In addition, crosslingual pretrained language models like XLM (Conneau and Lample, 2019) and Unicoder (Huang et al., 2019) can even generate cross-lingually transferable language representations to universally handle downstream tasks in different languages.",True,False
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,6,94,random,Link,No Link,"Q1. Even simple, I doubt its effectiveness.",Extensive experiments on the monolingual GLUE benchmark and the multilingual XTREME benchmark demonstrate the NoisyTune can consistently improve the performance of different PLMs in various downstream tasks.,False,False
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,7,7,retriever,Link,Link,"The experimental results of both GLUE and XTREAMRE show that the improvements are range from 0.2 to 0.8 (Avg score), which seems not significant.",Extensive experiments on the GLUE English benchmark and the XTREME multilingual benchmark show that NoisyTune can consistently improve the performance of different PLMs in many downstream tasks.,True,True
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,7,29,random,No Link,No Link,"The experimental results of both GLUE and XTREAMRE show that the improvements are range from 0.2 to 0.8 (Avg score), which seems not significant.",where N is the number of parameter types.,True,False
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,7,48,random,No Link,Link,"The experimental results of both GLUE and XTREAMRE show that the improvements are range from 0.2 to 0.8 (Avg score), which seems not significant.","Since the test labels of GLUE are not released, following (Bao et al., 2020) we report results on the dev set of GLUE.",False,False
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,7,60,llm,Link,No Link,"The experimental results of both GLUE and XTREAMRE show that the improvements are range from 0.2 to 0.8 (Avg score), which seems not significant.","From the results, we can see that NoisyTune can consistently improve the performance of different PLMs on different tasks.",False,False
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,7,61,llm,Link,No Link,"The experimental results of both GLUE and XTREAMRE show that the improvements are range from 0.2 to 0.8 (Avg score), which seems not significant.","In addition, the performance improvement on relatively small datasets is usually larger (e.g., RTE, CoLA and WNLI).",False,False
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,7,94,retriever,Link,Link,"The experimental results of both GLUE and XTREAMRE show that the improvements are range from 0.2 to 0.8 (Avg score), which seems not significant.",Extensive experiments on the monolingual GLUE benchmark and the multilingual XTREME benchmark demonstrate the NoisyTune can consistently improve the performance of different PLMs in various downstream tasks.,True,True
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,9,19,retriever,Link,Link,"Q2. The over-fitting problem is more serious in large models, so the experiment of applying NoisyTune on larger PLMs will be more convincing.","The key idea of NoisyTune is to add a little noise to perturb PLMs parameters before finetuning, which can help prevent them from overfitting the signals in the pretraining tasks, and reduce the gap between pretraining and downstreaming tasks.",True,True
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,9,71,random,No Link,No Link,"Q2. The over-fitting problem is more serious in large models, so the experiment of applying NoisyTune on larger PLMs will be more convincing.","In addition, we find an interesting phenomenon that adding uniform noise is better than using Gaussian noise.",True,False
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,9,79,random,Link,Link,"Q2. The over-fitting problem is more serious in large models, so the experiment of applying NoisyTune on larger PLMs will be more convincing.",This may be because the perturbed PLMs may have lower risks in overfitting pretraining tasks and have better generalization abilities.,True,True
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,9,83,llm,Link,Link,"Q2. The over-fitting problem is more serious in large models, so the experiment of applying NoisyTune on larger PLMs will be more convincing.","This means that directly finetuning PLMs may need more updates to adapt to downstream tasks, which may be due to the overfitting of pretraining tasks and their gaps with downstream tasks.",True,True
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,11,2,retriever,No Link,No Link,"Q4. Missing the most relevant work, ­­­raise a child in Large Language Model: Towards Effective and Generalizable Fine-tuning. Xu et al., EMNLP 2021. It masks out the gradients of the parameters during the backward process.",Effectively finetuning pretrained language models (PLMs) is critical for their success in downstream tasks.,True,False
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,11,13,llm,Link,Link,"Q4. Missing the most relevant work, ­­­raise a child in Large Language Model: Towards Effective and Generalizable Fine-tuning. Xu et al., EMNLP 2021. It masks out the gradients of the parameters during the backward process.","Besides naively finetuning PLMs with labeled data in downstream tasks, many works explore more effective and robust PLM finetuning methods Jiang et al., 2020;Lee et al., 2020;Aghajanyan et al., 2021;Xu et al., 2021).",True,True
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,11,16,random,Link,No Link,"Q4. Missing the most relevant work, ­­­raise a child in Large Language Model: Towards Effective and Generalizable Fine-tuning. Xu et al., EMNLP 2021. It masks out the gradients of the parameters during the backward process.",These finetuning methods mainly focus on preventing PLMs from overfitting limited labeled data in downstream tasks.,False,False
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,11,49,random,No Link,No Link,"Q4. Missing the most relevant work, ­­­raise a child in Large Language Model: Towards Effective and Generalizable Fine-tuning. Xu et al., EMNLP 2021. It masks out the gradients of the parameters during the backward process.",The XTREME results are evaluated on the test set.,True,False
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,12,3,random,No Link,No Link,The difference between NoisyTune and above-mentioned paper is using masking or noisy to perturb parameters.,"However, PLMs may have risks in overfitting pretraining signals, and there are some gaps between downstream tasks and the pretraining tasks.",True,False
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,12,72,random,No Link,No Link,The difference between NoisyTune and above-mentioned paper is using masking or noisy to perturb parameters.,This may be because Gaussian noise has wider ranges and some outliers may affect model performance.,True,False
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,13,28,random,No Link,No Link,Thus I am very interested to see which method (masking or noisy) will have better benefits.,as,True,False
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,13,5,retriever,Link,No Link,Thus I am very interested to see which method (masking or noisy) will have better benefits.,"In this paper, we propose a very simple yet effective method named NoisyTune which can help better finetune PLMs in downstream tasks by adding some noise to the parameters of PLMs before finetuning.",False,False
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,13,65,llm,No Link,Link,Thus I am very interested to see which method (masking or noisy) will have better benefits.,"Next, we study the influence of using different kinds of noise on NoisyTune.",False,False
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,13,71,random,No Link,No Link,Thus I am very interested to see which method (masking or noisy) will have better benefits.,"In addition, we find an interesting phenomenon that adding uniform noise is better than using Gaussian noise.",True,False
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,15,10,random,No Link,No Link,See above,"Many PLMs such as BERT (Devlin et al., 2019) and RoBERTa  have played critical roles in various applications, such as reading comprehension, machine translation and text classification (Dong et al., 2019).",True,False
04e268a99f3f1898997dd6ddc0bf734723dea4f418744e3233c527fecaa4af3816b7efc8c8a8689bfe88e2d44cb5683cfd6049a868d3fac6bb8d9696470ef3e5,15,42,random,No Link,No Link,See above,Datasets and Experimental Settings,True,False
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,1,3,mutual,Link,Link,"This paper presents an interesting finding, i.e., fine-tuning only the bias terms of pre-trained language models is competitive with fine-tuning the entire model.","We show that with small-to-medium training data, fine-tuning only the bias terms (or a subset of the bias terms) of pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model.",True,True
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,1,4,mutual,Link,Link,"This paper presents an interesting finding, i.e., fine-tuning only the bias terms of pre-trained language models is competitive with fine-tuning the entire model.","For larger data, bias-only fine-tuning is competitive with other sparse fine-tuning methods.",True,True
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,1,38,llm,Link,Link,"This paper presents an interesting finding, i.e., fine-tuning only the bias terms of pre-trained language models is competitive with fine-tuning the entire model.","We show that by freezing all the parameters W (•) and g (•) and fine-tuning only the additive bias terms b (•) , we achieve transfer learning performance which is comparable (and sometimes better!) than fine-tuning of the entire network.",True,True
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,1,49,random,No Link,No Link,"This paper presents an interesting finding, i.e., fine-tuning only the bias terms of pre-trained language models is competitive with fine-tuning the entire model.","(2019) (respectively), on their least-parameters setting.",True,False
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,1,58,random,No Link,No Link,"This paper presents an interesting finding, i.e., fine-tuning only the bias terms of pre-trained language models is competitive with fine-tuning the entire model.",We define the amount of change in a bias vector b to be,True,False
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,1,83,retriever,Link,Link,"This paper presents an interesting finding, i.e., fine-tuning only the bias terms of pre-trained language models is competitive with fine-tuning the entire model.","Besides its empirical utility, the remarkable effectiveness of bias-only fine-tuning raises intriguing questions on the fine-tuning dynamics of pretrained transformers, and the relation between the bias terms and transfer between LM and new tasks.",True,True
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,2,19,random,No Link,Link,"The authors compared the proposed method Bias-terms Fine-tuning (BitFit) with other parameter-efficient fine-tuning methods (e.g., Adapters, Diff-Pruning).","For small to medium training data, changing only these parameters reaches the same task accuracy as full fine-tuning, and sometimes even improves results.",False,False
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,2,23,retriever,Link,Link,"The authors compared the proposed method Bias-terms Fine-tuning (BitFit) with other parameter-efficient fine-tuning methods (e.g., Adapters, Diff-Pruning).","2 Background: fine-tuning and parameter-efficient fine-tuning  2020) (""Diff-Pruning""), achieves the same goal by adding a sparse, task-specific difference-vector to the original parameters, which remain fixed and are shared between tasks.",True,True
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,2,30,retriever,No Link,Link,"The authors compared the proposed method Bias-terms Fine-tuning (BitFit) with other parameter-efficient fine-tuning methods (e.g., Adapters, Diff-Pruning).",3 Bias-terms Fine-tuning (BitFit),False,False
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,2,31,retriever,Link,Link,"The authors compared the proposed method Bias-terms Fine-tuning (BitFit) with other parameter-efficient fine-tuning methods (e.g., Adapters, Diff-Pruning).","We propose a method we call BitFit (BIas-Term FIne-Tuning), in which we freeze most of the transformer-encoder parameters, and train only the bias-terms and the task-specific classification layer.",True,True
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,2,45,random,No Link,No Link,"The authors compared the proposed method Bias-terms Fine-tuning (BitFit) with other parameter-efficient fine-tuning methods (e.g., Adapters, Diff-Pruning).",Models and Optimization.,True,False
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,2,48,llm,Link,Link,"The authors compared the proposed method Bias-terms Fine-tuning (BitFit) with other parameter-efficient fine-tuning methods (e.g., Adapters, Diff-Pruning).","Comparison to Diff-Pruning and Adapters (Table 1) In the first experiment, we compare Bit-Fit to Diff-Pruning method and Adapters method, when using a fewer number of parameters.",True,True
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,2,51,llm,Link,Link,"The authors compared the proposed method Bias-terms Fine-tuning (BitFit) with other parameter-efficient fine-tuning methods (e.g., Adapters, Diff-Pruning).","On validation set, BitFit outperforms Diff-Pruning on 5 out of 9 tasks, and underperforms in 3, while using fewer trainable parameters 3 .",True,True
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,2,52,llm,Link,Link,"The authors compared the proposed method Bias-terms Fine-tuning (BitFit) with other parameter-efficient fine-tuning methods (e.g., Adapters, Diff-Pruning).","Test-set results are less conclusive (only one clear win compared to Diff-Pruning and 3 clear wins compared to Adapters), though BitFit is still competitive with both Diff-Pruning and Adapters.",True,True
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,3,30,random,No Link,No Link,The experimental results on GLUE benchmark show that BitFit can achieve strong performance with less trainable parameters.,3 Bias-terms Fine-tuning (BitFit),True,False
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,3,51,llm,Link,Link,The experimental results on GLUE benchmark show that BitFit can achieve strong performance with less trainable parameters.,"On validation set, BitFit outperforms Diff-Pruning on 5 out of 9 tasks, and underperforms in 3, while using fewer trainable parameters 3 .",True,True
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,3,57,random,Link,No Link,The experimental results on GLUE benchmark show that BitFit can achieve strong performance with less trainable parameters.,Fewer bias parameters (Table 3) Can we finetune on only a subset of the bias-parameter?,False,False
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,3,71,retriever,Link,Link,The experimental results on GLUE benchmark show that BitFit can achieve strong performance with less trainable parameters.,"The GLUE results suggest a reverse correlation between BitFit ability to reach Full-FT performance, and training set size.",True,True
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,5,11,random,No Link,No Link,- The paper is well written and easy to understand.,"We present a simple and effective approach to fine tuning ( §3), which has the following benefits:",True,False
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,5,96,random,No Link,No Link,- The paper is well written and easy to understand.,Learning rate configurations for best performing models are in Table 6.,True,False
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,6,31,llm,No Link,Link,- The proposed method (BitFit) is neat and novel.,"We propose a method we call BitFit (BIas-Term FIne-Tuning), in which we freeze most of the transformer-encoder parameters, and train only the bias-terms and the task-specific classification layer.",False,False
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,6,43,random,No Link,Link,- The proposed method (BitFit) is neat and novel.,"We evaluate BitFit on the GLUE benchmark (Wang et al., 2018).",False,False
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,6,74,retriever,Link,Link,- The proposed method (BitFit) is neat and novel.,We conclude that BitFit is a worthwhile targetted finetuning method in small-to-medium data regimes.,True,True
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,6,75,random,No Link,No Link,- The proposed method (BitFit) is neat and novel.,Related Work,True,False
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,7,37,random,No Link,No Link,- The authors show strong empirical results on GLUE benchmark.,"The collection of all matrices W The bias terms are additive, and correspond to a very small fraction of the network, in BERT BASE and BERT LARGE bias parameters make up 0.09% and 0.08% of the total number of parameters in each model, respectively.",True,False
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,7,43,llm,Link,Link,- The authors show strong empirical results on GLUE benchmark.,"We evaluate BitFit on the GLUE benchmark (Wang et al., 2018).",True,True
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,7,66,random,No Link,No Link,- The authors show strong empirical results on GLUE benchmark.,Token-level tasks.,True,False
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,7,95,retriever,Link,No Link,- The authors show strong empirical results on GLUE benchmark.,"We test our approach on the following subset of the GLUE (Wang et al., 2018)    The metrics that we used to evaluate GLUE Benchmark are in Table 5.",False,False
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,9,71,random,No Link,No Link,I do not have any concerns about this paper.,"The GLUE results suggest a reverse correlation between BitFit ability to reach Full-FT performance, and training set size.",True,False
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,9,78,random,No Link,No Link,I do not have any concerns about this paper.,An exception is the work of Wang et al. ( 2019) who analyzed bias terms from the perspective of attribution method.,True,False
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,11,52,retriever,Link,Link,"It would be helpful to compare BitFit with Adapter and Diff-Pruning base on other language models (e.g.,RoBERTa, T5).","Test-set results are less conclusive (only one clear win compared to Diff-Pruning and 3 clear wins compared to Adapters), though BitFit is still competitive with both Diff-Pruning and Adapters.",True,True
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,11,53,llm,Link,No Link,"It would be helpful to compare BitFit with Adapter and Diff-Pruning base on other language models (e.g.,RoBERTa, T5).",Different Base-models (Table 2) We repeat the BERT LARGE results on different base-models (the smaller BERT BASE and the better performing RoBERTa BASE ).,False,False
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,11,83,random,No Link,No Link,"It would be helpful to compare BitFit with Adapter and Diff-Pruning base on other language models (e.g.,RoBERTa, T5).","Besides its empirical utility, the remarkable effectiveness of bias-only fine-tuning raises intriguing questions on the fine-tuning dynamics of pretrained transformers, and the relation between the bias terms and transfer between LM and new tasks.",True,False
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,11,86,random,No Link,No Link,"It would be helpful to compare BitFit with Adapter and Diff-Pruning base on other language models (e.g.,RoBERTa, T5).","To perform classification with BERT, we follow the approach of Devlin et al. (2018), and attach a linear layer to the contextual embedding of the CLS token to predict the label.",True,False
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,12,14,random,No Link,No Link,But current version is good enough for a short paper.,2.,True,False
88ac10a6e12322a2c8840df75d487a2577b63632584a628e547dc5fac6aadab358ecb06ad1a6d4de7912453a986ee5327ed43f7e3e88687df9d29fa4d264f148,12,40,random,No Link,No Link,But current version is good enough for a short paper.,"m 2 ), and still achieve accuracies that rival full-model fine-tuning.",True,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,1,13,mutual,Link,Link,The paper argues that jointly adopting both label smoothing and vocabulary sharing techniques can be conflicting.,"Jointly adopting label smoothing and vocabulary sharing techniques cannot achieve further improvements, but leads to sub-optimal performance.",True,True
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,1,15,mutual,Link,Link,The paper argues that jointly adopting both label smoothing and vocabulary sharing techniques can be conflicting.,"However, in this paper, we argue that jointly adopting both label smoothing and vocabulary sharing techniques can be conflicting, and leads to suboptimal performance.",True,True
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,1,28,random,No Link,No Link,The paper argues that jointly adopting both label smoothing and vocabulary sharing techniques can be conflicting.,We formalized the categorization algorithm in Appendix A.,True,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,1,46,random,No Link,Link,The paper argues that jointly adopting both label smoothing and vocabulary sharing techniques can be conflicting.,"In this way, Masked Label Smoothing is parameter-free and implicitly injects external knowledge to the model. And we have found out that this simple setting can reach satisfactory results according our experiments.",False,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,1,81,mutual,Link,Link,The paper argues that jointly adopting both label smoothing and vocabulary sharing techniques can be conflicting.,"We reveal and analyse the conflict between label smoothing and vocabulary sharing techniques, and point out that jointly adopting them may lead to sub-optimal performance.",True,True
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,2,32,mutual,Link,Link,This comes from a simple intuition that words have zero overlap with the possible target words should have no chance to appear in the target sentence.,"Those words have zero overlap with the possible target words, therefore they have no chance to appear in the target sentence, which might introduce extra bias for the translation system during training process.",True,True
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,2,46,random,No Link,No Link,This comes from a simple intuition that words have zero overlap with the possible target words should have no chance to appear in the target sentence.,"In this way, Masked Label Smoothing is parameter-free and implicitly injects external knowledge to the model. And we have found out that this simple setting can reach satisfactory results according our experiments.",True,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,2,50,random,No Link,No Link,This comes from a simple intuition that words have zero overlap with the possible target words should have no chance to appear in the target sentence.,y is the current token during current decoding phase.,True,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,3,19,retriever,Link,Link,They propose to use a very simple approach (Masked label smoothing) that addresses the conflict.,To address the conflict of label smoothing and,True,True
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,3,34,random,No Link,Link,They propose to use a very simple approach (Masked label smoothing) that addresses the conflict.,Methods,False,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,3,37,random,No Link,No Link,They propose to use a very simple approach (Masked label smoothing) that addresses the conflict.,"Weighted Label Smoothing(WLS) has three parameters β t , β c , β s apart from the label smoothing parameter α, where the ratio of the three parameters represents the portion of smoothed probability allocated to the target, common and source class and the sum of the three parameters is 1.",True,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,3,45,llm,Link,Link,They propose to use a very simple approach (Masked label smoothing) that addresses the conflict.,"Based on the Weight Label Smoothing mechanism, we can now implement Masked Label Smoothing by set β s to 0 and regard the target and common category as one category.",True,True
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,3,46,mutual,Link,No Link,They propose to use a very simple approach (Masked label smoothing) that addresses the conflict.,"In this way, Masked Label Smoothing is parameter-free and implicitly injects external knowledge to the model. And we have found out that this simple setting can reach satisfactory results according our experiments.",False,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,3,82,mutual,Link,Link,They propose to use a very simple approach (Masked label smoothing) that addresses the conflict.,"To address this issue, we introduce a plug-and-play Masked Label Smoothing mechanism to eliminate the conflict.",True,True
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,4,6,retriever,Link,No Link,Results show that the method gains very incremental improvement in translation accuracy and some improvement in model calibration.,"Our extensive experiments show that MLS consistently yields improvement over original label smoothing on different datasets, including bilingual and multilingual translation in both BLEU and calibration scores.",False,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,4,12,random,No Link,No Link,Results show that the method gains very incremental improvement in translation accuracy and some improvement in model calibration.,"It enhances the semantic correlation   (Vaswani et al., 2017).",True,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,4,63,random,No Link,No Link,Results show that the method gains very incremental improvement in translation accuracy and some improvement in model calibration.,The results reveal the conflict between LS and VS that models with only LS outperform models with both LS and VS in all experiments.,True,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,4,74,llm,Link,No Link,Results show that the method gains very incremental improvement in translation accuracy and some improvement in model calibration.,The results indicate that MLS will lead to better calibration.,False,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,5,19,retriever,Link,No Link,Overall the conflict is interesting but the result is very mixed.,To address the conflict of label smoothing and,False,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,5,30,random,No Link,No Link,Overall the conflict is interesting but the result is very mixed.,Tokens in source class account for a large proportion up to 50%.,True,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,5,33,mutual,Link,No Link,Overall the conflict is interesting but the result is very mixed.,"Table 3 reveals the existence of conflict, that the joint use of label smoothing and vocabulary sharing doesn't compare with solely use one technique in all language pairs with a maximum loss of 0.32 BLEU score.",False,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,5,50,random,No Link,No Link,Overall the conflict is interesting but the result is very mixed.,y is the current token during current decoding phase.,True,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,5,63,mutual,Link,Link,Overall the conflict is interesting but the result is very mixed.,The results reveal the conflict between LS and VS that models with only LS outperform models with both LS and VS in all experiments.,True,True
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,5,67,llm,Link,Link,Overall the conflict is interesting but the result is very mixed.,"Compared with the imbalanced version, the balanced version gave better BLEU scores in DE-EN direction while much worse performance in RO-EN translation.",True,True
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,7,3,llm,Link,Link,The paper presents an interesting conflict between these two approaches.,"However, we argue that jointly adopting these two techniques can be conflicting and even leads to sub-optimal performance, since the soft label produced by label smoothing still considers the source-side words that would not appear at the target side.",True,True
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,7,14,random,No Link,No Link,The paper presents an interesting conflict between these two approaches.,between the two languages and reduces the number of total parameters of the embedding matrices.,True,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,7,15,mutual,Link,Link,The paper presents an interesting conflict between these two approaches.,"However, in this paper, we argue that jointly adopting both label smoothing and vocabulary sharing techniques can be conflicting, and leads to suboptimal performance.",True,True
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,7,19,retriever,Link,Link,The paper presents an interesting conflict between these two approaches.,To address the conflict of label smoothing and,True,True
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,7,31,random,Link,No Link,The paper presents an interesting conflict between these two approaches.,"When label smoothing and vocabulary sharing are together applied, the smoothed probability will be allocated to words that belong to the source class.",False,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,7,63,mutual,Link,Link,The paper presents an interesting conflict between these two approaches.,The results reveal the conflict between LS and VS that models with only LS outperform models with both LS and VS in all experiments.,True,True
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,8,59,random,No Link,No Link,I personally never thought about this before.,"During training, we fix the label smoothing parameter α to 0.1 whenever LS is applied.",True,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,8,72,random,No Link,No Link,I personally never thought about this before.,Inference ECE score  reflects models calibration during inference.,True,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,10,3,random,No Link,Link,Cons:,"However, we argue that jointly adopting these two techniques can be conflicting and even leads to sub-optimal performance, since the soft label produced by label smoothing still considers the source-side words that would not appear at the target side.",False,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,10,9,retriever,No Link,No Link,Cons:,"For most NMT studies (Vaswani et al., 2017;Song et al., 2019;Lin et al., 2020;Pan et al., 2021), there are two widely used techniques to improve the quality of the translation: Label Smoothing (LS) and Vocabulary Sharing (VS).",True,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,10,65,random,No Link,No Link,Cons:,"The effectiveness of MLS maintained under different α value as shown in Table 6,7 for both BLEU and chrF, which further proves that not only the increase in target vocabulary, but also the decrease of probabilities in source vocabulary matters in the improvement of translation performance.",True,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,10,67,llm,No Link,No Link,Cons:,"Compared with the imbalanced version, the balanced version gave better BLEU scores in DE-EN direction while much worse performance in RO-EN translation.",True,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,11,6,random,Link,No Link,"From translation accuracy perspective, the improvement from fixing the conflict is very incremental.","Our extensive experiments show that MLS consistently yields improvement over original label smoothing on different datasets, including bilingual and multilingual translation in both BLEU and calibration scores.",False,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,11,24,random,No Link,No Link,"From translation accuracy perspective, the improvement from fixing the conflict is very incremental.",Researchers have conducted in-depth studies in Vocabulary Sharing.,True,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,12,34,random,No Link,No Link,This is clearly shown as the translation improvement is only +0.47 at its best.,Methods,True,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,12,76,random,Link,Link,This is clearly shown as the translation improvement is only +0.47 at its best.,"As reported in Table 5, we further explore the influence of different weighted label smoothing settings on multiple translation tasks including IWSLT'16  According to the result, though the best BLEU score's WLS setting vary from different tasks, we still have two observations: First, applying WLS can generally boost the quality of translation compared to the original label smoothing.",True,True
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,14,35,random,No Link,No Link,"This shows that despite the intuition, the conflict actually does not matter in reality.",Weighted Label Smoothing,True,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,14,59,random,No Link,No Link,"This shows that despite the intuition, the conflict actually does not matter in reality.","During training, we fix the label smoothing parameter α to 0.1 whenever LS is applied.",True,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,17,33,random,No Link,Link,Typo + writing,"Table 3 reveals the existence of conflict, that the joint use of label smoothing and vocabulary sharing doesn't compare with solely use one technique in all language pairs with a maximum loss of 0.32 BLEU score.",False,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,17,48,random,No Link,Link,Typo + writing,It is worth noticing that MLS is different from setting WLS's parameters to 1-1-0 since there might be different number of tokens in the common and target vocab.,False,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,20,60,random,No Link,No Link,- About ECE score: There should be an explanation from the paper.,We list the concrete training and evaluation settings in Appendix B.,True,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,20,72,mutual,Link,Link,- About ECE score: There should be an explanation from the paper.,Inference ECE score  reflects models calibration during inference.,True,True
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,20,73,mutual,Link,Link,- About ECE score: There should be an explanation from the paper.,We compute the inference ECE scores of our models as shown in Table 4.,True,True
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,20,90,random,Link,Link,- About ECE score: There should be an explanation from the paper.,The inference ECE score 1 and chrF score 2 are computed through open source scripts.,True,True
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,21,20,random,No Link,No Link,- Writing style: the authors use the word outperform a lot in the paper.,Background,True,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,21,52,random,No Link,No Link,- Writing style: the authors use the word outperform a lot in the paper.,Experiments,True,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,22,12,random,No Link,No Link,"Clearly the improvement does not reflect an ""outperforming"" by anymeans.","It enhances the semantic correlation   (Vaswani et al., 2017).",True,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,22,18,retriever,Link,Link,"Clearly the improvement does not reflect an ""outperforming"" by anymeans.","As shown in Table 1, although introducing label smoothing or vocabulary sharing alone can outperform the vanilla Transformer, jointly adopting both of them cannot obtain further improvements but achieves sub-optimal results.",True,True
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,22,63,random,Link,Link,"Clearly the improvement does not reflect an ""outperforming"" by anymeans.",The results reveal the conflict between LS and VS that models with only LS outperform models with both LS and VS in all experiments.,True,True
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,22,77,llm,No Link,Link,"Clearly the improvement does not reflect an ""outperforming"" by anymeans.","Second, only WLS with β t , β c , β s each equals to 1/2-1/2-0 can outperform the original label smoothing on all tasks, which suggests the setting is the most robust one.",False,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,23,24,retriever,No Link,No Link,I suggest the authors lower the use of word...,Researchers have conducted in-depth studies in Vocabulary Sharing.,True,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,23,38,random,No Link,No Link,I suggest the authors lower the use of word...,The distribution within token class follows a uniform distribution.,True,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,23,78,llm,No Link,No Link,I suggest the authors lower the use of word...,Thus we recommend using this setting as the initial setting when applying our WLS.,True,False
af2f6b70c99b5a2bacf587857e7c7a4c1a949f542fe493f52e5c659c2dde07dfaadfc5fc21d89ed70a2dbe086c8cb226deea50fbabbab23e6229b3da5853dd66,23,81,random,No Link,No Link,I suggest the authors lower the use of word...,"We reveal and analyse the conflict between label smoothing and vocabulary sharing techniques, and point out that jointly adopting them may lead to sub-optimal performance.",True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,1,3,mutual,Link,Link,This paper studies the Machine Translation of the endangered language Livonian.,"In this paper we tackle the task of developing neural machine translation (NMT) between Livonian and English, with a two-fold aim: on one hand, preserving the language and on the other -enabling access to Livonian folklore, lifestories and other textual intangible heritage as well as making it easier to create further parallel corpora.",True,True
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,1,14,mutual,Link,Link,This paper studies the Machine Translation of the endangered language Livonian.,In this paper we set the goal of developing usable machine translation between English and Livonian.,True,True
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,1,26,random,Link,No Link,This paper studies the Machine Translation of the endangered language Livonian.,We explore several options of coping with the extremely lowresource settings and use Estonian and Latvian for cross-lingual transfer.,False,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,1,44,random,No Link,Link,This paper studies the Machine Translation of the endangered language Livonian.,Collected Data,False,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,1,95,mutual,Link,Link,This paper studies the Machine Translation of the endangered language Livonian.,"In this paper we presented a novel dataset for the highly endangered Livonian language, which can be useful for machine translation, language modelling and many other natural language processing and computational linguistic research tasks.",True,True
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,2,4,llm,Link,Link,"The authors first collect a parallel corpus on LIV, LV, ET and EN, by assembling available digital resources and via manual translations.",We rely on Livonian's linguistic similarity to Estonian and Latvian and collect parallel and monolingual data for the four languages for translation experiments.,True,True
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,2,16,retriever,Link,Link,"The authors first collect a parallel corpus on LIV, LV, ET and EN, by assembling available digital resources and via manual translations.","Although some digital linguistic resources exist for Livonian (including a dictionary with example sentences and a written monolingual corpus, (Ernštreits, 2016)), there is virtually no open parallel corpora with it, with the single exception of 35 parallel sentences in the OPUS Tatoeba corpus (Tiedemann, 2020).",True,True
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,2,22,random,No Link,No Link,"The authors first collect a parallel corpus on LIV, LV, ET and EN, by assembling available digital resources and via manual translations.",Our main contributions are two-fold.,True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,2,45,llm,Link,Link,"The authors first collect a parallel corpus on LIV, LV, ET and EN, by assembling available digital resources and via manual translations.",The first step in developing (supervised) machine translation is collecting parallel data.,True,True
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,2,46,mutual,Link,No Link,"The authors first collect a parallel corpus on LIV, LV, ET and EN, by assembling available digital resources and via manual translations.","While there was no pre-existing open parallel corpus with Livonian, we used all the possible sources of translations.",False,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,2,78,random,No Link,No Link,"The authors first collect a parallel corpus on LIV, LV, ET and EN, by assembling available digital resources and via manual translations.",Results,True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,2,89,retriever,Link,Link,"The authors first collect a parallel corpus on LIV, LV, ET and EN, by assembling available digital resources and via manual translations.","Results from these experiments in Table 3 show that not only ET is better than LV to pivot translate between EN and LIV, but also that this approach was able to reach the highest BLEU scores so far.",True,True
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,3,9,random,No Link,No Link,"Then different ""base"" machine translations are trained and finetuned on the LIV->EN data.","Many state-of-the-art natural language processing tasks have reached admirable quality on languages with abundant linguistic resources (Vaswani et al., 2017;Devlin et al., 2019).",True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,3,20,random,No Link,No Link,"Then different ""base"" machine translations are trained and finetuned on the LIV->EN data.","On the other hand, Livonian has taken part in forming Latvian language and Livonian speakers have historically co-existed side-by-side with Latvian speakers.",True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,3,73,mutual,Link,Link,"Then different ""base"" machine translations are trained and finetuned on the LIV->EN data.","Base models were trained on LV→EN, ET→EN, ET+LV→EN data, and a multilingual model using the tagged approach (Johnson et al., 2017) for translating in all directions between ENG/EST/LAT.",True,True
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,3,74,llm,Link,Link,"Then different ""base"" machine translations are trained and finetuned on the LIV->EN data.",The base models were then used as initialization for tuning on LIV→EN data.,True,True
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,3,83,mutual,Link,Link,"Then different ""base"" machine translations are trained and finetuned on the LIV->EN data.",We then turned to tuning each of these models with LIV-EN data mixed 1:1 with a random equal amount of the original training data for each of the models.,True,True
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,3,96,retriever,Link,Link,"Then different ""base"" machine translations are trained and finetuned on the LIV->EN data.","In our experiments we show how far one can get in training modern machine translation models with very scarce data, and which languages are more suitable for transfer learning when working with Livonian data.",True,True
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,4,12,random,No Link,No Link,Backtranslations and pivot translation are also explored.,Models and corpora will also be added to the Huggingface repository after de-anonymization (https://huggingface.co/).,True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,4,31,retriever,No Link,Link,Backtranslations and pivot translation are also explored.,3. Does zero-shot multilingual translation deliver better translation quality than pivottranslation through Estonian or Latvian?,False,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,4,43,random,No Link,No Link,Backtranslations and pivot translation are also explored.,"It is the only Finnic language that differentiates lexical tones -the plain tone and the broken tone or stød -and therefore shares similar characteristics with Latvian as well as Danish (Tuisk, 2016).",True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,4,77,llm,Link,Link,Backtranslations and pivot translation are also explored.,"Finally, we used the highest-scoring tuned model to perform performed backtranslation on the monolingual LIV data to generate additional training data for training the final models.",True,True
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,4,86,mutual,Link,Link,Backtranslations and pivot translation are also explored.,"In order to perform backtranslation models for both directions are required, so we scored the tuned multilingual model on the EN→LIV data as well, reaching 8.10 BLEU.",True,True
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,4,93,mutual,Link,Link,Backtranslations and pivot translation are also explored.,"Interestingly, pivot-translation through Estonian showed higher translation quality than direct Livonian↔English trained in a zero-shot / few-shot manner.",True,True
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,6,3,mutual,Link,Link,NMT for endangered languages is a critical and interesting research direction.,"In this paper we tackle the task of developing neural machine translation (NMT) between Livonian and English, with a two-fold aim: on one hand, preserving the language and on the other -enabling access to Livonian folklore, lifestories and other textual intangible heritage as well as making it easier to create further parallel corpora.",True,True
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,6,5,mutual,Link,No Link,NMT for endangered languages is a critical and interesting research direction.,"We combine different low-resource NMT techniques like zero-shot translation, cross-lingual transfer and synthetic data creation to reach the highest possible translation quality as well as to find which base languages are empirically more helpful for transfer to Livonian.",False,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,6,6,retriever,Link,Link,NMT for endangered languages is a critical and interesting research direction.,"The resulting NMT systems and the collected monolingual and parallel data, including a manually translated and verified translation benchmark, are publicly released.",True,True
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,6,25,llm,Link,Link,NMT for endangered languages is a critical and interesting research direction.,"The second half of our work focuses on neural machine translation (NMT, Vaswani et al., 2017), mainly targeting Livonian↔English.",True,True
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,6,41,random,No Link,No Link,NMT for endangered languages is a critical and interesting research direction.,"The most obvious Latvian influence on Livonian grammar is found in the Livonian case system (Ernštreits and Kl , ava, 2014).",True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,6,60,random,No Link,No Link,NMT for endangered languages is a critical and interesting research direction.,We hired professional translators to create translations for any missing parts so that these splits would be parallel between all four languages.,True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,7,6,llm,Link,No Link,The proposed parallel corpus is valuable and make great contributions to Livonian NMT.,"The resulting NMT systems and the collected monolingual and parallel data, including a manually translated and verified translation benchmark, are publicly released.",False,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,7,16,retriever,Link,Link,The proposed parallel corpus is valuable and make great contributions to Livonian NMT.,"Although some digital linguistic resources exist for Livonian (including a dictionary with example sentences and a written monolingual corpus, (Ernštreits, 2016)), there is virtually no open parallel corpora with it, with the single exception of 35 parallel sentences in the OPUS Tatoeba corpus (Tiedemann, 2020).",True,True
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,7,72,random,No Link,No Link,The proposed parallel corpus is valuable and make great contributions to Livonian NMT.,"We used Sentencepiece (Kudo and Richardson, 2018)  ies of size 25,000, and SacreBLEU 7 (Post, 2018) to generate BLEU scores for translations.",True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,7,80,random,No Link,No Link,The proposed parallel corpus is valuable and make great contributions to Livonian NMT.,"All BLEU scores are calculated for translations of our evaluation We compare the base single direction MT models to our multidirection model, as well as online translations from Google Translate 8 and Neurotolge 9 to evaluate performance from ET and LV into EN.",True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,8,59,random,No Link,No Link,"Interesting discoveries are made, e.g., the authors find that ET is a preferred pivot than LV in LIV-EN translation.",The splits are balanced in terms of the original source of the texts to resemble proportions from the remaining training data.,True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,8,72,random,No Link,No Link,"Interesting discoveries are made, e.g., the authors find that ET is a preferred pivot than LV in LIV-EN translation.","We used Sentencepiece (Kudo and Richardson, 2018)  ies of size 25,000, and SacreBLEU 7 (Post, 2018) to generate BLEU scores for translations.",True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,8,82,mutual,Link,Link,"Interesting discoveries are made, e.g., the authors find that ET is a preferred pivot than LV in LIV-EN translation.","However, when attempting to perform zero-shot translation from LIV into EN, the ET→EN model was able to outperform LV→EN (3.22 vs. 2.20), and the multilingual model achieved a very respectable BLEU score 8.92.",True,True
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,8,89,mutual,Link,Link,"Interesting discoveries are made, e.g., the authors find that ET is a preferred pivot than LV in LIV-EN translation.","Results from these experiments in Table 3 show that not only ET is better than LV to pivot translate between EN and LIV, but also that this approach was able to reach the highest BLEU scores so far.",True,True
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,8,93,mutual,Link,Link,"Interesting discoveries are made, e.g., the authors find that ET is a preferred pivot than LV in LIV-EN translation.","Interestingly, pivot-translation through Estonian showed higher translation quality than direct Livonian↔English trained in a zero-shot / few-shot manner.",True,True
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,10,3,random,No Link,No Link,Some evaluations results are mixing.,"In this paper we tackle the task of developing neural machine translation (NMT) between Livonian and English, with a two-fold aim: on one hand, preserving the language and on the other -enabling access to Livonian folklore, lifestories and other textual intangible heritage as well as making it easier to create further parallel corpora.",True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,10,58,retriever,Link,Link,Some evaluations results are mixing.,We separated balanced portions of development (503 sentences) and evaluation (749 sentences) splits from the full dataset.,True,True
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,10,65,random,No Link,No Link,Some evaluations results are mixing.,"Having just over 10, 000 parallel examples constitutes extremely low-resource settings for neural machine translation.",True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,10,87,llm,No Link,No Link,Some evaluations results are mixing.,"Finally, to verify how much the tiny amount (503 sentences) of LIV-EN parallel data brings for tuning we ran a separate experiment excluding it and achieved 11.87 BLEU for LIV-EN and 8.25 BLEU for EN-LIV, which is even slightly higher than with the data there.",True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,11,24,random,No Link,No Link,Please see the detailed comments below.,2,True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,11,6,random,No Link,No Link,Please see the detailed comments below.,"The resulting NMT systems and the collected monolingual and parallel data, including a manually translated and verified translation benchmark, are publicly released.",True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,13,34,random,No Link,No Link,Question:,the mid-20th century around 1500 speakers.,True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,13,76,random,No Link,No Link,Question:,"To facilitate further use of the base models for tuning on Livonian data, all Livonian sentences were used in addition to other data when creating the shared vocabularies.",True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,15,35,random,No Link,No Link,Could you provide more discussions/explanations on this?,"Nowadays Livonian is listed in UNESCO's Atlas of the World's Languages in Danger as a critically endangered language (Moseley, 2014).",True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,15,59,random,No Link,No Link,Could you provide more discussions/explanations on this?,The splits are balanced in terms of the original source of the texts to resemble proportions from the remaining training data.,True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,18,33,random,No Link,No Link,minor:,Section 4 provides the details of our NMT experiments and Section 5 concludes the paper.,True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,18,48,random,No Link,No Link,minor:,The main sources of data included Livonian-Latvian as well as Livonian-Estonian translations.,True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,20,60,random,No Link,No Link,I was a bit confused when first reading that footnote since I wasn't sure how the translation is done.,We hired professional translators to create translations for any missing parts so that these splits would be parallel between all four languages.,True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,20,61,llm,No Link,No Link,I was a bit confused when first reading that footnote since I wasn't sure how the translation is done.,We further turned to experts of the Livonian language to make sure that the newly created translations truly convey the meaning of the original text as a quality control measure.,True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,20,89,random,No Link,No Link,I was a bit confused when first reading that footnote since I wasn't sure how the translation is done.,"Results from these experiments in Table 3 show that not only ET is better than LV to pivot translate between EN and LIV, but also that this approach was able to reach the highest BLEU scores so far.",True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,20,91,retriever,No Link,Link,I was a bit confused when first reading that footnote since I wasn't sure how the translation is done.,"To answer the research questions, posed in the introduction, it seems that the resulting translation quality is still far from being usable.",False,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,22,17,retriever,Link,Link,"3. For future work, it may be worth considering adding cross-lingual contrastive learning to the training.","At the same time, cross-lingual transfer learning has recently helped improve the performance of several low-resource NLP tasks with the support of related languages (e.g. Hu et al., 2020).",True,True
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,22,40,random,Link,No Link,"3. For future work, it may be worth considering adding cross-lingual contrastive learning to the training.","Next to other loanwords, the Livonian loanword strata consists of words borrowed from Latvian (Suhonen, 1973;Winkler, 2014) and vice versa.",False,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,22,47,llm,No Link,Link,"3. For future work, it may be worth considering adding cross-lingual contrastive learning to the training.","This was limited to already digital resources, future work might include texts extracted by scanning older books and other materials.",False,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,22,59,random,No Link,No Link,"3. For future work, it may be worth considering adding cross-lingual contrastive learning to the training.",The splits are balanced in terms of the original source of the texts to resemble proportions from the remaining training data.,True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,24,12,random,No Link,No Link,Those in the 4-language parallel corpus?,Models and corpora will also be added to the Huggingface repository after de-anonymization (https://huggingface.co/).,True,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,24,16,retriever,No Link,Link,Those in the 4-language parallel corpus?,"Although some digital linguistic resources exist for Livonian (including a dictionary with example sentences and a written monolingual corpus, (Ernštreits, 2016)), there is virtually no open parallel corpora with it, with the single exception of 35 parallel sentences in the OPUS Tatoeba corpus (Tiedemann, 2020).",False,False
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,24,53,llm,Link,Link,Those in the 4-language parallel corpus?,"• the Livonian Institute's Facebook page posts, partially parallel between our 4 languages",True,True
14b56e716fb2e80bfd2ac52df5b675b7c1760acbc12ec82ad5270e11050c154bfc5bccd66ea31348cbc56338d85b99a3f1260b2aeaf71da83ad9c32f449824a6,24,62,random,No Link,No Link,Those in the 4-language parallel corpus?,"The resulting benchmark and the whole corpus is published in the OPUS collection (currently, anonymously).",True,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,1,6,mutual,Link,Link,"This paper used chi-squared measures, t-statistics, and raw frequency to build a token merging pre-processing step, in order to improve the results of LDA in languages without marked word boundaries.","Here, we explore the use of retokenization based on chi-squared measures, tstatistics, and raw frequency to merge frequent token ngrams into collocations when preparing input to the LDA model.",True,True
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,1,24,random,No Link,No Link,"This paper used chi-squared measures, t-statistics, and raw frequency to build a token merging pre-processing step, in order to improve the results of LDA in languages without marked word boundaries.","When it is separated into kōn (people) sǔa 448 (shirt) dāng (red), the key meaning is totally lost.",True,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,1,42,random,Link,Link,"This paper used chi-squared measures, t-statistics, and raw frequency to build a token merging pre-processing step, in order to improve the results of LDA in languages without marked word boundaries.","In this project, we investigate the effects of token merging as a pre-processing step, and study how those effects vary based on the writing systems and the morphological features of the languages.",True,True
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,1,43,mutual,Link,Link,"This paper used chi-squared measures, t-statistics, and raw frequency to build a token merging pre-processing step, in order to improve the results of LDA in languages without marked word boundaries.","We evaluate three measures to determine when to merge multiple adjacent words into conceptuallyunified phrasal tokens prior to LDA model training: chi-squared statistics, t-statistics, and raw frequency counts of phrases.",True,True
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,1,66,mutual,Link,No Link,"This paper used chi-squared measures, t-statistics, and raw frequency to build a token merging pre-processing step, in order to improve the results of LDA in languages without marked word boundaries.","In this paper, we evaluate the chisquared statistics (χ 2 ), the t-statistic and raw frequency as approaches to develop a threshold for merging collocations into multi-word tokens prior to topic model training.",False,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,2,5,mutual,Link,Link,"It is valuable to leverage collocations for languages without marked word boundaries (e.g., Chinese and Thai), and the experimental results indicated positive contributions of token emerging.","However, it is unclear how to achieve the best results for languages without marked word boundaries such as Chinese and Thai.",True,True
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,2,24,random,No Link,No Link,"It is valuable to leverage collocations for languages without marked word boundaries (e.g., Chinese and Thai), and the experimental results indicated positive contributions of token emerging.","When it is separated into kōn (people) sǔa 448 (shirt) dāng (red), the key meaning is totally lost.",True,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,2,33,mutual,Link,No Link,"It is valuable to leverage collocations for languages without marked word boundaries (e.g., Chinese and Thai), and the experimental results indicated positive contributions of token emerging.","This problem is amplified in languages without marked word boundaries, such as Chinese and Thai: while existing tokenizers in these languages can segment characters into words, there is always a question about to what extent the tokenizers should group words together.",False,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,2,63,mutual,Link,No Link,"It is valuable to leverage collocations for languages without marked word boundaries (e.g., Chinese and Thai), and the experimental results indicated positive contributions of token emerging.","For languages without clear word boundaries, there is a possible additional benefit to multi-word tokens: it can be hard to intuit whether inferred word boundaries will have a large impact on the final results.",False,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,2,87,random,No Link,No Link,"It is valuable to leverage collocations for languages without marked word boundaries (e.g., Chinese and Thai), and the experimental results indicated positive contributions of token emerging.","We run the experiment 3 times for each combination of corpus, type of retokenization (no retokenization, χ 2 , t or frequency) and number of topics to compute the means of the normalized held-out likelihood and CBES, discussed in section 4.",True,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,3,21,random,No Link,No Link,"However, this paper only compared the proposed approaches with LDA, without employing the existing similar approaches, e.g., the methods proposed in (Lau et al., 2013), as baselines.",We 438 see merged tokens in the topic key sets of almost 439 all topics in all corpora when retokenized based 440 on t or raw frequency.,True,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,3,34,random,No Link,No Link,"However, this paper only compared the proposed approaches with LDA, without employing the existing similar approaches, e.g., the methods proposed in (Lau et al., 2013), as baselines.",Words that have been segmented by tokenizers may not express the concept of the original text if they were found as parts of collocations.,True,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,3,53,mutual,Link,Link,"However, this paper only compared the proposed approaches with LDA, without employing the existing similar approaches, e.g., the methods proposed in (Lau et al., 2013), as baselines.","Many works recognize that LDA results can be improved when input are including phrases (Lindsey et al., 2012;Lau et al., 2013;Yu et al., 2013;El-Kishky et al., 2014;Wang et al., 2016;Bin et al., 2018;Li et al., 2018).",True,True
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,6,4,mutual,Link,Link,"1. The experiments were conducted on 7 languages, which was valuable to test whether representing bigrams collocations in the input could improve topic coherence or not in general.",Previous studies show that representing bigrams collocations in the input can improve topic coherence in English.,True,True
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,6,47,llm,Link,No Link,"1. The experiments were conducted on 7 languages, which was valuable to test whether representing bigrams collocations in the input could improve topic coherence or not in general.",• We determine through empirical studies that a t-statistic and raw-frequency approach to token merging improves the topic modeling results across all language types and writing systems for the corpora that do not differ much from the collocation training data.,False,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,6,59,random,No Link,No Link,"1. The experiments were conducted on 7 languages, which was valuable to test whether representing bigrams collocations in the input could improve topic coherence or not in general.","For our evaluation, we use a normalized log likelihood approach to capture fit while accounting for changes in vocabulary (Schofield and Mimno, 2016).",True,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,6,62,mutual,Link,Link,"1. The experiments were conducted on 7 languages, which was valuable to test whether representing bigrams collocations in the input could improve topic coherence or not in general.","We hypothesize that the introduction of multi-word tokens, which capture collocations as bigrams or trigrams by way of concatenation of adjacent tokens, can help achieve more useful and coherent topic models.",True,True
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,6,64,random,No Link,No Link,"1. The experiments were conducted on 7 languages, which was valuable to test whether representing bigrams collocations in the input could improve topic coherence or not in general.",Merging adjacent words into 'multi-word' tokens may help remedy the potential problem of a segmentation that is not optimal for topic modeling purposes.,True,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,6,68,retriever,Link,Link,"1. The experiments were conducted on 7 languages, which was valuable to test whether representing bigrams collocations in the input could improve topic coherence or not in general.",We first compute the collocation measures for all bigrams on a large collocation training corpus.,True,True
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,7,28,random,No Link,No Link,2. The experimental results indicated that the t-statistic and raw frequency approaches for token merging could improve the topic modeling results across 7 languages.,Introduction,True,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,7,47,mutual,Link,Link,2. The experimental results indicated that the t-statistic and raw frequency approaches for token merging could improve the topic modeling results across 7 languages.,• We determine through empirical studies that a t-statistic and raw-frequency approach to token merging improves the topic modeling results across all language types and writing systems for the corpora that do not differ much from the collocation training data.,True,True
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,7,48,llm,Link,Link,2. The experimental results indicated that the t-statistic and raw frequency approaches for token merging could improve the topic modeling results across 7 languages.,• We also show the positive consequences of token merging: the percentage of merged tokens in the LDA training data is correlated with the quality of the topic modeling results.,True,True
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,7,66,retriever,Link,Link,2. The experimental results indicated that the t-statistic and raw frequency approaches for token merging could improve the topic modeling results across 7 languages.,"In this paper, we evaluate the chisquared statistics (χ 2 ), the t-statistic and raw frequency as approaches to develop a threshold for merging collocations into multi-word tokens prior to topic model training.",True,True
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,7,96,random,Link,No Link,2. The experimental results indicated that the t-statistic and raw frequency approaches for token merging could improve the topic modeling results across 7 languages.,"First, we observe a large variation of percentages of merged tokens across corpora.",False,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,10,59,random,Link,No Link,"To evaluate the influence of these large collocation training corpora, it is also suggested to compute the collocation measures for all bigrams on each document collection (i.e., without any external corpora).","For our evaluation, we use a normalized log likelihood approach to capture fit while accounting for changes in vocabulary (Schofield and Mimno, 2016).",False,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,10,68,mutual,Link,Link,"To evaluate the influence of these large collocation training corpora, it is also suggested to compute the collocation measures for all bigrams on each document collection (i.e., without any external corpora).",We first compute the collocation measures for all bigrams on a large collocation training corpus.,True,True
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,10,71,random,Link,Link,"To evaluate the influence of these large collocation training corpora, it is also suggested to compute the collocation measures for all bigrams on each document collection (i.e., without any external corpora).",We find all of the bigrams in the LDA training data that are also found in the top bigram lexicons that we obtain from the collocation training corpus.,True,True
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,13,40,random,No Link,Link,Are the results of other languages consistent with these three languages?,"On the other hand, analytic languages such as Thai and Chinese convey meanings by relating many words together, and morphological devices are more rarely used.",False,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,13,83,random,No Link,Link,Are the results of other languages consistent with these three languages?,"For English, we lemmatize and lowercase the data.",False,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,13,94,retriever,Link,Link,Are the results of other languages consistent with these three languages?,"Consistent with the normalized log-likelihood results, Japanese and Korean corpora interact well with all three types of retokenization, suggesting that the morphology or typology of these two languages consistently benefit from collocation before training LDA models.",True,True
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,13,95,llm,No Link,No Link,Are the results of other languages consistent with these three languages?,What could account for this discrepancy across languages and corpora?,True,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,15,83,random,No Link,No Link,There are limited reviews on collocations and topic models in recent years.,"For English, we lemmatize and lowercase the data.",True,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,15,91,random,No Link,No Link,There are limited reviews on collocations and topic models in recent years.,"Shaded cells mean that the results are inferior to the baseline, while bolded cells show the best results for each corpus and number of topics.",True,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,16,40,random,No Link,No Link,"One of the relevant early studies was cited, i.e., (Lau et al., 2013), in which, Lau et al. compared topic models learned from unigram bag-of-words data, with topic models learned from bag-of-words data that includes preextracted bigram collocations.","On the other hand, analytic languages such as Thai and Chinese convey meanings by relating many words together, and morphological devices are more rarely used.",True,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,16,43,random,No Link,No Link,"One of the relevant early studies was cited, i.e., (Lau et al., 2013), in which, Lau et al. compared topic models learned from unigram bag-of-words data, with topic models learned from bag-of-words data that includes preextracted bigram collocations.","We evaluate three measures to determine when to merge multiple adjacent words into conceptuallyunified phrasal tokens prior to LDA model training: chi-squared statistics, t-statistics, and raw frequency counts of phrases.",True,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,16,53,llm,No Link,No Link,"One of the relevant early studies was cited, i.e., (Lau et al., 2013), in which, Lau et al. compared topic models learned from unigram bag-of-words data, with topic models learned from bag-of-words data that includes preextracted bigram collocations.","Many works recognize that LDA results can be improved when input are including phrases (Lindsey et al., 2012;Lau et al., 2013;Yu et al., 2013;El-Kishky et al., 2014;Wang et al., 2016;Bin et al., 2018;Li et al., 2018).",True,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,16,62,retriever,No Link,Link,"One of the relevant early studies was cited, i.e., (Lau et al., 2013), in which, Lau et al. compared topic models learned from unigram bag-of-words data, with topic models learned from bag-of-words data that includes preextracted bigram collocations.","We hypothesize that the introduction of multi-word tokens, which capture collocations as bigrams or trigrams by way of concatenation of adjacent tokens, can help achieve more useful and coherent topic models.",False,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,17,11,random,No Link,No Link,"As shown in (Lau et al., 2013), they considered four different bigram replacement methods.",This 332 suggests that t and frequency-based retokenization 333 might be a more reliable method for improving the 334 goodness of fit of the LDA model.,True,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,17,31,random,No Link,No Link,"As shown in (Lau et al., 2013), they considered four different bigram replacement methods.","Unfortunately, the context in which these tokens arise can be obscured in the bag-of-words rendering of text as unigram counts in documents.",True,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,17,53,llm,Link,No Link,"As shown in (Lau et al., 2013), they considered four different bigram replacement methods.","Many works recognize that LDA results can be improved when input are including phrases (Lindsey et al., 2012;Lau et al., 2013;Yu et al., 2013;El-Kishky et al., 2014;Wang et al., 2016;Bin et al., 2018;Li et al., 2018).",False,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,17,97,retriever,No Link,Link,"As shown in (Lau et al., 2013), they considered four different bigram replacement methods.","Because we fix the number of bigrams types to merge during the tokenizer training process to 50,000 for all three criteria (Table 1), we can use this analysis to find trends in the relative frequency of merged tokens.",False,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,18,45,random,Link,No Link,"Particularly, they first extracted bigrams for each document collection using the N-gram Statistics Package, identifying the top bigrams based on the Student’s t-test.",This set of languages are drawn from various writing systems and different morphological typology to see which type of language favors which type of merging strategy.,False,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,18,58,random,No Link,No Link,"Particularly, they first extracted bigrams for each document collection using the N-gram Statistics Package, identifying the top bigrams based on the Student’s t-test.","A limitation of these existing approaches, however, is that they expect the vocabulary and tokenization to remain constant between two models.",True,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,19,69,llm,Link,Link,"Then, they used the top 1k, 10k, and 100k as the three different bigram replacement methods.",Then we select the top bigrams that score the highest on the collocation measures and add those to our lexicon.,True,True
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,19,84,random,No Link,No Link,"Then, they used the top 1k, 10k, and 100k as the three different bigram replacement methods.","For Korean, Japanese, and Arabic, we lemmatize the data.",True,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,19,97,retriever,No Link,Link,"Then, they used the top 1k, 10k, and 100k as the three different bigram replacement methods.","Because we fix the number of bigrams types to merge during the tokenizer training process to 50,000 for all three criteria (Table 1), we can use this analysis to find trends in the relative frequency of merged tokens.",False,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,19,99,random,No Link,No Link,"Then, they used the top 1k, 10k, and 100k as the three different bigram replacement methods.",This could be because the news corpora are in a similar domain to that of,True,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,20,9,random,No Link,No Link,"Unfortunately, the above method was not employed as the baseline in this study, in order to validate the effectiveness of using large collocation training corpora to compute the collocation measures for all bigrams.","However, we observe mixed results from 329 χ 2 retokenization for some languages.",True,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,20,64,random,No Link,No Link,"Unfortunately, the above method was not employed as the baseline in this study, in order to validate the effectiveness of using large collocation training corpora to compute the collocation measures for all bigrams.",Merging adjacent words into 'multi-word' tokens may help remedy the potential problem of a segmentation that is not optimal for topic modeling purposes.,True,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,20,68,mutual,Link,Link,"Unfortunately, the above method was not employed as the baseline in this study, in order to validate the effectiveness of using large collocation training corpora to compute the collocation measures for all bigrams.",We first compute the collocation measures for all bigrams on a large collocation training corpus.,True,True
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,20,71,mutual,Link,Link,"Unfortunately, the above method was not employed as the baseline in this study, in order to validate the effectiveness of using large collocation training corpora to compute the collocation measures for all bigrams.",We find all of the bigrams in the LDA training data that are also found in the top bigram lexicons that we obtain from the collocation training corpus.,True,True
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,20,81,mutual,Link,Link,"Unfortunately, the above method was not employed as the baseline in this study, in order to validate the effectiveness of using large collocation training corpora to compute the collocation measures for all bigrams.",The training corpora and their collocation versions are prepared based on the tokenizers that we discuss above.,True,True
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,23,53,mutual,Link,Link,"2. The reference (El-Kishky et al., 2014) had been published at Proc. VLDB Endow. 8(3): 305-316 (2014).","Many works recognize that LDA results can be improved when input are including phrases (Lindsey et al., 2012;Lau et al., 2013;Yu et al., 2013;El-Kishky et al., 2014;Wang et al., 2016;Bin et al., 2018;Li et al., 2018).",True,True
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,23,93,random,No Link,No Link,"2. The reference (El-Kishky et al., 2014) had been published at Proc. VLDB Endow. 8(3): 305-316 (2014).",This suggests that the choice of retokenization strategy might depend on the language types or the content of corpora itself.,True,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,23,97,random,No Link,No Link,"2. The reference (El-Kishky et al., 2014) had been published at Proc. VLDB Endow. 8(3): 305-316 (2014).","Because we fix the number of bigrams types to merge during the tokenizer training process to 50,000 for all three criteria (Table 1), we can use this analysis to find trends in the relative frequency of merged tokens.",True,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,24,40,random,No Link,No Link,"Besides, the information of reference (Merity et al., 2016) are incomplete.","On the other hand, analytic languages such as Thai and Chinese convey meanings by relating many words together, and morphological devices are more rarely used.",True,False
ec7b1acd2cc7faf054d68f6a564a1b4888a860c6e9d66cbf40dd556e0aabdec0d7719bf6bf1317e60297ab85fc12d59daa91bd49aa13fa665f4568a4ccd5a475,24,67,random,No Link,No Link,"Besides, the information of reference (Merity et al., 2016) are incomplete.","The chi-squared measure χ 2 (w 1 , w 2 ) and t(w 1 , w 2 ) t-statistic for two adjacent tokens w 1 and w 2 are defined as: χ 2 (w 1 , w 2 ) = (P (w 1 , w 2 ) − P (w 1 )P (w 2 )) 2 P (w 1 )P (w 2 ) (1)",True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,1,4,mutual,Link,Link,This paper proposes to use the synonyms of the ICD codes to enrich the code representations.,"Unlike these works that model the label with the code hierarchy or description, we argue that the code synonyms can provide more comprehensive knowledge based on the observation that the code expressions in EMRs vary from their descriptions in ICD.",True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,1,5,mutual,Link,Link,This paper proposes to use the synonyms of the ICD codes to enrich the code representations.,"By aligning codes to concepts in UMLS, we collect synonyms of every code in ICD.",True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,1,28,random,No Link,Link,This paper proposes to use the synonyms of the ICD codes to enrich the code representations.,Approach,False,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,1,74,random,No Link,Link,This paper proposes to use the synonyms of the ICD codes to enrich the code representations.,"HyperCore (Cao et al., 2020) embeds ICD codes into the hyperbolic space to utilize code hierarchy and uses GCN to leverage the code co-occurrence.",False,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,1,97,mutual,Link,No Link,This paper proposes to use the synonyms of the ICD codes to enrich the code representations.,"In this paper, we propose MSMN to leverage code synonyms from UMLS to improve the automatic ICD coding.",False,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,2,6,mutual,Link,Link,A multiple synonyms matching network (MSMN) is proposed to encode multiple synonyms and improve the task of automatic ICD coding.,"Then, we propose a multiple synonyms matching network to leverage synonyms for better code representation learning, and finally help the code classification.",True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,2,21,mutual,Link,Link,A multiple synonyms matching network (MSMN) is proposed to encode multiple synonyms and improve the task of automatic ICD coding.,"To model the synonym and its matching to EMR text, we further propose a Multiple Synonyms Matching Network (MSMN).",True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,2,80,random,No Link,No Link,A multiple synonyms matching network (MSMN) is proposed to encode multiple synonyms and improve the task of automatic ICD coding.,We notice that the macro F 1 has large variance in MIMIC-III full setting because it is more sensitive in a long tail problem.,True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,2,94,random,Link,Link,A multiple synonyms matching network (MSMN) is proposed to encode multiple synonyms and improve the task of automatic ICD coding.,Automatic ICD coding is an important task in the medical NLP community.,True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,2,97,mutual,Link,Link,A multiple synonyms matching network (MSMN) is proposed to encode multiple synonyms and improve the task of automatic ICD coding.,"In this paper, we propose MSMN to leverage code synonyms from UMLS to improve the automatic ICD coding.",True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,3,7,mutual,Link,No Link,"Experimental results show that the proposed method improves the performance over the baseline methods, especially when evaluated using AUC.",Experiments on two settings of the MIMIC-III dataset show that our proposed method outperforms previous stateof-the-art methods.,False,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,3,10,random,No Link,No Link,"Experimental results show that the proposed method improves the performance over the baseline methods, especially when evaluated using AUC.","The task of ICD coding refers to assigning ICD codes to electronic medical records (EMRs) which is highly related to clinical tasks or systems including patient similarity learning (Suo et al., 2018), medical billing (Sonabend et al., 2020), and clinical decision support systems (Sutton et al., 2020).",True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,3,21,random,No Link,No Link,"Experimental results show that the proposed method improves the performance over the baseline methods, especially when evaluated using AUC.","To model the synonym and its matching to EMR text, we further propose a Multiple Synonyms Matching Network (MSMN).",True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,3,26,mutual,Link,Link,"Experimental results show that the proposed method improves the performance over the baseline methods, especially when evaluated using AUC.",Results show that our method performs better than previous state-of-the-art methods.,True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,3,84,retriever,Link,Link,"Experimental results show that the proposed method improves the performance over the baseline methods, especially when evaluated using AUC.","Using m = 4, 8 achieves the best performances in AUC, and m = 8 achieves the best performances in terms of F 1 and P@5.",True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,3,100,llm,Link,Link,"Experimental results show that the proposed method improves the performance over the baseline methods, especially when evaluated using AUC.",Experiments show that MSMN outperforms previous methods with label attention and achieves state-ofthe-art results in the MIMIC-III dataset.,True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,4,0,retriever,Link,Link,The idea of using synonyms of the ICD codes is simple and effective.,Code Synonyms Do Matter: Multiple Synonyms Matching Network for Automatic ICD Coding,True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,4,4,mutual,Link,No Link,The idea of using synonyms of the ICD codes is simple and effective.,"Unlike these works that model the label with the code hierarchy or description, we argue that the code synonyms can provide more comprehensive knowledge based on the observation that the code expressions in EMRs vary from their descriptions in ICD.",False,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,4,5,retriever,Link,Link,The idea of using synonyms of the ICD codes is simple and effective.,"By aligning codes to concepts in UMLS, we collect synonyms of every code in ICD.",True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,4,16,llm,Link,Link,The idea of using synonyms of the ICD codes is simple and effective.,"In this work, we argue that the synonyms of codes can provide more comprehensive information.",True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,4,40,random,No Link,No Link,The idea of using synonyms of the ICD codes is simple and effective.,"For code synonym l j , we apply the same encoder with a max-pooling layer to obtain representation q j ∈ R h .",True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,4,59,random,No Link,No Link,The idea of using synonyms of the ICD codes is simple and effective.,Training,True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,4,97,llm,Link,Link,The idea of using synonyms of the ICD codes is simple and effective.,"In this paper, we propose MSMN to leverage code synonyms from UMLS to improve the automatic ICD coding.",True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,5,70,random,No Link,No Link,The paper is well-written and concise.,Baselines,True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,5,72,random,No Link,No Link,The paper is well-written and concise.,"MSATT-KG (Xie et al., 2019) applies multi-scale attention and GCN to capture codes relations.",True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,6,7,mutual,Link,No Link,The experimental results show promising improvement over previous work.,Experiments on two settings of the MIMIC-III dataset show that our proposed method outperforms previous stateof-the-art methods.,False,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,6,26,mutual,Link,Link,The experimental results show promising improvement over previous work.,Results show that our method performs better than previous state-of-the-art methods.,True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,6,50,random,No Link,No Link,The experimental results show promising improvement over previous work.,We also split hidden representations into different heads for multi-synonyms attention.,True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,6,73,random,No Link,No Link,The experimental results show promising improvement over previous work.,"MultiResCNN (Li and Yu, 2020) encodes text using multi-filter residual CNN.",True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,6,83,retriever,Link,Link,The experimental results show promising improvement over previous work.,Results are shown in aging more synonyms from UMLS consistently improves the performance.,True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,6,100,llm,Link,Link,The experimental results show promising improvement over previous work.,Experiments show that MSMN outperforms previous methods with label attention and achieves state-ofthe-art results in the MIMIC-III dataset.,True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,7,63,random,No Link,No Link,I like that the authors include discussion of varying m and different scoring functions in the paper to make it more complete.,Detailed statistics of the MIMIC-III dataset are listed in Appendix A.,True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,7,81,llm,No Link,Link,I like that the authors include discussion of varying m and different scoring functions in the paper to make it more complete.,Discussion,False,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,7,84,retriever,Link,No Link,I like that the authors include discussion of varying m and different scoring functions in the paper to make it more complete.,"Using m = 4, 8 achieves the best performances in AUC, and m = 8 achieves the best performances in terms of F 1 and P@5.",False,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,7,86,llm,No Link,No Link,I like that the authors include discussion of varying m and different scoring functions in the paper to make it more complete.,"To evaluate the effectiveness of our proposed biaffine-based similarity function, we compare it with the baseline LAAT in Table 3.",True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,7,88,retriever,No Link,Link,I like that the authors include discussion of varying m and different scoring functions in the paper to make it more complete.,Results show the biaffine-based similarity scoring performs best among others.,False,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,7,99,random,No Link,No Link,I like that the authors include discussion of varying m and different scoring functions in the paper to make it more complete.,We also propose a biaffine transformation to calculate similarities among texts and codes for classification.,True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,8,12,retriever,Link,Link,"Overall, I think this paper introduces a simple but effective method to improve automatic ICD coding and conducts experiments to verify its effectiveness.","As a result, many methods have been proposed for automatic ICD coding since the 1990s (de Lima et al., 1998).",True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,8,26,llm,Link,Link,"Overall, I think this paper introduces a simple but effective method to improve automatic ICD coding and conducts experiments to verify its effectiveness.",Results show that our method performs better than previous state-of-the-art methods.,True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,8,54,random,No Link,No Link,"Overall, I think this paper introduces a simple but effective method to improve automatic ICD coding and conducts experiments to verify its effectiveness.",We aggregate code synonym representations {q j } to code representation q l ∈ R h by max-pooling.,True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,8,94,random,No Link,Link,"Overall, I think this paper introduces a simple but effective method to improve automatic ICD coding and conducts experiments to verify its effectiveness.",Automatic ICD coding is an important task in the medical NLP community.,False,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,8,97,mutual,Link,Link,"Overall, I think this paper introduces a simple but effective method to improve automatic ICD coding and conducts experiments to verify its effectiveness.","In this paper, we propose MSMN to leverage code synonyms from UMLS to improve the automatic ICD coding.",True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,9,77,random,No Link,No Link,I think the material presented is well-suited for a short paper.,"Table 1 and 2 show the main results under the MIMIC-III full and MIMIC-III 50 settings, respectively.",True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,9,87,random,No Link,No Link,I think the material presented is well-suited for a short paper.,We also provide a simple function by removing W to v T l q l in Equation 7.,True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,11,0,retriever,Link,Link,- The idea of using synonyms of the ICD codes is simple.,Code Synonyms Do Matter: Multiple Synonyms Matching Network for Automatic ICD Coding,True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,11,4,mutual,Link,No Link,- The idea of using synonyms of the ICD codes is simple.,"Unlike these works that model the label with the code hierarchy or description, we argue that the code synonyms can provide more comprehensive knowledge based on the observation that the code expressions in EMRs vary from their descriptions in ICD.",False,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,11,5,retriever,Link,Link,- The idea of using synonyms of the ICD codes is simple.,"By aligning codes to concepts in UMLS, we collect synonyms of every code in ICD.",True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,11,16,llm,Link,No Link,- The idea of using synonyms of the ICD codes is simple.,"In this work, we argue that the synonyms of codes can provide more comprehensive information.",False,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,11,22,random,Link,No Link,- The idea of using synonyms of the ICD codes is simple.,"Specifically, we first apply a shared LSTM to encode EMR texts and each synonym.",False,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,11,68,random,No Link,No Link,- The idea of using synonyms of the ICD codes is simple.,"We train MSMN with AdamW (Loshchilov and Hutter, 2019) with a linear learning rate decay.",True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,11,97,llm,Link,Link,- The idea of using synonyms of the ICD codes is simple.,"In this paper, we propose MSMN to leverage code synonyms from UMLS to improve the automatic ICD coding.",True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,12,26,random,No Link,No Link,- The paper is well-written and concise.,Results show that our method performs better than previous state-of-the-art methods.,True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,12,47,random,No Link,No Link,- The paper is well-written and concise.,We aggregate code-wise text representations v l ∈,True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,13,7,mutual,Link,No Link,- The experimental results show promising improvement over previous work.,Experiments on two settings of the MIMIC-III dataset show that our proposed method outperforms previous stateof-the-art methods.,False,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,13,26,mutual,Link,Link,- The experimental results show promising improvement over previous work.,Results show that our method performs better than previous state-of-the-art methods.,True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,13,35,random,No Link,No Link,- The experimental results show promising improvement over previous work.,"We denote the code synonyms as {l 2 , ..., l m } in which each code synonym l j is composed of words {l j i }",True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,13,65,random,No Link,No Link,- The experimental results show promising improvement over previous work.,We sample m = 4 and 8 synonyms per code for MIMIC-III full and MIMIC-III 50 respectively.,True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,13,83,retriever,Link,Link,- The experimental results show promising improvement over previous work.,Results are shown in aging more synonyms from UMLS consistently improves the performance.,True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,13,100,llm,Link,Link,- The experimental results show promising improvement over previous work.,Experiments show that MSMN outperforms previous methods with label attention and achieves state-ofthe-art results in the MIMIC-III dataset.,True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,15,8,random,No Link,No Link,"I have some questions regarding the results, see Questions below",Introduction,True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,15,76,retriever,No Link,No Link,"I have some questions regarding the results, see Questions below",Main Results,True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,15,81,llm,No Link,No Link,"I have some questions regarding the results, see Questions below",Discussion,True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,15,87,random,No Link,No Link,"I have some questions regarding the results, see Questions below",We also provide a simple function by removing W to v T l q l in Equation 7.,True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,17,7,retriever,Link,No Link,- The proposed method obtains larger gains on the top-50 setting.,Experiments on two settings of the MIMIC-III dataset show that our proposed method outperforms previous stateof-the-art methods.,False,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,17,25,mutual,Link,Link,- The proposed method obtains larger gains on the top-50 setting.,We conduct experiments on the MIMIC-III dataset with two settings: full codes and top-50 codes.,True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,17,77,mutual,Link,Link,- The proposed method obtains larger gains on the top-50 setting.,"Table 1 and 2 show the main results under the MIMIC-III full and MIMIC-III 50 settings, respectively.",True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,17,79,llm,Link,Link,- The proposed method obtains larger gains on the top-50 setting.,"Under the top-50 codes setting, MSMN performs better than LAAT in all metrics and achieves state-of-the-art scores of 92.8 (+0.3), 94.7 (+0.1), 68.3 (+1.7), 72.5 (+0.9), 68.0 (+0.5) on macro-AUC, micro-AUC, macro-F 1 , micro-F 1 , and P@5, respectively.",True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,17,89,random,Link,Link,- The proposed method obtains larger gains on the top-50 setting.,"To better understand what MSMN learns from the multi-synonyms attention, we plot the synonym representations q j under MIMIC-III 50 setting via t-SNE (van der Maaten and Hinton, 2008) in Figure 2.",True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,17,101,random,No Link,No Link,- The proposed method obtains larger gains on the top-50 setting.,Ablation studies show the effectiveness of multi-synonyms attention and biaffine-based similarity.,True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,18,33,random,No Link,No Link,My intuition was that the top codes would be benefited less since we have enough training data for those codes.,We first align the code to the Concept Unique Identifiers (CUIs) from UMLS.,True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,18,83,random,No Link,No Link,My intuition was that the top codes would be benefited less since we have enough training data for those codes.,Results are shown in aging more synonyms from UMLS consistently improves the performance.,True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,19,49,random,No Link,No Link,Why do you think this happens?,Different colors indicate different code synonyms.,True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,19,80,llm,No Link,No Link,Why do you think this happens?,We notice that the macro F 1 has large variance in MIMIC-III full setting because it is more sensitive in a long tail problem.,True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,19,81,mutual,No Link,Link,Why do you think this happens?,Discussion,False,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,19,90,random,No Link,Link,Why do you think this happens?,"We observe for some codes like 585.9 (""chronic kidney diseases""), all synonym representations cluster together, which indicates that synonyms extract similar text snippets.",False,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,19,96,retriever,No Link,No Link,Why do you think this happens?,Conclusions,True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,20,10,random,No Link,No Link,"- For the full setting, AUC and precision improved a lot, but not F1.","The task of ICD coding refers to assigning ICD codes to electronic medical records (EMRs) which is highly related to clinical tasks or systems including patient similarity learning (Suo et al., 2018), medical billing (Sonabend et al., 2020), and clinical decision support systems (Sutton et al., 2020).",True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,20,62,mutual,Link,Link,"- For the full setting, AUC and precision improved a lot, but not F1.","We measure the results using macro AUC, micro AUC, macro F 1 , micro F 1 and precision@k (k = 5 for MIMIC-III 50, 8 and 15 for MIMIC-III full).",True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,20,78,llm,No Link,Link,"- For the full setting, AUC and precision improved a lot, but not F1.","Under the full setting, our MSMN achieves 95.0 (+2.0), 99.2 (+0.0), 10.3 (-0.4), 58.4 (+0.9), 75.2 (+1.4), and 59.9 (+0.8) in terms of macro-AUC, micro-AUC, macro-F 1 , micro-F 1 , P@8, and P@15 respectively (parentheses shows the differences against previous best results), which shows that MSMN obtains state-of-the-art results in most metrics.",False,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,20,84,retriever,Link,Link,"- For the full setting, AUC and precision improved a lot, but not F1.","Using m = 4, 8 achieves the best performances in AUC, and m = 8 achieves the best performances in terms of F 1 and P@5.",True,True
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,20,99,random,No Link,No Link,"- For the full setting, AUC and precision improved a lot, but not F1.",We also propose a biaffine transformation to calculate similarities among texts and codes for classification.,True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,21,29,random,No Link,No Link,Do you have any observation or explanation for this?,Consider free text S (usually discharge summaries) from EMR with words {w i } N i=1 .,True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,21,80,llm,No Link,No Link,Do you have any observation or explanation for this?,We notice that the macro F 1 has large variance in MIMIC-III full setting because it is more sensitive in a long tail problem.,True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,21,81,mutual,No Link,Link,Do you have any observation or explanation for this?,Discussion,False,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,21,96,retriever,No Link,No Link,Do you have any observation or explanation for this?,Conclusions,True,False
32b27150ea037d50af8423d3ec43027c172eb702a5265660a68988ee861d25608339af371f529ee04489ef987a6152286d407c848caaf725ac9f36ed5deed223,21,98,random,No Link,No Link,Do you have any observation or explanation for this?,Multi-synonyms attention is proposed for extracting different related text snippets for code-wise text representations.,True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,1,3,mutual,Link,Link,This submission proposed a model for offensive span detection (OSD).,"While the majority of the prior work formulate this problem as text classification, i.e., if a text excerpt is offensive or not, in this work we propose a novel model for offensive span detection (OSD), whose goal is to identify the spans responsible for the offensive tone of the text.",True,True
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,1,15,mutual,Link,Link,This submission proposed a model for offensive span detection (OSD).,"As such, in this work, we fill this gap by proposing a novel model for the task of offensive span detection (OSD).",True,True
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,1,24,retriever,Link,Link,This submission proposed a model for offensive span detection (OSD).,We evaluate the proposed model on a recently released dataset for offensive span detection.,True,True
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,1,29,random,Link,No Link,This submission proposed a model for offensive span detection (OSD).,"This problem is modeled as a sequence labeling task in which the model predicts the label of every word w i in the document D. In this work, we propose a method to augment the original training samples O, with synthetic labeled text G generated by a fine-tuned GPT-2 model.",False,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,1,76,random,No Link,No Link,This submission proposed a model for offensive span detection (OSD).,"Also, this improvement proves that the generated sentences are in-domain and task specific, as such resulting in an improvement.",True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,1,99,llm,Link,Link,This submission proposed a model for offensive span detection (OSD).,"In this work, we propose a novel method for augmenting data for offensive span detection tasks.",True,True
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,2,5,mutual,Link,Link,"Specifically, the authors train GPT-2 in a dual-training setting with reinforcing learning to generate synthetic training data for OSD.","To address this limitation, in this work we propose a novel method in which the large-scale pre-trained language model GPT-2 is employed to generate synthetic training data for OSD.",True,True
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,2,6,mutual,Link,Link,"Specifically, the authors train GPT-2 in a dual-training setting with reinforcing learning to generate synthetic training data for OSD.","In particular, we propose to train the GPT-2 model in a dual-training setting using the REINFORCE algorithm to generate in-domain, natural and diverse training samples.",True,True
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,2,25,random,No Link,No Link,"Specifically, the authors train GPT-2 in a dual-training setting with reinforcing learning to generate synthetic training data for OSD.",Our extensive experiments show the effectiveness of the proposed model by outperforming the strong baselines.,True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,2,58,random,No Link,No Link,"Specifically, the authors train GPT-2 in a dual-training setting with reinforcing learning to generate synthetic training data for OSD.",Experiments,True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,2,102,mutual,Link,Link,"Specifically, the authors train GPT-2 in a dual-training setting with reinforcing learning to generate synthetic training data for OSD.","Moreover, to improve the quality of the generated documents, we propose a novel dual training setting in which the feedback from the OSD model is employed to guide the GPT-2 model to generate more impact synthetic data.",True,True
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,4,9,random,No Link,No Link,- Good writing and well organized,It's no secret that social networks are growing in popularity.,True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,4,20,random,No Link,No Link,- Good writing and well organized,"In particular, the original labeled samples of OSD, with special markers before and after each offensive span, are employed to fine-tune the parameters of the GPT-2 model to generate sentences containing offensive spans.",True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,5,13,llm,Link,No Link,- Interesting topic of offensive span detection,"Since this is an important requirement, the task of offensive language detection has been extensively studied in NLP community (Schmidt and Wiegand, 2017;Wulczyn et al., 2017;Feng et al., 2018;Borkan et al., 2019;Pavlopoulos et al., 2019;Sivanaiah et al., 2020;Yasaswini et al., 2021) Most existing works, however, only classify a text snippet as offensive or not, failing to provide further information on which specific words and phrases in the text contribute the most to its offensive tone.",False,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,5,15,random,Link,No Link,- Interesting topic of offensive span detection,"As such, in this work, we fill this gap by proposing a novel model for the task of offensive span detection (OSD).",False,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,5,16,llm,No Link,No Link,- Interesting topic of offensive span detection,"As an example, in the given text ""This live streamer clearly has no brain; he is such a tool!"", the phrase ""has no brain"" and the slang word ""tool"" are two offensive spans responsible for the toxicity of the text.",True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,5,24,retriever,Link,No Link,- Interesting topic of offensive span detection,We evaluate the proposed model on a recently released dataset for offensive span detection.,False,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,5,62,random,Link,No Link,- Interesting topic of offensive span detection,"For each document, the word indices of offensive spans are provided.",False,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,5,99,retriever,Link,No Link,- Interesting topic of offensive span detection,"In this work, we propose a novel method for augmenting data for offensive span detection tasks.",False,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,7,5,mutual,Link,Link,- Limited novelty: simple application via utilizing GPT-2 to generate data of a specific domain,"To address this limitation, in this work we propose a novel method in which the large-scale pre-trained language model GPT-2 is employed to generate synthetic training data for OSD.",True,True
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,7,18,llm,No Link,No Link,- Limited novelty: simple application via utilizing GPT-2 to generate data of a specific domain,"Inspired by the recent advances in the application of pre-trained language models to augment training data for low-resources tasks (Zhang et al., 2020;Yang et al., 2020;Peng et al., 2020;Kumar et al., 2020;Anaby-Tavor et al., 2020), we propose to employ the GPT-2 model to overcome the data scarcity of OSD.",True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,7,25,random,No Link,No Link,- Limited novelty: simple application via utilizing GPT-2 to generate data of a specific domain,Our extensive experiments show the effectiveness of the proposed model by outperforming the strong baselines.,True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,7,40,mutual,Link,No Link,- Limited novelty: simple application via utilizing GPT-2 to generate data of a specific domain,"To address this limitation, inspired by the success of the generative language models to augment data for other tasks, we propose to employ GPT-2 to generate labeled synthetic data.",False,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,7,44,retriever,Link,No Link,- Limited novelty: simple application via utilizing GPT-2 to generate data of a specific domain,"In order to fine-tune GPT-2 for generating labeled data for OSD, we propose to employ the original labeled data G. Specifically, the document D ∈ G is first augmented with special tokens at the beginning and the end of the document and also around the offensive spans:",False,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,7,62,random,No Link,No Link,- Limited novelty: simple application via utilizing GPT-2 to generate data of a specific domain,"For each document, the word indices of offensive spans are provided.",True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,8,62,random,No Link,No Link,- Insufficient evaluation: - only one dataset is used - lack of detailed dataset statistics - the comparison is not persuasive,"For each document, the word indices of offensive spans are provided.",True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,8,77,random,No Link,No Link,- Insufficient evaluation: - only one dataset is used - lack of detailed dataset statistics - the comparison is not persuasive,"The better performance of our model is impressive, especially considering that we use relatively simple base model compared to other baselines (in particular HITSZ-HLT which is an ensemble model).",True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,10,3,retriever,Link,Link,"Although OSD is an interesting topic, the submission should address the following issues before it can be published:","While the majority of the prior work formulate this problem as text classification, i.e., if a text excerpt is offensive or not, in this work we propose a novel model for offensive span detection (OSD), whose goal is to identify the spans responsible for the offensive tone of the text.",True,True
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,10,13,llm,Link,No Link,"Although OSD is an interesting topic, the submission should address the following issues before it can be published:","Since this is an important requirement, the task of offensive language detection has been extensively studied in NLP community (Schmidt and Wiegand, 2017;Wulczyn et al., 2017;Feng et al., 2018;Borkan et al., 2019;Pavlopoulos et al., 2019;Sivanaiah et al., 2020;Yasaswini et al., 2021) Most existing works, however, only classify a text snippet as offensive or not, failing to provide further information on which specific words and phrases in the text contribute the most to its offensive tone.",False,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,10,25,random,Link,No Link,"Although OSD is an interesting topic, the submission should address the following issues before it can be published:",Our extensive experiments show the effectiveness of the proposed model by outperforming the strong baselines.,False,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,10,35,random,No Link,No Link,"Although OSD is an interesting topic, the submission should address the following issues before it can be published:","Next, the representations x i are sent to a feed-forward network to predict the label distribution P (•|D, θ), where θ is the parameters of the BERT model.",True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,11,50,random,No Link,No Link,"- Only one dataset is used for evaluation, is there anything more?","Specifically, the model is prompted with [BOS] token and the generation is stopped by generating the [EOS] token.",True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,11,86,random,No Link,No Link,"- Only one dataset is used for evaluation, is there anything more?","Specifically, the dual training has the largest effect on the final performance, indicating the importance of the proposed method.",True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,12,40,mutual,Link,Link,- Is it possible to add more evaluations about the synthetic data itself?,"To address this limitation, inspired by the success of the generative language models to augment data for other tasks, we propose to employ GPT-2 to generate labeled synthetic data.",True,True
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,12,41,random,No Link,No Link,- Is it possible to add more evaluations about the synthetic data itself?,"We first discuss the generation process, then we provide details on how the generative model is encouraged to generate high-quality data.",True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,12,49,mutual,No Link,Link,- Is it possible to add more evaluations about the synthetic data itself?,"Finally, the fine-tuned GPT-2 model is employed to generate |O| synthetic data.",False,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,12,63,random,No Link,No Link,- Is it possible to add more evaluations about the synthetic data itself?,"In our experiments, we create the BIO labels (2) BERT+CRF: BERT base parameters are finetuned on OSD task and the task-specific head, i.e., CRF, is employed for label prediction; (3) HITSZ-HLT (Zhu et al., 2021): This baseline is the existing SOTA model on SemEval 2021 Task 5 dataset; (4) SANER (Nie et al., 2020): This baseline is the SOTA model for sequence labeling on usergenerated text; (5) DUAL-MRC (Mao et al., 2021): This is the SOTA model for opinion and aspect term extraction.",True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,12,102,mutual,No Link,Link,- Is it possible to add more evaluations about the synthetic data itself?,"Moreover, to improve the quality of the generated documents, we propose a novel dual training setting in which the feedback from the OSD model is employed to guide the GPT-2 model to generate more impact synthetic data.",False,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,13,48,random,No Link,No Link,"In the current paper, the comparison is between the whole pipeline and other baselines.","where w j is the j-th word in the label augmented document D i , D <j is the left context of the word w j in the document D i , and α is the parameters of the GPT-2 model.",True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,13,95,random,No Link,No Link,"In the current paper, the comparison is between the whole pipeline and other baselines.",The main limitation of these works is that they cannot recognize the spans in the text that are responsible for the toxicity of the text.,True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,14,79,random,No Link,No Link,"Not sure how much performance is made by the data itself, or the training mode itself?","Specifically, we ablate the quality improvement component which ensures the usefulness and diversity of the generated samples.",True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,14,81,retriever,No Link,Link,"Not sure how much performance is made by the data itself, or the training mode itself?","Also, we study the performance of the model when no dual training is employed (DT − ).",False,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,14,86,llm,No Link,Link,"Not sure how much performance is made by the data itself, or the training mode itself?","Specifically, the dual training has the largest effect on the final performance, indicating the importance of the proposed method.",False,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,14,97,random,No Link,No Link,"Not sure how much performance is made by the data itself, or the training mode itself?","The major limitation of all these models is that they require the existence of the target opinion (i.e., the word or phrase that the text has a sentiment polarity toward it).",True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,15,38,random,No Link,No Link,- Is it possible to generalize the application?,Data Augmentation,True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,15,7,random,No Link,No Link,- Is it possible to generalize the application?,Extensive experiments on the benchmark dataset for OSD reveal the effectiveness of the proposed method.,True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,16,3,retriever,No Link,Link,"For example, from the OSD to general opinion and aspect detection?","While the majority of the prior work formulate this problem as text classification, i.e., if a text excerpt is offensive or not, in this work we propose a novel model for offensive span detection (OSD), whose goal is to identify the spans responsible for the offensive tone of the text.",False,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,16,9,random,No Link,No Link,"For example, from the OSD to general opinion and aspect detection?",It's no secret that social networks are growing in popularity.,True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,16,15,retriever,Link,Link,"For example, from the OSD to general opinion and aspect detection?","As such, in this work, we fill this gap by proposing a novel model for the task of offensive span detection (OSD).",True,True
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,16,63,llm,Link,No Link,"For example, from the OSD to general opinion and aspect detection?","In our experiments, we create the BIO labels (2) BERT+CRF: BERT base parameters are finetuned on OSD task and the task-specific head, i.e., CRF, is employed for label prediction; (3) HITSZ-HLT (Zhu et al., 2021): This baseline is the existing SOTA model on SemEval 2021 Task 5 dataset; (4) SANER (Nie et al., 2020): This baseline is the SOTA model for sequence labeling on usergenerated text; (5) DUAL-MRC (Mao et al., 2021): This is the SOTA model for opinion and aspect term extraction.",False,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,16,79,random,No Link,No Link,"For example, from the OSD to general opinion and aspect detection?","Specifically, we ablate the quality improvement component which ensures the usefulness and diversity of the generated samples.",True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,16,96,llm,No Link,No Link,"For example, from the OSD to general opinion and aspect detection?","(ii) Opinion Word Extraction: In this group of prior works, models perform a sequence labeling task to identify the spans in the text that convey the sentiment (Liu et al., 2015;Xu et al., 2018;Yin et al., 2016;Wang et al., 2016Wang et al., , 2017Li and Lam, 2017;Mao et al., 2021).",True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,17,15,random,No Link,No Link,"- The method needs the manual labels as the start (the BIO labels mentioned in the experiments), is it possible to add more statistics about this label set?","As such, in this work, we fill this gap by proposing a novel model for the task of offensive span detection (OSD).",True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,17,28,mutual,Link,No Link,"- The method needs the manual labels as the start (the BIO labels mentioned in the experiments), is it possible to add more statistics about this label set?","The label provided for the document is also the sequence Y = [y 1 , y 2 , . . . , y n ] in which y i is the label for the word w i in BIO format.",False,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,17,29,retriever,No Link,No Link,"- The method needs the manual labels as the start (the BIO labels mentioned in the experiments), is it possible to add more statistics about this label set?","This problem is modeled as a sequence labeling task in which the model predicts the label of every word w i in the document D. In this work, we propose a method to augment the original training samples O, with synthetic labeled text G generated by a fine-tuned GPT-2 model.",True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,17,44,llm,No Link,No Link,"- The method needs the manual labels as the start (the BIO labels mentioned in the experiments), is it possible to add more statistics about this label set?","In order to fine-tune GPT-2 for generating labeled data for OSD, we propose to employ the original labeled data G. Specifically, the document D ∈ G is first augmented with special tokens at the beginning and the end of the document and also around the offensive spans:",True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,17,51,llm,No Link,No Link,"- The method needs the manual labels as the start (the BIO labels mentioned in the experiments), is it possible to add more statistics about this label set?","In order to ensure that the generated data are labeled, we keep only the generated samples with at least one pair of [OF F EN SIV E S ] and [OF F EN SIV E E ] tokens.",True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,17,63,retriever,Link,No Link,"- The method needs the manual labels as the start (the BIO labels mentioned in the experiments), is it possible to add more statistics about this label set?","In our experiments, we create the BIO labels (2) BERT+CRF: BERT base parameters are finetuned on OSD task and the task-specific head, i.e., CRF, is employed for label prediction; (3) HITSZ-HLT (Zhu et al., 2021): This baseline is the existing SOTA model on SemEval 2021 Task 5 dataset; (4) SANER (Nie et al., 2020): This baseline is the SOTA model for sequence labeling on usergenerated text; (5) DUAL-MRC (Mao et al., 2021): This is the SOTA model for opinion and aspect term extraction.",False,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,17,89,random,No Link,No Link,"- The method needs the manual labels as the start (the BIO labels mentioned in the experiments), is it possible to add more statistics about this label set?",The results are shown in table 3.,True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,18,4,random,No Link,No Link,- The definitions of usefulness and diversity seem quite intuitive.,One of the challenges to train a model for this novel setting is the lack of enough tanning data.,True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,18,5,random,No Link,Link,- The definitions of usefulness and diversity seem quite intuitive.,"To address this limitation, in this work we propose a novel method in which the large-scale pre-trained language model GPT-2 is employed to generate synthetic training data for OSD.",False,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,18,75,mutual,Link,Link,- The definitions of usefulness and diversity seem quite intuitive.,The increased diversity is realized by generating more diverse  sentences.,True,True
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,18,79,mutual,Link,No Link,- The definitions of usefulness and diversity seem quite intuitive.,"Specifically, we ablate the quality improvement component which ensures the usefulness and diversity of the generated samples.",False,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,18,80,mutual,Link,No Link,- The definitions of usefulness and diversity seem quite intuitive.,"In particular, we study the performance of the model when the Usefulness Reward (UR − ), the Diversity Reward (DR − ), or both of them (UDR − ) are ablated.",False,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,19,0,random,No Link,No Link,"Any motivations to justify this definition, or is there any alternative form of definition?",Data Augmentation with Dual Training for Offensive Span Detection,True,False
eb32c00a82fdc4cb6b0fa6820296035255944516c759902313dafa9e83de692dd98197f912101ad81acff63f2a9805abb7e0c83b441a48a727dbf9fb6f413a88,19,54,random,No Link,Link,"Any motivations to justify this definition, or is there any alternative form of definition?","Specifically, at the first epoch, the parameters of the GPT-2 model are updated using the loss f .",False,False
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,1,2,mutual,Link,Link,This paper investigates the aspect sentiment triplet extraction task and optimized bidirectional machine reading comprehension method with 4 improvements.,Aspect sentiment triplet extraction (ASTE) is a challenging subtask in aspect-based sentiment analysis.,True,True
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,1,4,mutual,Link,Link,This paper investigates the aspect sentiment triplet extraction task and optimized bidirectional machine reading comprehension method with 4 improvements.,"The bidirectional machine reading comprehension (BMRC) can effectively deal with ASTE task, but several problems remains, such as query conflict and probability unilateral decrease.",True,True
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,1,16,mutual,Link,Link,This paper investigates the aspect sentiment triplet extraction task and optimized bidirectional machine reading comprehension method with 4 improvements.,"Among them, aspect sentiment triplet extraction (ASTE) (Peng et al., 2020) becomes a subject of great interest, which is also the goal of our work.",True,True
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,1,47,random,No Link,No Link,This paper investigates the aspect sentiment triplet extraction task and optimized bidirectional machine reading comprehension method with 4 improvements.,"However, if they are subdivided into ""walk ##ing"", ""walk ##ed"", ""walk ##er"", and ""walk ##s"", their sub-word ""walk"" contains the same semantics which is quite common during training.",True,False
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,1,69,random,No Link,No Link,This paper investigates the aspect sentiment triplet extraction task and optimized bidirectional machine reading comprehension method with 4 improvements.,"In this way, the probability of pair decreases unilaterally and cannot well represent the prediction of the pair by the model.",True,False
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,2,10,retriever,Link,Link,The evaluation results look good against baseline models.,"We have conducted extensive experiments on multiple benchmark datasets, where our model achieves the state-of-the-art performance.",True,True
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,2,16,random,No Link,No Link,The evaluation results look good against baseline models.,"Among them, aspect sentiment triplet extraction (ASTE) (Peng et al., 2020) becomes a subject of great interest, which is also the goal of our work.",True,False
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,2,69,random,No Link,No Link,The evaluation results look good against baseline models.,"In this way, the probability of pair decreases unilaterally and cannot well represent the prediction of the pair by the model.",True,False
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,2,85,retriever,No Link,Link,The evaluation results look good against baseline models.,"In order to make a fair comparison with baselines, our F1 scores appeared at least three times in the experiments.",False,False
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,2,87,llm,Link,Link,The evaluation results look good against baseline models.,"In the Lap-top14, Rest14, Rest15, and Rest16 datasets of ASTE-Data-v1, the F1 scores of our improved model are increased by 2.97, 4.20, 5.61 and 5.52 respectively compared with the original BMRC, indicating that our improvement is very effective.",True,True
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,2,89,llm,Link,Link,The evaluation results look good against baseline models.,"the Laptop14, Rest14, Rest15, and Rest16 datasets of ASTE-Data-v2, we also increased the F1 scores of the Strong baseline Span-ASTE (Xu et al., 2021) by 2.74, 0.77, 2.36 and 2.90 respectively.",True,True
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,4,4,mutual,Link,Link,The bidirectional machine reading comprehension is suited for this task and the design of span matching and probability generation is useful(through ablation study).,"The bidirectional machine reading comprehension (BMRC) can effectively deal with ASTE task, but several problems remains, such as query conflict and probability unilateral decrease.",True,True
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,4,18,mutual,Link,Link,The bidirectional machine reading comprehension is suited for this task and the design of span matching and probability generation is useful(through ablation study).,"Many research efforts have been made (Xu et al., 2021;Mao et al., 2021;Chen et al., 2021), for example, using bidirectional machine reading comprehension (BMRC) for ASTE.",True,True
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,4,33,random,No Link,Link,The bidirectional machine reading comprehension is suited for this task and the design of span matching and probability generation is useful(through ablation study).,"In this section, we briefly review the BMRC (Chen et al., 2021), and then introduce our four improvements in detail.",False,False
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,4,53,random,No Link,No Link,The bidirectional machine reading comprehension is suited for this task and the design of span matching and probability generation is useful(through ablation study).,Another example is the aspect query in the forward query and the aspect query in the backward query.,True,False
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,4,67,retriever,Link,No Link,The bidirectional machine reading comprehension is suited for this task and the design of span matching and probability generation is useful(through ablation study).,"Once the bidirectional queries and span matching are completed, aspects, opinions and pairs with corresponding relationship are obtained.",False,False
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,4,93,llm,Link,Link,The bidirectional machine reading comprehension is suited for this task and the design of span matching and probability generation is useful(through ablation study).,"The model is a reproduction based on BMRC, and then gradually superimposes the four improvements of word segmentation, exclusive classifiers, span matching and probability generation to conduct ablation experiment.",True,True
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,5,4,random,Link,Link,The overall performance is good.,"The bidirectional machine reading comprehension (BMRC) can effectively deal with ASTE task, but several problems remains, such as query conflict and probability unilateral decrease.",True,True
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,5,11,random,No Link,No Link,The overall performance is good.,Introduction,True,False
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,5,86,mutual,Link,Link,The overall performance is good.,"It is worth noting that we have achieved state-ofthe-art performances on the ASTE-Data-v1 and ASTE-Data-v2 datasets, indicating that our improvement further improves the performance of BMRC in dealing with ASTE task.",True,True
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,5,87,mutual,Link,Link,The overall performance is good.,"In the Lap-top14, Rest14, Rest15, and Rest16 datasets of ASTE-Data-v1, the F1 scores of our improved model are increased by 2.97, 4.20, 5.61 and 5.52 respectively compared with the original BMRC, indicating that our improvement is very effective.",True,True
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,5,90,llm,Link,Link,The overall performance is good.,This indicates that our improvement is very significant.,True,True
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,5,99,retriever,Link,No Link,The overall performance is good.,"Each improvement advances the performance of the model, demonstrating their advantages and effectiveness.",False,False
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,8,4,llm,Link,Link,2. What if the forward query and backward query give conflict prediction?,"The bidirectional machine reading comprehension (BMRC) can effectively deal with ASTE task, but several problems remains, such as query conflict and probability unilateral decrease.",True,True
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,8,20,llm,Link,Link,2. What if the forward query and backward query give conflict prediction?,"In the structure of BMRC, the shared classifiers may lead to query conflicts based on specific context, thus affecting the model performance.",True,True
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,8,36,retriever,Link,Link,2. What if the forward query and backward query give conflict prediction?,"Forward Query BMRC will query all aspects based on context; Then, according to the aspect of each prediction, all opinions describing it are queried from the context.",True,True
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,8,37,retriever,No Link,Link,2. What if the forward query and backward query give conflict prediction?,"Backward Query BMRC will query all opinions based on context; Then, according to the opinion of each prediction, all aspects describing it are queried from the context.",False,False
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,8,51,retriever,Link,Link,2. What if the forward query and backward query give conflict prediction?,"For example, the aspect query in forward query is different from the opinion query in backward query.",True,True
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,8,57,llm,Link,Link,2. What if the forward query and backward query give conflict prediction?,These different types of queries will interfere with each other and cause the query conflict.,True,True
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,8,99,random,No Link,No Link,2. What if the forward query and backward query give conflict prediction?,"Each improvement advances the performance of the model, demonstrating their advantages and effectiveness.",True,False
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,8,104,random,No Link,No Link,2. What if the forward query and backward query give conflict prediction?,Extensive experiments are conducted to demonstrate the advantages of our improvements.,True,False
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,11,70,random,No Link,No Link,3. What are the differences between two versions of ASTE-Data datasets?,"For example, the probability of the four positions of pair is 0.9, while the probability of pair is 0.9 4 = 0.6561, which seems not so reasonable.",True,False
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,11,79,llm,Link,Link,3. What are the differences between two versions of ASTE-Data datasets?,"We evaluate the model performance on ASTE-Data-v1 (Peng et al., 2020) and ASTE-Data-v2 , which are popular benchmark datasets for ASTE task.",True,True
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,11,83,retriever,No Link,Link,3. What are the differences between two versions of ASTE-Data datasets?,We conducted many experiments on the ASTE-Data-v1 and ASTE-Data-v2 datasets.,False,False
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,11,104,random,No Link,No Link,3. What are the differences between two versions of ASTE-Data datasets?,Extensive experiments are conducted to demonstrate the advantages of our improvements.,True,False
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,12,56,random,No Link,No Link,"4. The evaluation metrics are not explained, is it an exact match rule: the prediction is correct only if all three elements in the triplet are correct?","However, if different types of queries use the same classifier, it cannot serve any part very well.",True,False
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,12,96,random,No Link,No Link,"4. The evaluation metrics are not explained, is it an exact match rule: the prediction is correct only if all three elements in the triplet are correct?",The datasets and various parameters of the five experiments are the same.,True,False
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,15,39,random,No Link,No Link,"The variables in formula 1 and 2 should be explained, for example, what is pair_asp.","After that, the sentiments and aspect-opinion pairs are combined into triplets.",True,False
3149648e5774b4cad7693fd8600f255328e62c395dfa804aabe286a6358502b7dac551c05fa3575496b23c7c57bfde0f43e7cd6418c5b5363a64a0c355d9c32d,15,93,random,No Link,No Link,"The variables in formula 1 and 2 should be explained, for example, what is pair_asp.","The model is a reproduction based on BMRC, and then gradually superimposes the four improvements of word segmentation, exclusive classifiers, span matching and probability generation to conduct ablation experiment.",True,False
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,1,4,llm,Link,No Link,This paper proposes add copy probability in a transformer-based model to improve the generation quality on question answering task.,"In this article, we focus on improving the effectiveness of the reader module and propose a novel copy-augmented generative approach that integrates the merits of both extractive and generative readers.",False,False
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,1,20,llm,Link,No Link,This paper proposes add copy probability in a transformer-based model to improve the generation quality on question answering task.,"We add a linear layer to calculate the generation probability, which decides the weights of generating words from vocabulary or copying from source passages.",False,False
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,1,41,mutual,Link,Link,This paper proposes add copy probability in a transformer-based model to improve the generation quality on question answering task.,"At each decoding stage, the model is able to either directly copy a word from the input or generate one with certain probability, and thus can be viewed as a combination of extractive and generative approaches.",True,True
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,1,44,random,No Link,No Link,This paper proposes add copy probability in a transformer-based model to improve the generation quality on question answering task.,Our model follows the standard two-stage retrieverreader framework with a focus on the enhancement of the reader module built upon the FiD model.,True,False
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,1,57,retriever,Link,Link,This paper proposes add copy probability in a transformer-based model to improve the generation quality on question answering task.,"In addition, the probability of copying is 1 − p gen .",True,True
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,1,69,random,No Link,No Link,This paper proposes add copy probability in a transformer-based model to improve the generation quality on question answering task.,"We evaluate the performance of our approach on two standard ODQA datasets, NQ and TriviaQA.",True,False
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,1,88,retriever,Link,No Link,This paper proposes add copy probability in a transformer-based model to improve the generation quality on question answering task.,Note that a higher generation probability means that more tokens are produced from the vocabulary instead of copying from the input.,False,False
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,2,15,random,No Link,No Link,"Specifically, the attention distribution from the last encoder-decoder attention layer is used as the copy probability.",We found that the phenomenon also happens in ODQA.,True,False
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,2,22,llm,Link,Link,"Specifically, the attention distribution from the last encoder-decoder attention layer is used as the copy probability.","To be more specific, our model fusionin-decoder pointer-generator network (FiD-PGN) is built upon the state-of-the-art model FiD. We reuse the encoder-decoder attention scores as the copy distribution to reduce the computational cost.",True,True
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,2,55,retriever,No Link,No Link,"Specifically, the attention distribution from the last encoder-decoder attention layer is used as the copy probability.","Specifically, at each decoding step t, let e t ∈ R d be the embedding vector of the input token at this step, and denote s L t ∈ R d as the output representation of the last layer L of transformer decoder, then the probability of generation is given as follows,",True,False
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,2,61,mutual,Link,Link,"Specifically, the attention distribution from the last encoder-decoder attention layer is used as the copy probability.","Benefiting from the encoder-decoder attention layer in transformer architecture, we directly utilize the cross-attention score α L t of the last decoder layer L over the source tokens for the target token y t as copy distribution.",True,True
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,2,85,random,No Link,No Link,"Specifically, the attention distribution from the last encoder-decoder attention layer is used as the copy probability.",Generation Probability.,True,False
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,3,20,retriever,Link,Link,"In addition, the word embedding and the output representation are used together to compute generation probability.","We add a linear layer to calculate the generation probability, which decides the weights of generating words from vocabulary or copying from source passages.",True,True
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,3,27,random,No Link,No Link,"In addition, the word embedding and the output representation are used together to compute generation probability.","In this era of data explosion, ODQA offers a way to rapidly and accurately fulfill user's information needs, and hence has recently received significant attention from both industry and academia (Min et al., 2021a).",True,False
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,3,41,retriever,Link,No Link,"In addition, the word embedding and the output representation are used together to compute generation probability.","At each decoding stage, the model is able to either directly copy a word from the input or generate one with certain probability, and thus can be viewed as a combination of extractive and generative approaches.",False,False
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,3,48,random,No Link,No Link,"In addition, the word embedding and the output representation are used together to compute generation probability.",Reader Encoder.,True,False
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,3,55,mutual,Link,No Link,"In addition, the word embedding and the output representation are used together to compute generation probability.","Specifically, at each decoding step t, let e t ∈ R d be the embedding vector of the input token at this step, and denote s L t ∈ R d as the output representation of the last layer L of transformer decoder, then the probability of generation is given as follows,",False,False
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,3,59,llm,Link,No Link,"In addition, the word embedding and the output representation are used together to compute generation probability.","Then at step t, the probability distribution of words generation over the vocabulary is computed as,",False,False
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,3,65,llm,Link,No Link,"In addition, the word embedding and the output representation are used together to compute generation probability.","Finally, put all the above together, the target token y t could both be generated from vocabulary with probability p gen , and copy from the source passages.",False,False
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,4,7,mutual,Link,Link,The experiment results show the proposal method outperforms baseline model on two benchmark datasets.,"We conduct experiments on the two benchmark datasets, Natural Questions and TriviaQA, and the empirical results demonstrate the performance gains of our proposed approach.",True,True
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,4,81,mutual,Link,Link,The experiment results show the proposal method outperforms baseline model on two benchmark datasets.,"As shown in Table 2, our model outperforms FiD-KD on both NQ and TriviaQA datasets under the same setting.",True,True
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,4,82,random,Link,No Link,The experiment results show the proposal method outperforms baseline model on two benchmark datasets.,This demonstrates that the pointer network could help to generate answers more accurately.,False,False
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,4,85,random,No Link,No Link,The experiment results show the proposal method outperforms baseline model on two benchmark datasets.,Generation Probability.,True,False
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,4,101,mutual,Link,Link,The experiment results show the proposal method outperforms baseline model on two benchmark datasets.,"Experimental results show that our model outperforms FiD-KD on two benchmark datasets under the same setting, demonstrating the advantages of our method.",True,True
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,6,17,random,No Link,No Link,"1. The paper is well organized, and it explains the proposed solution clearly.","While in both cases, the ground-truth answers are present in the retrieved passages.",True,False
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,6,74,random,No Link,No Link,"1. The paper is well organized, and it explains the proposed solution clearly.","We follow the experimental settings as in FiD. Our model is initialized with a pre-trained T5-base model, and trained using AdamW (Loshchilov and Hutter, 2017) algorithm with a learning rate of 10 −4 , linear scheduling with 15k total steps and 1k warm-up steps.",True,False
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,8,43,random,No Link,No Link,1. The idea is incremental.,Method,True,False
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,8,69,random,No Link,No Link,1. The idea is incremental.,"We evaluate the performance of our approach on two standard ODQA datasets, NQ and TriviaQA.",True,False
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,9,3,random,No Link,No Link,"Applying the copy mechanism in the point generator paper on transformer model has been studied in many papers. [1] Deaton, Jon, et al. ""Transformers and pointer-generator networks for abstractive summarization."" ( 2019). [2] Prabhu, Nikhil, and Katharina Kann. "" Making a Point: Pointer-Generator Transformers for Disjoint Vocabularies.""",Existing modern approaches mostly follow a standard two-stage paradigm: retriever then reader.,True,False
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,9,19,mutual,Link,No Link,"Applying the copy mechanism in the point generator paper on transformer model has been studied in many papers. [1] Deaton, Jon, et al. ""Transformers and pointer-generator networks for abstractive summarization."" ( 2019). [2] Prabhu, Nikhil, and Katharina Kann. "" Making a Point: Pointer-Generator Transformers for Disjoint Vocabularies.""","Inspired by the work of See et al. (2017), we enhance the generative model with a pointer network (Vinyals et al., 2017), that enables the model Figure 1: The overall architecture of our proposed model.",False,False
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,9,22,mutual,Link,Link,"Applying the copy mechanism in the point generator paper on transformer model has been studied in many papers. [1] Deaton, Jon, et al. ""Transformers and pointer-generator networks for abstractive summarization."" ( 2019). [2] Prabhu, Nikhil, and Katharina Kann. "" Making a Point: Pointer-Generator Transformers for Disjoint Vocabularies.""","To be more specific, our model fusionin-decoder pointer-generator network (FiD-PGN) is built upon the state-of-the-art model FiD. We reuse the encoder-decoder attention scores as the copy distribution to reduce the computational cost.",True,True
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,9,40,mutual,Link,Link,"Applying the copy mechanism in the point generator paper on transformer model has been studied in many papers. [1] Deaton, Jon, et al. ""Transformers and pointer-generator networks for abstractive summarization."" ( 2019). [2] Prabhu, Nikhil, and Katharina Kann. "" Making a Point: Pointer-Generator Transformers for Disjoint Vocabularies.""","Pointer-Generator Network (See et al., 2017) is an extension of the sequence-to-sequence model by integrating a copy mechanism (Vinyals et al., 2017) into the generator.",True,True
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,9,81,random,No Link,No Link,"Applying the copy mechanism in the point generator paper on transformer model has been studied in many papers. [1] Deaton, Jon, et al. ""Transformers and pointer-generator networks for abstractive summarization."" ( 2019). [2] Prabhu, Nikhil, and Katharina Kann. "" Making a Point: Pointer-Generator Transformers for Disjoint Vocabularies.""","As shown in Table 2, our model outperforms FiD-KD on both NQ and TriviaQA datasets under the same setting.",True,False
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,10,27,random,No Link,No Link,"2. The experimental results are not convincing enough. Only compared to the base version of FiD/FiD-KD model, and lack of result on a large version.","In this era of data explosion, ODQA offers a way to rapidly and accurately fulfill user's information needs, and hence has recently received significant attention from both industry and academia (Min et al., 2021a).",True,False
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,10,70,random,No Link,No Link,"2. The experimental results are not convincing enough. Only compared to the base version of FiD/FiD-KD model, and lack of result on a large version.",The NQ dataset comprises real queries that user issued on Google search engine along with answers.,True,False
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,12,63,random,No Link,No Link,1. What’s the performance of the proposed method on the Yes/No answer which cannot be extracted from passage?,"where x 1:k denotes the concatenation of the top-k retrieved passages, x 1:k,j is the j-th token of x 1:k , and α L t,j is the j-th element of α L t .",True,False
198f32a7ba5e1083bf995ca2e958172d04fd4e91c2328a0a7b3579cc414539ea76212bcbd78c43bed2ea3174a8857cc2b88b5b220c8c8534766884c8903104ce,12,104,random,No Link,No Link,1. What’s the performance of the proposed method on the Yes/No answer which cannot be extracted from passage?,"We can observe that the matching scores of both models increase with respect to the number of passages used in training, consistent with the findings in Izacard and Grave (2020b) that sequence-to-sequence model is capable of gathering information across multiple retrieved passages.",True,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,1,5,mutual,Link,Link,"In this paper, authors propose a new variation of slot accuracy metric, called as relative slot accuracy (RSA), for dialog state tracking (DST) evaluation.","Additionally, we propose relative slot accuracy to complement existing metrics.",True,True
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,1,18,llm,Link,Link,"In this paper, authors propose a new variation of slot accuracy metric, called as relative slot accuracy (RSA), for dialog state tracking (DST) evaluation.","To address the above challenge, we propose reporting the relative slot accuracy along with the existing metrics in MultiWOZ.",True,True
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,1,56,random,No Link,No Link,"In this paper, authors propose a new variation of slot accuracy metric, called as relative slot accuracy (RSA), for dialog state tracking (DST) evaluation.",Experiments,True,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,1,69,retriever,Link,Link,"In this paper, authors propose a new variation of slot accuracy metric, called as relative slot accuracy (RSA), for dialog state tracking (DST) evaluation.","In summary, relative slot accuracy enables relative comparison according to the distribution of the domain in a dialog.",True,True
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,1,79,retriever,Link,Link,"In this paper, authors propose a new variation of slot accuracy metric, called as relative slot accuracy (RSA), for dialog state tracking (DST) evaluation.","Therefore, relative slot accuracy can provide an intuitive evaluation reflecting the current belief state recording method, in which the number of slots accumulates incrementally as the conversation progresses.",True,True
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,1,82,llm,Link,Link,"In this paper, authors propose a new variation of slot accuracy metric, called as relative slot accuracy (RSA), for dialog state tracking (DST) evaluation.","Accordingly, the relative slot accuracy is proposed.",True,True
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,1,90,random,No Link,No Link,"In this paper, authors propose a new variation of slot accuracy metric, called as relative slot accuracy (RSA), for dialog state tracking (DST) evaluation.","In the first dialog presented in Table A1, the joint goal accuracy is measured as 1 at turn 2.",True,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,2,15,retriever,No Link,Link,"Unlike the slot accuracy that tracks all the pre-defined slots based on ontology, RSA only uses the slots that either appear in the predicted or the gold states in the given turn.","Joint goal accuracy strictly determines whether every predicted state is identical to the gold state, whereas slot accuracy measures the ratio of correct predictions.",False,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,2,19,llm,Link,Link,"Unlike the slot accuracy that tracks all the pre-defined slots based on ontology, RSA only uses the slots that either appear in the predicted or the gold states in the given turn.","Because slot accuracy has the challenge of overestimation by always considering all predefined slots in every turn, relative slot accuracy does not depend on predefined slots, and calculates a score that is affected solely by slots that appear in the current dialog.",True,True
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,2,39,random,No Link,No Link,"Unlike the slot accuracy that tracks all the pre-defined slots based on ontology, RSA only uses the slots that either appear in the predicted or the gold states in the given turn.","Each value of x-axis in Figure 2 indicates the ""maximum"" number of slots that appear in a single dialog, and we confirmed that approximately 85% of the test set utilized solely less than 12 of the 30 predefined slots in the experiment.",True,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,2,43,retriever,No Link,No Link,"Unlike the slot accuracy that tracks all the pre-defined slots based on ontology, RSA only uses the slots that either appear in the predicted or the gold states in the given turn.","The slot accuracies of turns 0 and 1 show approximately 96% accuracy, despite the model not correctly predicting states at all.",True,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,2,53,llm,Link,Link,"Unlike the slot accuracy that tracks all the pre-defined slots based on ontology, RSA only uses the slots that either appear in the predicted or the gold states in the given turn.","Equation 3 expresses how to calculate the relative slot accuracy, and T * denotes the number of unique slots appearing in the predicted and gold states in a particular turn.",True,True
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,2,54,retriever,Link,Link,"Unlike the slot accuracy that tracks all the pre-defined slots based on ontology, RSA only uses the slots that either appear in the predicted or the gold states in the given turn.",Relative slot accuracy rewards well-predicted belief states by measuring the scores in accumulating turns.,True,True
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,2,66,llm,Link,Link,"Unlike the slot accuracy that tracks all the pre-defined slots based on ontology, RSA only uses the slots that either appear in the predicted or the gold states in the given turn.",Relative slot accuracy derives a specific score in the turn configuration and prediction ratio of each domain by excluding slots that do not appear in the conversation.,True,True
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,2,90,random,No Link,No Link,"Unlike the slot accuracy that tracks all the pre-defined slots based on ontology, RSA only uses the slots that either appear in the predicted or the gold states in the given turn.","In the first dialog presented in Table A1, the joint goal accuracy is measured as 1 at turn 2.",True,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,3,21,mutual,Link,Link,The metric is very intuitive and makes perfect sense.,It is expected that the proposed metric can be adopted to evaluate model performance more intuitively.,True,True
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,3,27,random,No Link,No Link,The metric is very intuitive and makes perfect sense.,"As illustrated in Figure 1, we measured the relative position of the turn causing this phenomenon for the dialog.",True,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,3,47,random,No Link,No Link,The metric is very intuitive and makes perfect sense.,"Therefore, the slot accuracy measured according to Equation 2 differs from our intuition.",True,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,5,7,mutual,Link,Link,- Evaluating DST appropriately is still challenging and joint goal accuracy (JGA) heavily penalizes a system for even one mistake.,"This study also encourages not solely the reporting of joint goal accuracy, but also various complementary metrics in DST tasks for the sake of a realistic evaluation.",True,True
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,5,16,mutual,Link,Link,- Evaluating DST appropriately is still challenging and joint goal accuracy (JGA) heavily penalizes a system for even one mistake.,"However, we determined that these two metrics solely focus on ""penalizing states that fail to predict,"" not considering ""reward for well-predicted states."" Accordingly, as also pointed out in Rastogi et al. (2020a), joint goal accuracy underestimates the model prediction because of its error accumulation attribute, while slot accuracy overestimates it because of its dependency on predefined slots.",True,True
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,5,26,mutual,Link,Link,- Evaluating DST appropriately is still challenging and joint goal accuracy (JGA) heavily penalizes a system for even one mistake.,"JGA = 1 if predicted state = gold state 0 otherwise (1) However, the joint goal accuracy underestimates the accumulated states because it scores the performances of later turn to zero if the model mispredicts even once in a particular turn, regardless of the model prediction quality at later turns.",True,True
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,5,49,random,No Link,No Link,- Evaluating DST appropriately is still challenging and joint goal accuracy (JGA) heavily penalizes a system for even one mistake.,"As can be observed in Equation 2, slot accuracy has the characteristic that the larger the number of predefined slots (T ), the smaller the deviation between the prediction results.",True,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,5,75,random,Link,No Link,- Evaluating DST appropriately is still challenging and joint goal accuracy (JGA) heavily penalizes a system for even one mistake.,Reward & penalty on relative dialog turn Relative slot accuracy is able to reward the model's correct prediction by measuring the accuracy on a relative basis for each turn.,False,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,6,16,mutual,Link,Link,"Thus, JGA underestimates the performance of system, while another commonly used slot accuracy metric overestimates the performance.","However, we determined that these two metrics solely focus on ""penalizing states that fail to predict,"" not considering ""reward for well-predicted states."" Accordingly, as also pointed out in Rastogi et al. (2020a), joint goal accuracy underestimates the model prediction because of its error accumulation attribute, while slot accuracy overestimates it because of its dependency on predefined slots.",True,True
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,6,18,random,Link,No Link,"Thus, JGA underestimates the performance of system, while another commonly used slot accuracy metric overestimates the performance.","To address the above challenge, we propose reporting the relative slot accuracy along with the existing metrics in MultiWOZ.",False,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,6,75,random,Link,No Link,"Thus, JGA underestimates the performance of system, while another commonly used slot accuracy metric overestimates the performance.",Reward & penalty on relative dialog turn Relative slot accuracy is able to reward the model's correct prediction by measuring the accuracy on a relative basis for each turn.,False,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,7,5,mutual,Link,Link,The proposed modification to slot accuracy metric is very intuitive and can be used to better compare DST systems.,"Additionally, we propose relative slot accuracy to complement existing metrics.",True,True
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,7,15,random,No Link,No Link,The proposed modification to slot accuracy metric is very intuitive and can be used to better compare DST systems.,"Joint goal accuracy strictly determines whether every predicted state is identical to the gold state, whereas slot accuracy measures the ratio of correct predictions.",True,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,7,18,llm,Link,Link,The proposed modification to slot accuracy metric is very intuitive and can be used to better compare DST systems.,"To address the above challenge, we propose reporting the relative slot accuracy along with the existing metrics in MultiWOZ.",True,True
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,7,23,random,No Link,No Link,The proposed modification to slot accuracy metric is very intuitive and can be used to better compare DST systems.,Joint Goal Accuracy,True,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,7,82,mutual,No Link,No Link,The proposed modification to slot accuracy metric is very intuitive and can be used to better compare DST systems.,"Accordingly, the relative slot accuracy is proposed.",True,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,7,84,retriever,Link,Link,The proposed modification to slot accuracy metric is very intuitive and can be used to better compare DST systems.,"When the DST task is scaled up to deal with more diverse conversational situations, a realistic model evaluation will be possible using relative slot accuracy.",True,True
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,9,3,random,No Link,No Link,- A major concern is the lack of comparison against the newly proposed average goal accuracy (AGA) metric.,"A belief state, one of the core pieces of information, refers to the subject and its specific content, and appears in the form of domain-slot-value.",True,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,9,33,random,No Link,No Link,- A major concern is the lack of comparison against the newly proposed average goal accuracy (AGA) metric.,Slot Accuracy,True,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,10,27,random,No Link,No Link,Please refer to the question.,"As illustrated in Figure 1, we measured the relative position of the turn causing this phenomenon for the dialog.",True,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,10,69,random,No Link,No Link,Please refer to the question.,"In summary, relative slot accuracy enables relative comparison according to the distribution of the domain in a dialog.",True,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,11,5,retriever,Link,Link,"It is important to understand how relative slot accuracy metric differs from the existing AGA metric, and what are the benefit of using this new metric along with (or in place of) the AGA metric.","Additionally, we propose relative slot accuracy to complement existing metrics.",True,True
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,11,18,llm,Link,Link,"It is important to understand how relative slot accuracy metric differs from the existing AGA metric, and what are the benefit of using this new metric along with (or in place of) the AGA metric.","To address the above challenge, we propose reporting the relative slot accuracy along with the existing metrics in MultiWOZ.",True,True
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,11,59,random,No Link,No Link,"It is important to understand how relative slot accuracy metric differs from the existing AGA metric, and what are the benefit of using this new metric along with (or in place of) the AGA metric.",We selected the DST models in Table A5 that perform the Mul-tiWOZ experiment with the original authors' reproducible code 2 .,True,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,11,63,random,Link,No Link,"It is important to understand how relative slot accuracy metric differs from the existing AGA metric, and what are the benefit of using this new metric along with (or in place of) the AGA metric.","Specifically, it can be compared with a different perspective when using the proposed reward-considering evaluation metric.",False,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,14,16,retriever,Link,Link,How does this metric correlate with the Average Goal Metric proposed in Rastogi et al. (2020b)?,"However, we determined that these two metrics solely focus on ""penalizing states that fail to predict,"" not considering ""reward for well-predicted states."" Accordingly, as also pointed out in Rastogi et al. (2020a), joint goal accuracy underestimates the model prediction because of its error accumulation attribute, while slot accuracy overestimates it because of its dependency on predefined slots.",True,True
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,14,53,random,No Link,No Link,How does this metric correlate with the Average Goal Metric proposed in Rastogi et al. (2020b)?,"Equation 3 expresses how to calculate the relative slot accuracy, and T * denotes the number of unique slots appearing in the predicted and gold states in a particular turn.",True,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,14,62,llm,Link,Link,How does this metric correlate with the Average Goal Metric proposed in Rastogi et al. (2020b)?,"Furthermore, the correlation with joint goal accuracy, a mainly adopted metric, and relative slot accuracy with respect to each turn is lower than the correlation with joint goal accuracy and slot accuracy, as illustrated in Figure 3.",True,True
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,14,65,random,No Link,No Link,How does this metric correlate with the Average Goal Metric proposed in Rastogi et al. (2020b)?,"We reported the joint goal, slot, and relative slot accuracies per domain utilizing the SOM-DST model in Table 2.",True,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,15,40,retriever,Link,Link,"If you are using entire belief/predicted states for calculating T*, your metric is almost identical to the AGA.","Because the number of belief states appearing in the early and middle turns of the dialog are smaller, and even fewer states make false predictions, calculating slot accuracy using Equation 2 reduces the influence of M and W , and the final score is dominated by the total slot number T .",True,True
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,15,44,random,Link,No Link,"If you are using entire belief/predicted states for calculating T*, your metric is almost identical to the AGA.","It becomes difficult to compare various models in detail, if each model shows a high performance, even though nothing is adequately predicted.",False,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,15,53,llm,Link,Link,"If you are using entire belief/predicted states for calculating T*, your metric is almost identical to the AGA.","Equation 3 expresses how to calculate the relative slot accuracy, and T * denotes the number of unique slots appearing in the predicted and gold states in a particular turn.",True,True
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,15,76,random,Link,No Link,"If you are using entire belief/predicted states for calculating T*, your metric is almost identical to the AGA.",Table A6 compares the slot and relative slot accuracies.,False,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,16,7,llm,Link,No Link,I think you should include evaluation results with the AGA metric.,"This study also encourages not solely the reporting of joint goal accuracy, but also various complementary metrics in DST tasks for the sake of a realistic evaluation.",False,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,16,10,random,No Link,No Link,I think you should include evaluation results with the AGA metric.,"Recently, multi-turn DST datasets have been constructed using the Wizard-of-Oz method to reflect more realistic dialog situations (Wen et al., 2017;Mrkšić et al., 2017;Budzianowski et al., 2018).",True,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,16,22,retriever,No Link,Link,I think you should include evaluation results with the AGA metric.,Current Evaluation Metrics,False,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,16,78,random,No Link,No Link,I think you should include evaluation results with the AGA metric.,"In addition, regarding slot accuracy in turns 4, 5, and 6, there is no score improvement for the additional wellpredicted state by the model, whereas the score increases when the newly added state is matched in the case of relative slot accuracy.",True,False
1f63cc5901bef369c232cedfdf722fdf15b03dbc2d1570dfdcb2d635e82e96e3a23aee4c4ea0f71b12d5eeff81d92af1bc50ed0dc14a11a7822247a3869cfabc,16,85,mutual,Link,Link,I think you should include evaluation results with the AGA metric.,"Moreover, we suggest reporting various evaluation metrics to complement the limitations of each metric in future studies, not solely reporting the joint goal accuracy.",True,True
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,1,5,mutual,Link,No Link,"The authors propose and evaluate an approach for automatic question generation in an educational context, specifically looking at the role of summarization (whether it be human or automatic) in the process.","We also find that, in the absence of human-written summaries, automatic summarization can serve as a good middle ground.",False,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,1,8,retriever,Link,No Link,"The authors propose and evaluate an approach for automatic question generation in an educational context, specifically looking at the role of summarization (whether it be human or automatic) in the process.",Automatic Question Generation (QG) is a powerful tool that could be used to significantly lessen the amount of time it takes to write such questions.,False,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,1,60,random,Link,No Link,"The authors propose and evaluate an approach for automatic question generation in an educational context, specifically looking at the role of summarization (whether it be human or automatic) in the process.",These RAs were encouraged to make these summaries easily readable by humans rather than to be easily understandable by machines.,False,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,1,78,mutual,No Link,No Link,"The authors propose and evaluate an approach for automatic question generation in an educational context, specifically looking at the role of summarization (whether it be human or automatic) in the process.","Finally, in the case of automatic summaries, we see that relevance and in-context interpretability are somewhat improved as compared to the original text questions while grammaticality suffers slightly.",True,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,1,90,llm,Link,Link,"The authors propose and evaluate an approach for automatic question generation in an educational context, specifically looking at the role of summarization (whether it be human or automatic) in the process.",We show that asking questions on summarized text ameliorates this in large part and that these gains can be approximated by the use of automatic summarization.,True,True
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,1,92,random,Link,No Link,"The authors propose and evaluate an approach for automatic question generation in an educational context, specifically looking at the role of summarization (whether it be human or automatic) in the process.","Work done concurrently to ours by Lyu et al. (2021) already has promising results in this direction, showing that training a QG model on synthetic data from summarized text improves performance on downstream QA.",False,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,4,4,mutual,Link,Link,"The paper contains insightful empirical results showing the high value of using human summaries in the process,  but also shows that machine generated summaries can also be useful.",We find that giving these models human-written summaries instead of the original text results in a significant increase in acceptability of generated questions (33% -> 83%) as determined by expert annotators.,True,True
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,4,5,mutual,No Link,No Link,"The paper contains insightful empirical results showing the high value of using human summaries in the process,  but also shows that machine generated summaries can also be useful.","We also find that, in the absence of human-written summaries, automatic summarization can serve as a good middle ground.",True,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,4,18,mutual,Link,Link,"The paper contains insightful empirical results showing the high value of using human summaries in the process,  but also shows that machine generated summaries can also be useful.","• We show that, in the absence of humanwritten summaries, providing automatically generated summaries as input is a good alternative.",True,True
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,4,41,random,No Link,No Link,"The paper contains insightful empirical results showing the high value of using human summaries in the process,  but also shows that machine generated summaries can also be useful.",We assume that answer extraction will help both QA and QG and therefore use a model that was fine-tuned on all three.,True,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,4,87,random,No Link,No Link,"The paper contains insightful empirical results showing the high value of using human summaries in the process,  but also shows that machine generated summaries can also be useful.",Examples of questions for each category on which there was significant disagreement are listed in Appendix B.,True,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,5,5,retriever,No Link,No Link,It also contains well organized related literature.,"We also find that, in the absence of human-written summaries, automatic summarization can serve as a good middle ground.",True,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,5,27,random,Link,No Link,It also contains well organized related literature.,"The feasibility of using answer-aware neural QG in an educational setting was investigated by Wang et al. (2018), who used a BiLSTM encoder (Zhang et al., 2015) to encode C and A and a unidirectional LSTM decoder to generate Q. They trained on the SQuAD dataset (Rajpurkar et al., 2018) and evaluated on textbooks from various domains (history, sociology, biology).",False,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,5,47,random,No Link,No Link,It also contains well organized related literature.,"The answer extraction fine-tuning task thus becomes modeling P (A|C ′ ) where A = {c k , ..., c k+l } such that k ≥ s and k + l ≤ e.",True,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,5,60,retriever,No Link,No Link,It also contains well organized related literature.,These RAs were encouraged to make these summaries easily readable by humans rather than to be easily understandable by machines.,True,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,5,73,llm,No Link,No Link,It also contains well organized related literature.,We provided many annotation examples to our annotators and wrote clear guidelines about each category to ensure high agreement.,True,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,5,98,llm,No Link,No Link,It also contains well organized related literature.,Several discussion sessions were held between the authors and annotators to ensure that these guidelines were well understood and that they were sensible for the task.,True,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,6,76,random,No Link,No Link,"Overall, the paper is well written, and easy to understand for a large audience, with good use of examples.",We note that a majority of observed errors in the original text questions stem from them being either irrelevant or un-interpretable out of context.,True,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,6,83,random,No Link,No Link,"Overall, the paper is well written, and easy to understand for a large audience, with good use of examples.","For the other three categories we see an average pairwise agree- Table 2: Comparison between our three annotators (A1, A2, A3) on all 300 questions across all categories.",True,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,11,65,random,No Link,No Link,Were there other datasets that were considered?,The summarized output sub-passages were then concatenated together before running question generation.,True,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,11,81,random,No Link,No Link,Were there other datasets that were considered?,In Table 2 we report the per-annotator statistics as well as the pairwise inter-annotator agreement (IAA).,True,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,13,36,retriever,Link,Link,How did you determine what questions you were going to ask the annotators?,"In our study, we explicitly ask annotators to determine whether or not a generated question is relevant to the topic of the textbook chapter from which it is generated.",True,True
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,13,51,random,No Link,No Link,How did you determine what questions you were going to ask the annotators?,4 We then generate one question per extracted answer span using the same model in an answer-aware fashion.,True,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,13,71,llm,Link,Link,How did you determine what questions you were going to ask the annotators?,"We asked the annotators to answer the following yes/no questions: a) Would you directly use this question as a flashcard?, b) Is this question grammatical?, c) Does this question make sense out of context?, d) Is this question relevant? and e) Is the answer to this question correct?",True,True
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,13,76,random,No Link,No Link,How did you determine what questions you were going to ask the annotators?,We note that a majority of observed errors in the original text questions stem from them being either irrelevant or un-interpretable out of context.,True,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,14,3,random,No Link,No Link,How is it related to what has been done in related research?,We show that a significant portion of errors in such systems arise from asking irrelevant or un-interpretable questions and that such errors can be ameliorated by providing summarized input.,True,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,14,19,retriever,No Link,No Link,How is it related to what has been done in related research?,Related Work & Background,True,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,14,28,retriever,No Link,No Link,How is it related to what has been done in related research?,"They showed that generated questions were largely grammatical, relevant, and had high n-gram overlap with humanauthored questions.",True,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,14,34,llm,No Link,No Link,How is it related to what has been done in related research?,"However, their human evaluation centered around question correctness and fluency rather than relevance of answer selection.",True,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,14,35,mutual,Link,Link,How is it related to what has been done in related research?,"Similar follow-up studies also fail to explicitly ask human annotators whether or not the extracted answers, and subsequent generated questions, were relevant to the broader topic of the context passage (Willis et al., 2019;Cui et al., 2021;Wang et al., 2019;Du and Cardie, 2018;Alberti et al., 2019;Back et al., 2021).",True,True
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,14,36,llm,No Link,No Link,How is it related to what has been done in related research?,"In our study, we explicitly ask annotators to determine whether or not a generated question is relevant to the topic of the textbook chapter from which it is generated.",True,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,14,76,random,No Link,No Link,How is it related to what has been done in related research?,We note that a majority of observed errors in the original text questions stem from them being either irrelevant or un-interpretable out of context.,True,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,15,48,random,No Link,No Link,What was the process for identifying annotators?,"Because T5 has a fixed maximum context length of 512 tokens, input passages that contain n > 512 tokens must be split up into smaller sub-passages.",True,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,15,66,random,No Link,No Link,What was the process for identifying annotators?,In total we generated 318 question-answer pairs from our automatic summaries.,True,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,16,61,random,No Link,Link,Why only three annotators?,From these 3 sets of summaries we generated a total of 667 question-answer pairs.,False,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,16,69,mutual,Link,Link,Why only three annotators?,"We recruited three expert annotators, all undergraduates in computer science, to evaluate the quality of the question-answer pairs.",True,True
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,16,70,mutual,Link,Link,Why only three annotators?,All 300 pairs were given to all three annotators.,True,True
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,16,99,random,No Link,Link,Why only three annotators?,"During annotation, annotators were not given the original source text from which the question was generated.",False,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,17,26,random,No Link,No Link,"Glad to see that the ""full annotation data"" will be available, and I'm assuming that any other required data will also be available to other researchers as well.","These models are typically evaluated using n-gram overlap metrics such as BLEU/ROUGE/METEOR (Papineni et al., 2002;Lin, 2004;Banerjee and Lavie, 2005) with the reference being the original human-authored question as provided by the extractive QA dataset.",True,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,17,47,random,No Link,No Link,"Glad to see that the ""full annotation data"" will be available, and I'm assuming that any other required data will also be available to other researchers as well.","The answer extraction fine-tuning task thus becomes modeling P (A|C ′ ) where A = {c k , ..., c k+l } such that k ≥ s and k + l ≤ e.",True,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,17,108,mutual,Link,Link,"Glad to see that the ""full annotation data"" will be available, and I'm assuming that any other required data will also be available to other researchers as well.",We provide our full annotation data in CSV form in the supplementary material for further inspection.,True,True
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,19,16,random,No Link,No Link,No ethical concerns.,"Input: The perplexity of a language model on a test set is the inverse probability of the test set, normalized by the number of words.",True,False
e2bbccf70bac178157aed9b379a8c457837d1219e497b0a5529865ba660bf47ce45eb8d30ce9eb680ab7a321ee07ca84b73a73b82f24297897c544b69f807833,19,29,random,No Link,No Link,No ethical concerns.,"However, given that we may not always have a convenient list of key terms to use as answer spans for an input passage, there is a desire to move past answer-aware QG models and evaluate the feasibility of answer-unaware models for use in education.",True,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,1,2,mutual,Link,Link,"This paper investigates the degree of knowledge that pre-trained LM, with only access to a vocabulary of subword tokens, have about the character composition of these tokens, and if enriching these models with orthographic information about the tokens can improve them.",Standard pretrained language models operate on sequences of subword tokens without direct access to the characters that compose each token's string representation.,True,True
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,1,3,mutual,Link,Link,"This paper investigates the degree of knowledge that pre-trained LM, with only access to a vocabulary of subword tokens, have about the character composition of these tokens, and if enriching these models with orthographic information about the tokens can improve them.","We probe the embedding layer of pretrained language models and show that models learn the internal character composition of whole word and subword tokens to a surprising extent, without ever seeing the characters coupled with the tokens.",True,True
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,1,45,random,No Link,No Link,"This paper investigates the degree of knowledge that pre-trained LM, with only access to a vocabulary of subword tokens, have about the character composition of these tokens, and if enriching these models with orthographic information about the tokens can improve them.",3,True,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,1,69,random,Link,No Link,"This paper investigates the degree of knowledge that pre-trained LM, with only access to a vocabulary of subword tokens, have about the character composition of these tokens, and if enriching these models with orthographic information about the tokens can improve them.","We find that more data makes for a better probe, but that even a few thousand examples are enough to train SpellingBee to extract significant character information from the embeddings, which cannot be extracted from randomized vectors (the control).",False,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,1,85,mutual,Link,Link,"This paper investigates the degree of knowledge that pre-trained LM, with only access to a vocabulary of subword tokens, have about the character composition of these tokens, and if enriching these models with orthographic information about the tokens can improve them.","This work reveals that pretrained language models learn, to some extent, the character composition of subword tokens.",True,True
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,2,14,mutual,Link,Link,"It proposes to use a probe they name SpellingBee: a generative character-based LM that takes as input an uncontextualized word embedding from a model, and tries to predict the correct character sequence.",SpellingBee is a generative language model that predicts the character composition of a token given only its (uncontextualized) vector representation from the pretrained model's embeddings matrix.,True,True
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,2,16,random,Link,No Link,"It proposes to use a probe they name SpellingBee: a generative character-based LM that takes as input an uncontextualized word embedding from a model, and tries to predict the correct character sequence.","If the probe can successfully reconstruct the correct character sequence from an unseen token's embedding, then there must be significant orthographic information encoded in the vector.",False,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,2,26,mutual,Link,Link,"It proposes to use a probe they name SpellingBee: a generative character-based LM that takes as input an uncontextualized word embedding from a model, and tries to predict the correct character sequence.","To measure how much a model knows the character composition of its tokens, we introduce Spelling-Bee, a generative probe that tries to spell out a token character-by-character.",True,True
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,2,29,llm,Link,Link,"It proposes to use a probe they name SpellingBee: a generative character-based LM that takes as input an uncontextualized word embedding from a model, and tries to predict the correct character sequence.","We do so by modeling SpellingBee as a character-based language model, where the first token is a vector representation of the vocabulary item.",True,True
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,2,40,retriever,Link,Link,"It proposes to use a probe they name SpellingBee: a generative character-based LM that takes as input an uncontextualized word embedding from a model, and tries to predict the correct character sequence.","While we mainly use SpellingBee as a probe, a variation of our method could potentially imbue the embedding layer with character information before training a language model.",True,True
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,2,98,random,Link,Link,"It proposes to use a probe they name SpellingBee: a generative character-based LM that takes as input an uncontextualized word embedding from a model, and tries to predict the correct character sequence.","This trend is particularly sharp when the similarity filter is applied, as the probe is hardly able to spell tokens with 6 or more characters accurately; having said that, the probe is able to generate many partially correct spellings, as measured by chrF (Figure 6).",True,True
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,3,27,random,Link,Link,"It is trained on part of the model's vocabulary, and tested on the other: if it manages to succesfully generalize, the embedding must contain orthographic information.","Specifically, Spelling-Bee probes the original model's embedding matrix, since spelling is a property of token types, invariant to context.",True,True
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,3,82,random,Link,No Link,"It is trained on part of the model's vocabulary, and tested on the other: if it manages to succesfully generalize, the embedding must contain orthographic information.",This result indicates that the model does not utilize the character information injected into the tokens' embeddings.,False,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,4,40,random,No Link,No Link,"The probe is tested on 4 models: Roberta-base, and 3 others showing change in a particular aspect: Roberta-large for size, AraBert for language, and GPT2-Medium for an autoregressive model.","While we mainly use SpellingBee as a probe, a variation of our method could potentially imbue the embedding layer with character information before training a language model.",True,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,4,46,mutual,Link,Link,"The probe is tested on 4 models: Roberta-base, and 3 others showing change in a particular aspect: Roberta-large for size, AraBert for language, and GPT2-Medium for an autoregressive model.","Pretrained Models We probe four pretrained models: RoBERTa-Base and Large (Liu et al., 2019), GPT2-Medium (Radford et al., 2019), and AraBERT-Large (Antoun et al., 2020).",True,True
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,4,52,random,No Link,Link,"The probe is tested on 4 models: Roberta-base, and 3 others showing change in a particular aspect: Roberta-large for size, AraBert for language, and GPT2-Medium for an autoregressive model.","We then filter the remaining  4 The lemma filter always applies the similarity filter first, providing an even more adversarial approach for splitting the data.",False,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,5,16,retriever,Link,No Link,"The probe's capacity to predict the character sequence is evaluated by counting the exact matches, and with a finer-grained metric measuring overlap.","If the probe can successfully reconstruct the correct character sequence from an unseen token's embedding, then there must be significant orthographic information encoded in the vector.",False,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,5,45,random,No Link,No Link,"The probe's capacity to predict the character sequence is evaluated by counting the exact matches, and with a finer-grained metric measuring overlap.",3,True,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,5,36,llm,Link,Link,"The probe's capacity to predict the character sequence is evaluated by counting the exact matches, and with a finer-grained metric measuring overlap.","For each vocabulary item w in the test set, SpellingBee is given only the corresponding embedding vector e w , and is expected to generate the character sequence w 1 , . . . , w n that defines w. We measure success on the test set using two metrics:",True,True
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,5,37,llm,Link,No Link,"The probe's capacity to predict the character sequence is evaluated by counting the exact matches, and with a finer-grained metric measuring overlap.","exact match (EM), and character ngram overlap score using chrF (Popović, 2015).",False,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,5,82,random,Link,No Link,"The probe's capacity to predict the character sequence is evaluated by counting the exact matches, and with a finer-grained metric measuring overlap.",This result indicates that the model does not utilize the character information injected into the tokens' embeddings.,False,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,5,98,retriever,Link,Link,"The probe's capacity to predict the character sequence is evaluated by counting the exact matches, and with a finer-grained metric measuring overlap.","This trend is particularly sharp when the similarity filter is applied, as the probe is hardly able to spell tokens with 6 or more characters accurately; having said that, the probe is able to generate many partially correct spellings, as measured by chrF (Figure 6).",True,True
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,6,69,mutual,Link,Link,"Compared to a control experiment where the probe is not trained and only randomly initialized, the probe is able to better rebuild character sequences when fed with embeddings from the LMs (up to 30-40% from 0 for exact matches); however, it's performance is weakened when the training part of the vocabulary is filtered (removing token too similar to those in the testing part, or with the same lemma).","We find that more data makes for a better probe, but that even a few thousand examples are enough to train SpellingBee to extract significant character information from the embeddings, which cannot be extracted from randomized vectors (the control).",True,True
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,6,72,random,Link,No Link,"Compared to a control experiment where the probe is not trained and only randomly initialized, the probe is able to better rebuild character sequences when fed with embeddings from the LMs (up to 30-40% from 0 for exact matches); however, it's performance is weakened when the training part of the vocabulary is filtered (removing token too similar to those in the testing part, or with the same lemma).","Our probing experiments reveal that language models learn some partial notion of spelling, despite the lack of direct access to characters.",False,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,6,77,random,No Link,No Link,"Compared to a control experiment where the probe is not trained and only randomly initialized, the probe is able to better rebuild character sequences when fed with embeddings from the LMs (up to 30-40% from 0 for exact matches); however, it's performance is weakened when the training part of the vocabulary is filtered (removing token too similar to those in the testing part, or with the same lemma).","6 We train RoBERTa-Large on English Wikipedia using the hyperparameter configuration of 24hBERT (Izsak et al., 2021), and cease training after 24 hours (∼16000 steps).",True,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,7,3,random,No Link,No Link,"However, using a probe trained on the full vocabulary as a way to initialize a LM does not seem to be useful, as the LM reaches the same training loss as a control one rather quickly.","We probe the embedding layer of pretrained language models and show that models learn the internal character composition of whole word and subword tokens to a surprising extent, without ever seeing the characters coupled with the tokens.",True,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,7,41,retriever,Link,Link,"However, using a probe trained on the full vocabulary as a way to initialize a LM does not seem to be useful, as the LM reaches the same training loss as a control one rather quickly.","We could train a probe with randomly-initialized embeddings (instead of pretrained embeddings from another model) to predict the spelling of all vocabulary items, and use these trained probe embeddings to initialize any target model's embedding layer (instead of random initialization).",True,True
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,7,59,random,No Link,No Link,"However, using a probe trained on the full vocabulary as a way to initialize a LM does not seem to be useful, as the LM reaches the same training loss as a control one rather quickly.",One may suggest that training SpellingBee over 32000 examples may leak information from the test set.,True,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,7,73,llm,Link,Link,"However, using a probe trained on the full vocabulary as a way to initialize a LM does not seem to be useful, as the LM reaches the same training loss as a control one rather quickly.","Therefore, we hypothesize that learning to spell is beneficial for language models, and propose pretraining the embedding layer using a variant of the SpellingBee probe described in Section 2.",True,True
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,9,5,llm,Link,Link,"- This paper poses a research question of great interest, and answers it while carefully considering many possible factors (models, filtering the training vocabulary).","We further test whether enriching subword models with additional character information can improve language modeling, and observe that this method has a near-identical learning curve as training without spelling-based enrichment.",True,True
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,9,46,llm,No Link,No Link,"- This paper poses a research question of great interest, and answers it while carefully considering many possible factors (models, filtering the training vocabulary).","Pretrained Models We probe four pretrained models: RoBERTa-Base and Large (Liu et al., 2019), GPT2-Medium (Radford et al., 2019), and AraBERT-Large (Antoun et al., 2020).",True,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,9,47,mutual,Link,Link,"- This paper poses a research question of great interest, and answers it while carefully considering many possible factors (models, filtering the training vocabulary).","This set introduces some diversity in vocabulary, objective, and scale: the first three models are trained on English corpora, while AraBERT is trained on text in Arabic; GPT2 is an autoregressive language model, while the rest are masked language models; RoBERTa-Base consists of 125M parameters (with 768 dimensions per embedding), while the other models have approximately 350M parameters (with 1024 dimensions per embedding).",True,True
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,9,60,random,No Link,No Link,"- This paper poses a research question of great interest, and answers it while carefully considering many possible factors (models, filtering the training vocabulary).","For example, if dog was seen during training, then spelling out dogs might be easy.",True,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,9,61,retriever,Link,Link,"- This paper poses a research question of great interest, and answers it while carefully considering many possible factors (models, filtering the training vocabulary).","We thus consider the similarity and lemma filters, which remove such near-neighbors from the training set.",True,True
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,9,63,retriever,Link,Link,"- This paper poses a research question of great interest, and answers it while carefully considering many possible factors (models, filtering the training vocabulary).","Finally, we find that the properties of pretrained models also seem to have a significant effect on the amount of spelling information SpellingBee can extract.",True,True
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,9,95,random,No Link,No Link,"- This paper poses a research question of great interest, and answers it while carefully considering many possible factors (models, filtering the training vocabulary).",We analyze the effect of token length on the probe's ability to spell.,True,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,10,42,llm,No Link,No Link,- It investigates a potential application of this answer.,"We experiment with this method in Section 5, but find that it does not have any significant impact on the convergence of language models.",True,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,10,44,retriever,No Link,No Link,- It investigates a potential application of this answer.,"We begin with a series of probing experiments, where we apply SpellingBee to the embedding layer of various pretrained models.",True,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,10,63,random,No Link,No Link,- It investigates a potential application of this answer.,"Finally, we find that the properties of pretrained models also seem to have a significant effect on the amount of spelling information SpellingBee can extract.",True,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,10,99,random,No Link,No Link,- It investigates a potential application of this answer.,Perhaps a less intuitive result is the probe's failure to spell single-character tokens.,True,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,11,15,random,Link,No Link,"- The paper is very clearly written, and easy to follow.","SpellingBee is trained on part of the model's vocabulary, and then tested by spelling unseen token types.",False,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,11,27,random,No Link,No Link,"- The paper is very clearly written, and easy to follow.","Specifically, Spelling-Bee probes the original model's embedding matrix, since spelling is a property of token types, invariant to context.",True,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,13,16,retriever,Link,Link,"- The results given by the probe are a little difficult to interpret; as, while there is a control experiment where the probe has no information, it would be very useful to have an idea of what the probe can do when fed with embeddings that we know contain orthographic information.","If the probe can successfully reconstruct the correct character sequence from an unseen token's embedding, then there must be significant orthographic information encoded in the vector.",True,True
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,13,48,llm,Link,Link,"- The results given by the probe are a little difficult to interpret; as, while there is a control experiment where the probe has no information, it would be very useful to have an idea of what the probe can do when fed with embeddings that we know contain orthographic information.","Control Since SpellingBee is a trained probe, we wish to establish the probe's baseline performance when provided with inputs with no orthographic information.",True,True
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,13,62,llm,Link,No Link,"- The results given by the probe are a little difficult to interpret; as, while there is a control experiment where the probe has no information, it would be very useful to have an idea of what the probe can do when fed with embeddings that we know contain orthographic information.","While results are indeed lower (and probably do account for some level of information leakage), they are still considerably higher than the control, both in terms of EM and chrF. Results using the similarity and lemma filters are rather similar, suggesting that embedding-space similarity captures some information about each token's lemma.",False,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,13,69,retriever,Link,Link,"- The results given by the probe are a little difficult to interpret; as, while there is a control experiment where the probe has no information, it would be very useful to have an idea of what the probe can do when fed with embeddings that we know contain orthographic information.","We find that more data makes for a better probe, but that even a few thousand examples are enough to train SpellingBee to extract significant character information from the embeddings, which cannot be extracted from randomized vectors (the control).",True,True
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,13,93,random,Link,No Link,"- The results given by the probe are a little difficult to interpret; as, while there is a control experiment where the probe has no information, it would be very useful to have an idea of what the probe can do when fed with embeddings that we know contain orthographic information.",Figures 3 and  4 shows the results with and without the similarity filter.,False,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,13,108,random,No Link,No Link,"- The results given by the probe are a little difficult to interpret; as, while there is a control experiment where the probe has no information, it would be very useful to have an idea of what the probe can do when fed with embeddings that we know contain orthographic information.",These are the default hyperpa-  rameters for training a transformer language model in Fairseq .,True,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,14,5,random,Link,No Link,"If testing the probe on static embeddings (word2vec, glove), fasttext embeddings could work; in this setting, I believe uncontextualized embeddings from CharacterBERT (El Boukkouri et al, 2020) could work.","We further test whether enriching subword models with additional character information can improve language modeling, and observe that this method has a near-identical learning curve as training without spelling-based enrichment.",False,False
5b9d91bada37eb8d4cd914403912187721b0f63d27a39a0573dce8d1ba21ebdc9dbfefa6e21284305099ed3e590fe0b0d1b103d4cd10963a1d212ec59ae9dc22,14,40,random,No Link,Link,"If testing the probe on static embeddings (word2vec, glove), fasttext embeddings could work; in this setting, I believe uncontextualized embeddings from CharacterBERT (El Boukkouri et al, 2020) could work.","While we mainly use SpellingBee as a probe, a variation of our method could potentially imbue the embedding layer with character information before training a language model.",False,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,1,3,mutual,Link,Link,"The paper presents NAKDIMON, a character-LSTM Hebrew diacritizer that does not use a morphological analyzer/dictionary.","We present NAKDIMON, a two-layer characterlevel LSTM, that performs on par with much more complicated curation-dependent systems, across a diverse array of modern Hebrew sources.",True,True
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,1,11,random,No Link,No Link,"The paper presents NAKDIMON, a character-LSTM Hebrew diacritizer that does not use a morphological analyzer/dictionary.","In contrast, an automatic system processing Hebrew text may not be as sensitive to this kind of grammatical knowledge and instead interpret the undotted token as the more frequent word in (c), harming downstream performance.",True,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,1,25,mutual,Link,Link,"The paper presents NAKDIMON, a character-LSTM Hebrew diacritizer that does not use a morphological analyzer/dictionary.","Our system, NAKDIMON, accepts the undotted character sequence as its input, consults no external resources or lexical components, and produces diacritics for each character, resulting in dotted text whose quality is comparable to that of the commercial Morfix, on both character-level and word-level accuracy.",True,True
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,1,80,random,No Link,No Link,"The paper presents NAKDIMON, a character-LSTM Hebrew diacritizer that does not use a morphological analyzer/dictionary.","These types of errors are more friendly to the typical use cases of a dotting system, as they are likely to stand out to a reader.",True,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,1,94,mutual,Link,Link,"The paper presents NAKDIMON, a character-LSTM Hebrew diacritizer that does not use a morphological analyzer/dictionary.",NAKDIMON demonstrates that a simple architecture for diacritizing Hebrew text as a sequence tagging problem can achieve performance on par with much more complex systems.,True,True
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,2,9,random,No Link,No Link,It compares its performance to SOTA on the task; and presents a new test set for the task that is more diverse that previous sets.,"In NLP systems, recovering such signals is difficult, and indeed their performance on Hebrew tasks is adversely affected by the presence of undotted text (Shacham and Wintner, 2007;Goldberg and Elhadad, 2010;Tsarfaty et al., 2019).",True,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,2,15,random,No Link,No Link,It compares its performance to SOTA on the task; and presents a new test set for the task that is more diverse that previous sets.,"Moreover, the process of manually adding diacritics in either handwritten script or through digital input devices is mechanically cumbersome.",True,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,2,28,mutual,Link,Link,It compares its performance to SOTA on the task; and presents a new test set for the task that is more diverse that previous sets.,"We introduce a novel test set for Modern Hebrew dotting, derived from larger and more diverse sources than existing datasets.",True,True
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,2,41,llm,Link,Link,It compares its performance to SOTA on the task; and presents a new test set for the task that is more diverse that previous sets.,"Therefore, we created a new test set 5 from a larger variety of texts, including high-quality Wikipedia articles and edited news stories, as well as usergenerated blog posts.",True,True
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,2,62,mutual,Link,Link,It compares its performance to SOTA on the task; and presents a new test set for the task that is more diverse that previous sets.,"We compare the performance of NAKDIMON against Dicta (retrieved 2022-1-9), Snopi, 9 and Morfix (Kamir et al., 2002), on our new test set ( §2).",True,True
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,2,108,retriever,Link,No Link,It compares its performance to SOTA on the task; and presents a new test set for the task that is more diverse that previous sets.,"In order to provide fair comparison and to preempt overfitting on this test data, we ran this test in a preliminary setup on a variant of NAKDIMON which was not tuned or otherwise unfairly trained.",False,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,4,9,random,No Link,No Link,The paper is well written and clear.,"In NLP systems, recovering such signals is difficult, and indeed their performance on Hebrew tasks is adversely affected by the presence of undotted text (Shacham and Wintner, 2007;Goldberg and Elhadad, 2010;Tsarfaty et al., 2019).",True,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,4,45,random,No Link,No Link,The paper is well written and clear.,"The LSTM output is fed into a single linear layer, which then feeds three linear layers, one for each diacritic category (see §2).",True,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,5,78,random,No Link,No Link,The work is well motivated and enough related work is presented.,"The largest category for NAKDIMON-only errors (∼18% of 90 sampled) are ones where a fused preposition+determiner character is dotted to only include the preposition, perhaps due to its inability to detect the explicit determiner clitic ‫ה‬ in neighboring words, on which the complex systems apply morphological segmentation.",True,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,5,109,random,No Link,No Link,The work is well motivated and enough related work is presented.,"This system, NAKDIMON 0 , differs from our final variant in three main aspects: it is not trained on the Dicta portion of our training corpus ( §3), it is not trained on the AUTOMATIC corpus, and it employs a residual connection between the two character Bi-LSTM layers.",True,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,6,70,random,No Link,No Link,The error analysis is clear and helpful.,"We optimize using Adam (Kingma and Ba, 2014)",True,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,6,78,random,Link,No Link,The error analysis is clear and helpful.,"The largest category for NAKDIMON-only errors (∼18% of 90 sampled) are ones where a fused preposition+determiner character is dotted to only include the preposition, perhaps due to its inability to detect the explicit determiner clitic ‫ה‬ in neighboring words, on which the complex systems apply morphological segmentation.",False,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,7,67,random,No Link,No Link,This is a good short paper with a negative result.,"We pre-process the input by removing all but Hebrew characters, spaces and punctuation; digits are converted to a dedicated symbol, as are Latin characters.",True,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,7,110,random,No Link,No Link,This is a good short paper with a negative result.,"Testing on the Dicta test set required some minimal evaluation adaptations resulting from encoding constraints (for example, we do not distinguish between kamatz katan and kamatz gadol).",True,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,9,50,random,No Link,Link,The paper presents inconclusive negative results: it is unclear what the results would look like had millions of additional words that are *automatically diacritized* using Dicta are added in Nakdimon's training. The authors added 1.3M such words.,"As we are unaware of legally-obtainable dotted modern corpora, we use a combination of dotted pre-modern texts and semi-automatically dotted modern sources to train NAKDIMON:",False,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,9,66,llm,No Link,No Link,The paper presents inconclusive negative results: it is unclear what the results would look like had millions of additional words that are *automatically diacritized* using Dicta are added in Nakdimon's training. The authors added 1.3M such words.,"8 www.shortstoryproject.com/he/ 9 http://www.nakdan.com/Nakdan.aspx 10 Results on Dicta's test set (Shmidman et al., 2020)   We train NAKDIMON over PRE-MODERN for a single epoch, followed by two epochs over the AU-TOMATIC corpus, and then by three epochs over the MODERN corpus.",True,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,9,97,random,Link,No Link,The paper presents inconclusive negative results: it is unclear what the results would look like had millions of additional words that are *automatically diacritized* using Dicta are added in Nakdimon's training. The authors added 1.3M such words.,"We cannot report our full NAKDIMON's performance on the former, as we use the test set for parts of its training.",False,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,9,102,retriever,No Link,No Link,The paper presents inconclusive negative results: it is unclear what the results would look like had millions of additional words that are *automatically diacritized* using Dicta are added in Nakdimon's training. The authors added 1.3M such words.,"While developing NAKDIMON, we performed several evaluations over a held-out validation set of 40 documents with 27,681 tokens, on which Dicta performs at 91.56% WOR accuracy.",True,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,10,36,random,No Link,No Link,I think an additional experiment that uses more words (automatically diacritized) or just generated word forms from a morphological dictionary can make the result more convincing (negative or positive it may be).,"For the sake of input integrity, and unlike some other systems, we opt not to remove these characters, but instead employ a dotting policy consistent with full script.",True,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,10,47,random,No Link,No Link,I think an additional experiment that uses more words (automatically diacritized) or just generated word forms from a morphological dictionary can make the result more convincing (negative or positive it may be).,"In training, we sum the cross-entropy loss from all categories.",True,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,12,70,random,No Link,No Link,- it would help to report OOV rates; and performance on in-vocabulary vs OOV for your system.,"We optimize using Adam (Kingma and Ba, 2014)",True,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,12,77,random,No Link,No Link,- it would help to report OOV rates; and performance on in-vocabulary vs OOV for your system.,"In Table 4 we present examples of words dotted incorrectly, or correctly, only by NAKDIMON, compared with Morfix and Dicta.",True,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,13,44,random,No Link,No Link,- the discussion of Arabic use of diacritization is not accurate.,"NAKDIMON embeds the input characters and passes them through a two-layer Bi-LSTM (Hochreiter and Schmidhuber, 1997).",True,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,13,85,llm,Link,Link,- the discussion of Arabic use of diacritization is not accurate.,"In Arabic, diacritization serves a comparable purpose to that in Hebrew, but not exclusively: most diacritic marks differentiate letters from each other (which only the sin/shin dot does in Hebrew), while vocalization marks are in a one-to-one relationship with their phonetic realizations.",True,True
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,13,89,retriever,No Link,Link,- the discussion of Arabic use of diacritization is not accurate.,"Mubarak et al. (2019) tackled Arabic diacritization as a sequenceto-sequence problem, tasking the model with reproducing the characters as well as the marks.",False,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,13,94,random,Link,Link,- the discussion of Arabic use of diacritization is not accurate.,NAKDIMON demonstrates that a simple architecture for diacritizing Hebrew text as a sequence tagging problem can achieve performance on par with much more complex systems.,True,True
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,14,63,random,No Link,Link,"Arabic ""dots"" are not optional in common use of Arabic; diacritical marks (vowels, nunation, gemination) are. Check out https://aclanthology.org/N07-2014.pdf https://aclanthology.org/2007.mtsummit-papers.20.pdf","10 We report the following metrics: decision accuracy (DEC) is computed over the entire set of individual possible decisions: dagesh/mappiq for letters that allow it, sin/shin dot for the letter ‫,ש‬ and all other diacritics for letters that allow them; char-6 Obtained from the Ben-Yehuda Project www.benyehuda.org.",False,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,14,85,mutual,Link,Link,"Arabic ""dots"" are not optional in common use of Arabic; diacritical marks (vowels, nunation, gemination) are. Check out https://aclanthology.org/N07-2014.pdf https://aclanthology.org/2007.mtsummit-papers.20.pdf","In Arabic, diacritization serves a comparable purpose to that in Hebrew, but not exclusively: most diacritic marks differentiate letters from each other (which only the sin/shin dot does in Hebrew), while vocalization marks are in a one-to-one relationship with their phonetic realizations.",True,True
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,14,92,random,No Link,No Link,"Arabic ""dots"" are not optional in common use of Arabic; diacritical marks (vowels, nunation, gemination) are. Check out https://aclanthology.org/N07-2014.pdf https://aclanthology.org/2007.mtsummit-papers.20.pdf",Conclusion,True,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,15,24,random,No Link,No Link,- I am puzzled by a difference between the Dicta test set and the new test set: the difference between CHA and WOR for the Dicta test set is much bigger that the respective difference in the new test set (between 1.4 and 2.4 times bigger). Any thoughts on that?,"3 In this work, we set out to simplify the dotting task as much as possible to standard modules.",True,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,15,61,random,No Link,No Link,- I am puzzled by a difference between the Dicta test set and the new test set: the difference between CHA and WOR for the Dicta test set is much bigger that the respective difference in the new test set (between 1.4 and 2.4 times bigger). Any thoughts on that?,Experiments,True,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,15,62,retriever,No Link,No Link,- I am puzzled by a difference between the Dicta test set and the new test set: the difference between CHA and WOR for the Dicta test set is much bigger that the respective difference in the new test set (between 1.4 and 2.4 times bigger). Any thoughts on that?,"We compare the performance of NAKDIMON against Dicta (retrieved 2022-1-9), Snopi, 9 and Morfix (Kamir et al., 2002), on our new test set ( §2).",True,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,15,74,llm,No Link,No Link,- I am puzzled by a difference between the Dicta test set and the new test set: the difference between CHA and WOR for the Dicta test set is much bigger that the respective difference in the new test set (between 1.4 and 2.4 times bigger). Any thoughts on that?,"We note the substantial improvement our model achieves on the VOC metric compared to the WOR metric: 18.43% of word-level errors are attributable to vocalization-agnostic dotting, compared to 13.80% for Dicta and 10.41% for Snopi (but 20.91% for Morfix).",True,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,17,40,random,No Link,No Link,,"However, it is relatively small and nondiverse: all 22 documents in the dataset originate in a single source, namely Hebrew Wikipedia articles.",True,False
df4051328ee57291cf600f10ef67af6872fcef0deb3ff7323b89142b68beb16ea1fbe09c44be8ccca24fe30d67a1e6dbfb715a77384c30fbeb37c362c25c743c,17,67,random,No Link,No Link,,"We pre-process the input by removing all but Hebrew characters, spaces and punctuation; digits are converted to a dedicated symbol, as are Latin characters.",True,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,1,10,random,No Link,No Link,"This paper extends the worst-case-aware curriculum learning work (Zhang et al., 2020) to zero-shot dependency parsing.","There is, however, a sharp divide between languages that benefit from this transfer and languages that do not, and there is ample evidence that transfer works best between typologically similar languages (Pires et al., 2019).",True,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,1,24,retriever,Link,Link,"This paper extends the worst-case-aware curriculum learning work (Zhang et al., 2020) to zero-shot dependency parsing.",Our research question can be formulated as such: Can worst-case aware automated curriculum learning improve zero-shot dependency parsing?,True,True
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,1,27,llm,Link,Link,"This paper extends the worst-case-aware curriculum learning work (Zhang et al., 2020) to zero-shot dependency parsing.","where l i is the loss of task i. The architecture we use in this paper is adapted from Zhang et al. (2020), which is an automated curriculum learning (Graves et al., 2017) framework to learn a worstcase-aware loss in a multi-task learning scenario.",True,True
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,1,46,llm,Link,Link,"This paper extends the worst-case-aware curriculum learning work (Zhang et al., 2020) to zero-shot dependency parsing.","Zhang et al. ( 2020) present a stochastic generalization of the L ∞ loss summarization and a practical approach to minimizing this family of losses through automated curriculum learning (Graves et al., 2017): The core idea behind their generalization is to optimize the worst-case loss with a certain probability, otherwise optimize the average (loss-proportional) loss with the remaining probability.",True,True
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,1,75,retriever,Link,Link,"This paper extends the worst-case-aware curriculum learning work (Zhang et al., 2020) to zero-shot dependency parsing.",This answers positively our research question Can worst-case aware automated curriculum learning improve zero-shot dependency parsing?,True,True
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,1,82,random,No Link,No Link,"This paper extends the worst-case-aware curriculum learning work (Zhang et al., 2020) to zero-shot dependency parsing.","We can, however, easily construct samples that are not representative, for example, by taking a sample of related languages.",True,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,2,15,random,No Link,No Link,The key idea is to control the sampling to sample more from languages with higher losses.,"However, this is difficult to achieve in practice, as multilingual datasets are not well balanced for typological diversity and contain a skewed distribution of typological features (Ponti et al., 2021).",True,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,2,35,llm,Link,Link,The key idea is to control the sampling to sample more from languages with higher losses.,"The sampler should choose bandits that have higher rewards, and in our scenario, data batches that have a higher loss on the model are more likely to be selected by the sampler and therefore, in a later stage, used by the trainer.",True,True
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,2,60,random,No Link,No Link,The key idea is to control the sampling to sample more from languages with higher losses.,Experiments,True,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,2,107,retriever,Link,Link,The key idea is to control the sampling to sample more from languages with higher losses.,"We found this method to improve dependency parsing on a sample of 30 test languages in the zeroshot setting, compared to sampling data uniformly across treebanks from different languages, or proportionally to the size of the treebanks.",True,True
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,3,14,random,No Link,No Link,"Empirical analyses show that the proposed method improves zero-shot accuracy on unseen languages, outperforming uniform sampling or sampling proportionally to the size of the treebanks.",Large multilingual PLMs are typically fine-tuned using training data from a sample of languages that is supposed to be representative of the languages that the models are later applied to.,True,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,3,60,random,No Link,No Link,"Empirical analyses show that the proposed method improves zero-shot accuracy on unseen languages, outperforming uniform sampling or sampling proportionally to the size of the treebanks.",Experiments,True,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,3,107,mutual,Link,Link,"Empirical analyses show that the proposed method improves zero-shot accuracy on unseen languages, outperforming uniform sampling or sampling proportionally to the size of the treebanks.","We found this method to improve dependency parsing on a sample of 30 test languages in the zeroshot setting, compared to sampling data uniformly across treebanks from different languages, or proportionally to the size of the treebanks.",True,True
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,5,50,random,No Link,No Link,- Zero-shot dependency parsing is a difficult but valuable task in both academia and industry.,"If p < φ the model choose the maximum loss among all tasks, otherwise, it randomly chooses one loss according to the loss distribution.",True,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,5,109,random,No Link,No Link,- Zero-shot dependency parsing is a difficult but valuable task in both academia and industry.,This leaves open questions about the relationship between the languages used for training and the ones used for testing.,True,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,6,81,random,No Link,No Link,- The proposed method is intuitive.,"(de Lhoneux et al., 2017;Schluter and Agić, 2017;de Lhoneux, 2019).",True,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,6,83,random,No Link,No Link,- The proposed method is intuitive.,We expect worst-case aware learning to lead to larger improvements in cases where some language types are underrepresented in the sample.,True,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,7,14,random,No Link,Link,"- The empirical analyses were conducted on a large set of languages, which makes the results more convincible.",Large multilingual PLMs are typically fine-tuned using training data from a sample of languages that is supposed to be representative of the languages that the models are later applied to.,False,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,7,64,llm,Link,No Link,"- The empirical analyses were conducted on a large set of languages, which makes the results more convincible.","For testing, they use 30 test sets from treebanks whose language has not been seen at finetuning time.",False,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,7,98,retriever,No Link,No Link,"- The empirical analyses were conducted on a large set of languages, which makes the results more convincible.",These results could be due to the different scripts of the languages involved both in training and testing.,True,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,7,99,retriever,No Link,Link,"- The empirical analyses were conducted on a large set of languages, which makes the results more convincible.","Looking at results of the different models on individual test languages (see Figure 1 in Appendix C), we find no clear pattern of the settings in which this method works best.",False,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,7,103,random,No Link,No Link,"- The empirical analyses were conducted on a large set of languages, which makes the results more convincible.","For Swiss German, worst-case learning helps least when using the GERMANIC sample where it is less of an outlier.",True,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,7,107,llm,Link,No Link,"- The empirical analyses were conducted on a large set of languages, which makes the results more convincible.","We found this method to improve dependency parsing on a sample of 30 test languages in the zeroshot setting, compared to sampling data uniformly across treebanks from different languages, or proportionally to the size of the treebanks.",False,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,10,3,random,No Link,No Link,It seems the results on mBERT and XLM-R give different conclusions.,"However, source and training languages are rarely related, when parsing truly low-resource languages.",True,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,10,39,random,No Link,No Link,It seems the results on mBERT and XLM-R give different conclusions.,"Worst-case-aware risk minimization In multilingual and multi-task learning scenarios, in which we jointly minimize our risk across n languages or tasks, we are confronted with the question of how to summarize n losses.",True,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,10,65,retriever,Link,Link,It seems the results on mBERT and XLM-R give different conclusions.,We use the same training and test sets and experiment both with mBERT and XLM-R as PLMs.,True,True
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,10,76,llm,Link,Link,It seems the results on mBERT and XLM-R give different conclusions.,Our results using mBERT are more than 1 LAS point above the corresponding baselines.,True,True
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,10,78,llm,Link,Link,It seems the results on mBERT and XLM-R give different conclusions.,The results with XLM-R are much higher in general 2 but the trends are similar: all our models outperform all of our baselines albeit with smaller differences (there is only a 0.4 LAS difference between our best model and the best baseline).,True,True
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,10,111,retriever,Link,Link,It seems the results on mBERT and XLM-R give different conclusions.,"2020) using mBERT and XLM-R. S-P=size-proportional, S-S = smooth-sampling, U=uniform.",True,True
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,11,70,random,No Link,No Link,The relatively small differences between $\phi=0$ and $\phi=1$ in the mBERT and XLM-R settings also make me wonder if the differences are statistically significant.,"We implement all models using MaChAmp (van der Goot et al., 2021), a library for multi-task learning based on AllenNLP (Gardner et al., 2018).",True,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,11,78,llm,Link,Link,The relatively small differences between $\phi=0$ and $\phi=1$ in the mBERT and XLM-R settings also make me wonder if the differences are statistically significant.,The results with XLM-R are much higher in general 2 but the trends are similar: all our models outperform all of our baselines albeit with smaller differences (there is only a 0.4 LAS difference between our best model and the best baseline).,True,True
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,11,97,random,No Link,No Link,The relatively small differences between $\phi=0$ and $\phi=1$ in the mBERT and XLM-R settings also make me wonder if the differences are statistically significant.,"Additionally, there are slightly more gains from using worst-case aware learning with the SLAVIC sample than for our typologically diverse sample.",True,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,11,110,retriever,No Link,Link,The relatively small differences between $\phi=0$ and $\phi=1$ in the mBERT and XLM-R settings also make me wonder if the differences are statistically significant.,mBERT XLM-R iso φ=0 φ=0.5 φ=1 S-P S-S U φ=0 φ=0.,False,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,12,15,random,No Link,No Link,"- The proposed work is an extension of previous Zhang et al., 2020. So the contribution seems a bit incremental.","However, this is difficult to achieve in practice, as multilingual datasets are not well balanced for typological diversity and contain a skewed distribution of typological features (Ponti et al., 2021).",True,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,12,17,mutual,Link,Link,"- The proposed work is an extension of previous Zhang et al., 2020. So the contribution seems a bit incremental.",Zhang et al. (2020) recently developed such a method.,True,True
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,12,55,random,No Link,No Link,"- The proposed work is an extension of previous Zhang et al., 2020. So the contribution seems a bit incremental.","At each round, the policy of the task that is selected by the trainer receives positive rewards and the policy of all other tasks that have been selected by the sampler receive negative rewards.",True,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,17,15,random,No Link,No Link,What is the $\phi$?,"However, this is difficult to achieve in practice, as multilingual datasets are not well balanced for typological diversity and contain a skewed distribution of typological features (Ponti et al., 2021).",True,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,17,27,random,No Link,No Link,What is the $\phi$?,"where l i is the loss of task i. The architecture we use in this paper is adapted from Zhang et al. (2020), which is an automated curriculum learning (Graves et al., 2017) framework to learn a worstcase-aware loss in a multi-task learning scenario.",True,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,17,47,mutual,Link,No Link,What is the $\phi$?,The hyperparameter φ is introduced by the worst-case-aware risk minimization to trade off the balance between the worst-case and the lossproportional losses.,False,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,17,52,mutual,Link,No Link,What is the $\phi$?,"On the contrary, if φ = 0, the trainer loss-proportionally samples one loss.",False,False
cde23afeacf47981e351e407439745af0766bb60647e66831b93b3233e4a8ec34ecedfc200c7516b67a24bbeaa9fb7b93abfef3cc66e8d3b32693ca75d22f0a3,17,110,mutual,No Link,No Link,What is the $\phi$?,mBERT XLM-R iso φ=0 φ=0.5 φ=1 S-P S-S U φ=0 φ=0.,True,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,1,2,mutual,Link,No Link,"This paper proposed how to adapt the graph-based semantic parser PERIN to the task of structured sentiment analysis, aiming to analyze a polar expression, an optional holder, an optional sentiment target, and sentiment polarity.","This paper demonstrates how a graph-based semantic parser can be applied to the task of structured sentiment analysis, directly predicting sentiment graphs from text.",False,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,1,6,mutual,Link,Link,"This paper proposed how to adapt the graph-based semantic parser PERIN to the task of structured sentiment analysis, aiming to analyze a polar expression, an optional holder, an optional sentiment target, and sentiment polarity.","The task of structured sentiment analysis (SSA) is aimed at locating all opinion tuples within a sentence, where a single opinion contains a) a polar expression, b) an optional holder, c) an optional sentiment target, and d) a positive, negative or neutral polarity.",True,True
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,1,13,random,No Link,No Link,"This paper proposed how to adapt the graph-based semantic parser PERIN to the task of structured sentiment analysis, aiming to analyze a polar expression, an optional holder, an optional sentiment target, and sentiment polarity.","Figure 1: A sentiment graph for the phrase ""I actually enjoyed the bad acting"", which contains an example of nesting of two opposing opinions.",True,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,1,21,llm,Link,Link,"This paper proposed how to adapt the graph-based semantic parser PERIN to the task of structured sentiment analysis, aiming to analyze a polar expression, an optional holder, an optional sentiment target, and sentiment polarity.","In this paper, we adopt PERIN (Samuel and Straka, 2020), a state-of-theart graph-based parser capable of modeling a superset of graph features needed for our task.",True,True
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,1,80,retriever,Link,No Link,"This paper proposed how to adapt the graph-based semantic parser PERIN to the task of structured sentiment analysis, aiming to analyze a polar expression, an optional holder, an optional sentiment target, and sentiment polarity.","Previous work cast the task of structured sentiment analysis (SSA) as dependency parsing, converting the sentiment graphs into lossy dependency graphs.",False,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,1,84,random,No Link,No Link,"This paper proposed how to adapt the graph-based semantic parser PERIN to the task of structured sentiment analysis, aiming to analyze a polar expression, an optional holder, an optional sentiment target, and sentiment polarity.","Specifically, the most direct opinion-tuple encoding provides the highest performance gains.",True,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,2,9,random,No Link,No Link,"Based on PERIN, the weighted bipartite graph between all queries and nodes is applied to indicate the prior ordering of the graph.","Dependency parsing approaches have recently shown promising results for SSA (Barnes et al., 2021;Peng et al., 2021).",True,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,2,31,mutual,Link,Link,"Based on PERIN, the weighted bipartite graph between all queries and nodes is applied to indicate the prior ordering of the graph.","Based on the predicted probabilities of labels and anchors, we create a weighted bipartite graph between all queries and nodes.",True,True
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,2,104,random,No Link,No Link,"Based on PERIN, the weighted bipartite graph between all queries and nodes is applied to indicate the prior ordering of the graph.","When one system is significantly better in 15 out of the 25 comparisons, and additionally significantly better in the first joint test, we finally mark it as significantly better.",True,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,3,3,mutual,Link,Link,The proposed method advances the-state-of-the art method on 4 out of 5 standard benchmark sets.,We advance the state of the art on 4 out of 5 standard benchmark sets.,True,True
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,3,10,random,No Link,No Link,The proposed method advances the-state-of-the art method on 4 out of 5 standard benchmark sets.,"Here we present a novel sentiment parser which, unlike previous attempts, predicts sentiment graphs directly from text without reliance on heuristic lossy conversions to intermediate dependency representations.",True,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,3,15,random,No Link,No Link,The proposed method advances the-state-of-the art method on 4 out of 5 standard benchmark sets.,"Proposing a dependency parsing approach to the full task of SSA, Barnes et al. (2021) show that it leads to strong improvements over state-of-the-art baselines.",True,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,3,83,mutual,Link,No Link,The proposed method advances the-state-of-the art method on 4 out of 5 standard benchmark sets.,"The results suggest that our approach to SSA has clear performance benefits, advancing the state of the art on four out of five commonly used benchmarks.",False,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,5,2,mutual,Link,Link,"1) This work applies the graph-based semantic parser PERIN for structured sentiment analysis with task-specific adaption, which refines the parallel queries process and gold nodes mapping.","This paper demonstrates how a graph-based semantic parser can be applied to the task of structured sentiment analysis, directly predicting sentiment graphs from text.",True,True
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,5,11,llm,Link,Link,"1) This work applies the graph-based semantic parser PERIN for structured sentiment analysis with task-specific adaption, which refines the parallel queries process and gold nodes mapping.","The model takes inspiration from successful work in meaning representation parsing, and in particular the permutation-invariant graph-based parser of Samuel and Straka (2020) called PERIN.",True,True
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,5,21,mutual,Link,Link,"1) This work applies the graph-based semantic parser PERIN for structured sentiment analysis with task-specific adaption, which refines the parallel queries process and gold nodes mapping.","In this paper, we adopt PERIN (Samuel and Straka, 2020), a state-of-theart graph-based parser capable of modeling a superset of graph features needed for our task.",True,True
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,5,78,random,No Link,No Link,"1) This work applies the graph-based semantic parser PERIN for structured sentiment analysis with task-specific adaption, which refines the parallel queries process and gold nodes mapping.","This is further supported by the fact that the highest gains are found on the datasets with the most nested sentiment expressions and dependency arcs lost due to overlap, which are difficult to encode in bi-lexical graphs (see Appendix A).",True,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,5,80,retriever,No Link,No Link,"1) This work applies the graph-based semantic parser PERIN for structured sentiment analysis with task-specific adaption, which refines the parallel queries process and gold nodes mapping.","Previous work cast the task of structured sentiment analysis (SSA) as dependency parsing, converting the sentiment graphs into lossy dependency graphs.",True,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,5,107,random,No Link,No Link,"1) This work applies the graph-based semantic parser PERIN for structured sentiment analysis with task-specific adaption, which refines the parallel queries process and gold nodes mapping.","The trainable parameters are updated with the AdamW optimizer (Loshchilov and Hutter, 2019), and their learning rate is linearly warmed-up for the first 10% of the training to improve stability, and then decayed with a cosine schedule.",True,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,6,70,random,No Link,No Link,2) The paper is clearly written and well structured.,"In this section, we aim to isolate the effect of predicting intermediate dependency graphs vs. directly predicting sentiment graphs by creating more comparable dependency 2 and PERIN models.",True,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,6,78,random,No Link,No Link,2) The paper is clearly written and well structured.,"This is further supported by the fact that the highest gains are found on the datasets with the most nested sentiment expressions and dependency arcs lost due to overlap, which are difficult to encode in bi-lexical graphs (see Appendix A).",True,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,7,67,random,No Link,No Link,"Given the page limitation of a short paper, this paper still provides abundant information.","This suggests that the main benefit of PERIN is at the structural level, rather than local extraction.",True,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,7,86,random,Link,No Link,"Given the page limitation of a short paper, this paper still provides abundant information.","We will release the source code, models and predictions in the camera-ready version of this paper at https://github.com/censored/for-review.",False,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,8,3,llm,No Link,Link,3) The results of performance evaluation are quite convincing for the comparison with state-of-the-art methods and complete experimental settings.,We advance the state of the art on 4 out of 5 standard benchmark sets.,False,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,8,63,random,No Link,No Link,3) The results of performance evaluation are quite convincing for the comparison with state-of-the-art methods and complete experimental settings.,Table 2 shows the main results.,True,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,8,82,llm,No Link,No Link,3) The results of performance evaluation are quite convincing for the comparison with state-of-the-art methods and complete experimental settings.,We adapted a state-ofthe-art meaning representation parser to SSA and experimentally evaluated three candidate graph encodings of the sentiment structures.,True,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,8,83,mutual,Link,Link,3) The results of performance evaluation are quite convincing for the comparison with state-of-the-art methods and complete experimental settings.,"The results suggest that our approach to SSA has clear performance benefits, advancing the state of the art on four out of five commonly used benchmarks.",True,True
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,8,98,random,No Link,No Link,3) The results of performance evaluation are quite convincing for the comparison with state-of-the-art methods and complete experimental settings.,We found out that the official data published at https://competitions.codalab.org/ competitions/33556 was slightly changed from the data used in previous related work.,True,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,8,100,retriever,No Link,No Link,3) The results of performance evaluation are quite convincing for the comparison with state-of-the-art methods and complete experimental settings.,We re-run the experiments for the comparable baseline model and show the performance differences in Table 8.,True,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,8,101,retriever,Link,Link,3) The results of performance evaluation are quite convincing for the comparison with state-of-the-art methods and complete experimental settings.,"In order to see whether the performance differences for the experiments are significant, we do bootstrap significance testing Berg-Kirkpatrick et al. (2012), combining two variations.",True,True
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,10,36,random,No Link,No Link,"1) Lacking innovation is the main weakness of this paper, though proven the usefulness of PERIN and the refinement.","In parsing the abstract sentiment structures, there are several possible lossless graph encodings depending on the positioning of the polarity information and the sentiment node type (see Figure 3):",True,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,10,47,random,No Link,No Link,"1) Lacking innovation is the main weakness of this paper, though proven the usefulness of PERIN and the refinement.","The parser utilizes a multiclass node head and three anchor classifiers, it does not need an edge classifier.",True,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,11,65,mutual,No Link,No Link,"2) Due to the page limitation, this work lacks detailed analysis on Node-centric encoding, Labeled-edge encoding and Opinion-tuple encoding, which are the essential design towards SSA.","The opinion-tuple encoding gives the best performance on SF 1 (an average of 6.2 percentage points (pp.) better than Peng et al. ( 2021)), followed by the labeled edge encoding (3.0) and finally the node-centric encoding (2.1).",True,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,11,66,mutual,No Link,Link,"2) Due to the page limitation, this work lacks detailed analysis on Node-centric encoding, Labeled-edge encoding and Opinion-tuple encoding, which are the essential design towards SSA.","For extracting spans, the opinion tuple encoding also achieves the the best results on NoReC, either labeled-edge or node centric on CA and MPQA, while Peng et al. ( 2021) is best on EU and DSU.",False,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,11,84,mutual,No Link,No Link,"2) Due to the page limitation, this work lacks detailed analysis on Node-centric encoding, Labeled-edge encoding and Opinion-tuple encoding, which are the essential design towards SSA.","Specifically, the most direct opinion-tuple encoding provides the highest performance gains.",True,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,11,90,random,No Link,No Link,"2) Due to the page limitation, this work lacks detailed analysis on Node-centric encoding, Labeled-edge encoding and Opinion-tuple encoding, which are the essential design towards SSA.","To be concrete, we can use the running example ""I actually enjoyed the bad acting"", which has two opinions with nested targets ""the bad acting"" and ""acting"".",True,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,11,113,random,No Link,No Link,"2) Due to the page limitation, this work lacks detailed analysis on Node-centric encoding, Labeled-edge encoding and Opinion-tuple encoding, which are the essential design towards SSA.",We made five runs from different seeds for each reported value to better estimate the expected error.,True,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,14,7,llm,Link,Link,2) It would be better to present a few case studies for readers.,An example is provided in Figure 1.,True,True
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,14,41,random,No Link,No Link,2) It would be better to present a few case studies for readers.,"Labeled-edge encoding, with deduplicated unlabeled nodes and labeled arcs.",True,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,14,45,random,No Link,No Link,2) It would be better to present a few case studies for readers.,"Opinion-tuple encoding, which represents the structured sentiment information as a sequence of opinion four-tuples.",True,False
48605528fab850fff3a24d4262a276f9da6df7743fb50b7d841d84ecd42e33d58143bdba4d9878d91773d149908ea2ec4fe30dd08c2a8eae5ab7ffbe4c3643d0,14,104,retriever,No Link,No Link,2) It would be better to present a few case studies for readers.,"When one system is significantly better in 15 out of the 25 comparisons, and additionally significantly better in the first joint test, we finally mark it as significantly better.",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,1,2,retriever,Link,Link,The present paper investigates the Word-in-Context task in a few-shot setting.,"As a recent development in few-shot learning, prompt-based techniques have demonstrated promising potential in a variety of natural language processing tasks.",True,True
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,1,10,retriever,Link,Link,The present paper investigates the Word-in-Context task in a few-shot setting.,"The current dominant few-shot approach is the so-called promptbased learning which involves a simple reformulation of the target task as a cloze-style (Taylor, 1953) fill-in-the-blank objective.",True,True
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,1,14,random,Link,Link,The present paper investigates the Word-in-Context task in a few-shot setting.,"Prompt-based techniques have shown impressive performance in the few-shot setting, especially when compared to standard fine-tuning on datasets of hundreds of data points (Scao and Rush, 2021).",True,True
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,1,15,mutual,No Link,No Link,The present paper investigates the Word-in-Context task in a few-shot setting.,"However, surprisingly, the Word-in-Context task (Pilehvar and Camacho-Collados, 2019) -one of the tasks in the SuperGLUE benchmark )-is one exception on which these methods fail to stay on par with their fine-tuned counterparts.",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,1,47,llm,Link,Link,The present paper investigates the Word-in-Context task in a few-shot setting.,"SP for WiC. The surprising failure of existing prompt-based techniques on the Word-in-Context task (Pilehvar and Camacho-Collados, 2019, WiC), motivated us to focus on filling this gap.",True,True
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,1,83,random,No Link,Link,The present paper investigates the Word-in-Context task in a few-shot setting.,Results,False,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,1,111,llm,Link,Link,The present paper investigates the Word-in-Context task in a few-shot setting.,"In this work we showed that similarity based approach to promptbased learning is capable of achieving comparable results to purely fine-tuning based methods on Word-in-Context task, in which previous few-shot attempts have failed.",True,True
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,2,15,random,No Link,No Link,"Utilising a prompt-based approach, rather than optimising models to predict the expected label (i.e. whether two word senses are different), this paper proposes to predict (embeddings of) words synonymous to those in question and compare the distances of these embeddings.","However, surprisingly, the Word-in-Context task (Pilehvar and Camacho-Collados, 2019) -one of the tasks in the SuperGLUE benchmark )-is one exception on which these methods fail to stay on par with their fine-tuned counterparts.",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,2,33,retriever,Link,No Link,"Utilising a prompt-based approach, rather than optimising models to predict the expected label (i.e. whether two word senses are different), this paper proposes to predict (embeddings of) words synonymous to those in question and compare the distances of these embeddings.","Existing methods often pick a set of one or few word predictions as a representative for each class, utilizing the language model's response in a sub-optimal manner.",False,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,2,40,llm,No Link,No Link,"Utilising a prompt-based approach, rather than optimising models to predict the expected label (i.e. whether two word senses are different), this paper proposes to predict (embeddings of) words synonymous to those in question and compare the distances of these embeddings.",This is done by giving the generated prompts to the PLM as input and obtaining its contextualized embedding at the MASK index.,True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,2,53,llm,No Link,No Link,"Utilising a prompt-based approach, rather than optimising models to predict the expected label (i.e. whether two word senses are different), this paper proposes to predict (embeddings of) words synonymous to those in question and compare the distances of these embeddings.","Next the prompts are separately fed to PLM, resulting in a pair of mask embeddings as PLM's response.",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,2,62,random,No Link,No Link,"Utilising a prompt-based approach, rather than optimising models to predict the expected label (i.e. whether two word senses are different), this paper proposes to predict (embeddings of) words synonymous to those in question and compare the distances of these embeddings.","PET (Schick and Schütze, 2021b) prefers ALBERT-xxlarge-v2 (Lan et al., 2019) over RoBERTa (with an average gain of 8 points on a subset of SuperGLUE tasks) and fine-tunes it with manually engineered cloze-style prompts.",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,2,111,retriever,Link,Link,"Utilising a prompt-based approach, rather than optimising models to predict the expected label (i.e. whether two word senses are different), this paper proposes to predict (embeddings of) words synonymous to those in question and compare the distances of these embeddings.","In this work we showed that similarity based approach to promptbased learning is capable of achieving comparable results to purely fine-tuning based methods on Word-in-Context task, in which previous few-shot attempts have failed.",True,True
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,3,12,retriever,Link,No Link,Models optimised in this way reach the performance of models finetuned on the whole training set and the approach is argued to generalise to some other tasks.,"This paradigm has proven its effectiveness in the few-shot setting, even for relatively smaller models, such as BERT (Devlin et al., 2019) and RoBERTA , when combined with ensembling and fine-tuning (Schick and Schütze, 2021a).",False,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,3,15,random,Link,Link,Models optimised in this way reach the performance of models finetuned on the whole training set and the approach is argued to generalise to some other tasks.,"However, surprisingly, the Word-in-Context task (Pilehvar and Camacho-Collados, 2019) -one of the tasks in the SuperGLUE benchmark )-is one exception on which these methods fail to stay on par with their fine-tuned counterparts.",True,True
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,3,26,llm,Link,Link,Models optimised in this way reach the performance of models finetuned on the whole training set and the approach is argued to generalise to some other tasks.,"The experimental results on the WiC dataset shows that, with only 16 instances per class, our proposed prompt-based technique can achieve comparable results to the fine-tuned models (with access to full training data of 2700+ instances per class).",True,True
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,3,69,retriever,No Link,No Link,Models optimised in this way reach the performance of models finetuned on the whole training set and the approach is argued to generalise to some other tasks.,The approach makes use of full training set to optimize discrete prompts for each specific target task.,True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,3,86,random,Link,No Link,Models optimised in this way reach the performance of models finetuned on the whole training set and the approach is argued to generalise to some other tasks.,"This observation suggests that PLMs already encode a certain amount of task-related knowledge and the supervised fine-tuning mainly updates their task description (i.e., what the task is, not how to solve it).",False,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,3,111,llm,Link,Link,Models optimised in this way reach the performance of models finetuned on the whole training set and the approach is argued to generalise to some other tasks.,"In this work we showed that similarity based approach to promptbased learning is capable of achieving comparable results to purely fine-tuning based methods on Word-in-Context task, in which previous few-shot attempts have failed.",True,True
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,5,5,llm,Link,Link,"The method is surprisingly simple, the results are convincing as they significantly improve upon the state-of-the-art on the WiC task in the few-shot setting.","Trying to fill this gap, we propose a new prompting technique, based on similarity metrics, which boosts few-shot performance to the level of fully supervised methods.",True,True
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,5,18,retriever,No Link,Link,"The method is surprisingly simple, the results are convincing as they significantly improve upon the state-of-the-art on the WiC task in the few-shot setting.",The natural question that arises here is if the failure of few-shot techniques on WiC is due to lack of relevant encoded knowledge in PLMs or the inefficiency of the employed prompt-based methods.,False,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,5,24,retriever,No Link,No Link,"The method is surprisingly simple, the results are convincing as they significantly improve upon the state-of-the-art on the WiC task in the few-shot setting.","Given the comparison-based nature of WiC, we hypothesize that conventional prompting methods fall short since they only utilize a single prompt response.",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,5,26,mutual,Link,Link,"The method is surprisingly simple, the results are convincing as they significantly improve upon the state-of-the-art on the WiC task in the few-shot setting.","The experimental results on the WiC dataset shows that, with only 16 instances per class, our proposed prompt-based technique can achieve comparable results to the fine-tuned models (with access to full training data of 2700+ instances per class).",True,True
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,5,87,llm,Link,Link,"The method is surprisingly simple, the results are convincing as they significantly improve upon the state-of-the-art on the WiC task in the few-shot setting.","Therefore, using limited examples in the fewshot setting they are able to reach their maximum fine-tuning potential on WiC. We report SP's performance on WiC for other PLMs in the Appendix which shows our method/observation does not depend on a specific PLM.",True,True
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,5,92,random,No Link,No Link,"The method is surprisingly simple, the results are convincing as they significantly improve upon the state-of-the-art on the WiC task in the few-shot setting.","For SST-2, we observe that SP can exploit a manual prompt template significantly better than AutoPrompt, while being competitive using the best template optimized by AutoPrompt (auto-generated).",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,5,96,random,No Link,Link,"The method is surprisingly simple, the results are convincing as they significantly improve upon the state-of-the-art on the WiC task in the few-shot setting.","Notably, the Spearman correlation score, which is less commonly used for comparing embeddings, outperforms the cosine similarity on WiC by a large margin while maintaining the same level performance on other tasks.",False,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,6,5,retriever,No Link,No Link,The paper shows that the methodology is applicable to some other few-shot tasks as well.,"Trying to fill this gap, we propose a new prompting technique, based on similarity metrics, which boosts few-shot performance to the level of fully supervised methods.",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,6,27,llm,Link,Link,The paper shows that the methodology is applicable to some other few-shot tasks as well.,"Moreover, we show that with few adjustments, this simple approach can be effectively used for other downstream tasks.",True,True
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,6,73,random,No Link,No Link,The paper shows that the methodology is applicable to some other few-shot tasks as well.,Systems are evaluated either on a five-way fine-grained or binary classification task.,True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,6,83,random,No Link,No Link,The paper shows that the methodology is applicable to some other few-shot tasks as well.,Results,True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,7,14,random,No Link,No Link,Claims are substantiated and backed by evidence.,"Prompt-based techniques have shown impressive performance in the few-shot setting, especially when compared to standard fine-tuning on datasets of hundreds of data points (Scao and Rush, 2021).",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,7,66,random,No Link,No Link,Claims are substantiated and backed by evidence.,"In addition to WiC, we also carried out experiments on two more tasks.",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,9,3,random,No Link,No Link,"The main weakness of the paper is that it's very terse, partly due to the 4-page limit of a short submission.","However, despite proving competitive on most tasks in the GLUE and SuperGLUE benchmarks, existing prompt-based techniques fail on the semantic distinction task of the Word-in-Context (WiC) dataset.",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,9,50,random,No Link,No Link,"The main weakness of the paper is that it's very terse, partly due to the 4-page limit of a short submission.","Therefore, we ask PLM about the triggered meaning of the target word, separately for each context, and leave the comparison to similarity measures.",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,10,23,retriever,No Link,No Link,"This leads to potentially important information being omitted, see detailed comments below.",In this work we investigate the latter issue by introducing a new configuration for prompting.,True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,10,41,random,No Link,No Link,"This leads to potentially important information being omitted, see detailed comments below.",The third step is where SP differs from existing prompt-based approaches.,True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,10,44,random,No Link,No Link,"This leads to potentially important information being omitted, see detailed comments below.","However, this assumes the variance of different classes to be equal in the embedding space.",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,10,88,llm,No Link,No Link,"This leads to potentially important information being omitted, see detailed comments below.",We also include some detailed examples of how SP works for WiC in the Appendix.,True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,11,74,random,No Link,No Link,Some of the mentioned points could be easily alleviated by utilising the additional page that is provided upon acceptance in a venue.,We follow the latter (SST-2) in our experiments.,True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,11,97,random,No Link,No Link,Some of the mentioned points could be easily alleviated by utilising the additional page that is provided upon acceptance in a venue.,This superiority can be explained by the assumption that cosine similarity is more susceptible to variations in the dominant dimensions.,True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,13,19,llm,No Link,No Link,The following are points where i would like to see further clarifications: (NB: I do not have access to the forum/comments of the previous submission so some of my comments might have been addressed earlier.),Two issues could be responsible for the latter case:,True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,13,22,retriever,No Link,No Link,The following are points where i would like to see further clarifications: (NB: I do not have access to the forum/comments of the previous submission so some of my comments might have been addressed earlier.),"However, none of these have shown success on the WiC task.",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,13,23,retriever,No Link,No Link,The following are points where i would like to see further clarifications: (NB: I do not have access to the forum/comments of the previous submission so some of my comments might have been addressed earlier.),In this work we investigate the latter issue by introducing a new configuration for prompting.,True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,13,49,llm,No Link,No Link,The following are points where i would like to see further clarifications: (NB: I do not have access to the forum/comments of the previous submission so some of my comments might have been addressed earlier.),"Previous work has fallen short of designing a single prompt template which make the PLM answer about the target word having the same meaning or not (e.g., with ""yes"" or ""no"").",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,13,73,random,No Link,Link,The following are points where i would like to see further clarifications: (NB: I do not have access to the forum/comments of the previous submission so some of my comments might have been addressed earlier.),Systems are evaluated either on a five-way fine-grained or binary classification task.,False,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,13,100,random,No Link,No Link,The following are points where i would like to see further clarifications: (NB: I do not have access to the forum/comments of the previous submission so some of my comments might have been addressed earlier.),"However, the gain in the other two tasks is negligible.",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,14,18,random,No Link,No Link,- Why are SST-2 and SICK-E chosen as representative tasks to show that the proposed method generalises to other few-shot settings?,The natural question that arises here is if the failure of few-shot techniques on WiC is due to lack of relevant encoded knowledge in PLMs or the inefficiency of the employed prompt-based methods.,True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,14,67,llm,No Link,No Link,- Why are SST-2 and SICK-E chosen as representative tasks to show that the proposed method generalises to other few-shot settings?,"The goal of this additional experiment is twofold: first, to show the applicability of SP to other settings, including tasks with single input sequence; and second, to evaluate if SP is effective when using prompt templates from other techniques, including those optimized for specific tasks.",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,14,83,random,No Link,No Link,- Why are SST-2 and SICK-E chosen as representative tasks to show that the proposed method generalises to other few-shot settings?,Results,True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,14,90,retriever,No Link,Link,- Why are SST-2 and SICK-E chosen as representative tasks to show that the proposed method generalises to other few-shot settings?,The results on SST-2 and SICK-E are shown in Table 2.,False,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,15,24,random,No Link,No Link,Other papers seem to go for the full SuperGLUE suite.,"Given the comparison-based nature of WiC, we hypothesize that conventional prompting methods fall short since they only utilize a single prompt response.",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,15,44,random,No Link,No Link,Other papers seem to go for the full SuperGLUE suite.,"However, this assumes the variance of different classes to be equal in the embedding space.",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,15,62,retriever,Link,Link,Other papers seem to go for the full SuperGLUE suite.,"PET (Schick and Schütze, 2021b) prefers ALBERT-xxlarge-v2 (Lan et al., 2019) over RoBERTa (with an average gain of 8 points on a subset of SuperGLUE tasks) and fine-tunes it with manually engineered cloze-style prompts.",True,True
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,15,69,llm,No Link,No Link,Other papers seem to go for the full SuperGLUE suite.,The approach makes use of full training set to optimize discrete prompts for each specific target task.,True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,16,12,random,No Link,No Link,"Similarly, for those tasks, why is Autoprompt chosen as the only reference approach to compare against?","This paradigm has proven its effectiveness in the few-shot setting, even for relatively smaller models, such as BERT (Devlin et al., 2019) and RoBERTA , when combined with ensembling and fine-tuning (Schick and Schütze, 2021a).",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,16,65,random,No Link,Link,"Similarly, for those tasks, why is Autoprompt chosen as the only reference approach to compare against?",Tasks,False,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,16,91,retriever,No Link,Link,"Similarly, for those tasks, why is Autoprompt chosen as the only reference approach to compare against?",We compare SP with AutoPrompt which searches for the best template for each task.,False,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,16,92,llm,Link,Link,"Similarly, for those tasks, why is Autoprompt chosen as the only reference approach to compare against?","For SST-2, we observe that SP can exploit a manual prompt template significantly better than AutoPrompt, while being competitive using the best template optimized by AutoPrompt (auto-generated).",True,True
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,17,66,random,No Link,No Link,"Admittedly, the difference in performance to other approaches (at least for the SST-2 task) appears to not differ too much.","In addition to WiC, we also carried out experiments on two more tasks.",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,17,74,retriever,No Link,Link,"Admittedly, the difference in performance to other approaches (at least for the SST-2 task) appears to not differ too much.",We follow the latter (SST-2) in our experiments.,False,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,17,92,llm,No Link,No Link,"Admittedly, the difference in performance to other approaches (at least for the SST-2 task) appears to not differ too much.","For SST-2, we observe that SP can exploit a manual prompt template significantly better than AutoPrompt, while being competitive using the best template optimized by AutoPrompt (auto-generated).",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,17,99,random,No Link,No Link,"Admittedly, the difference in performance to other approaches (at least for the SST-2 task) appears to not differ too much.","The results approve the assumption: pruned cosine similarity gains around 10% absolute performance boost on WiC, filling the gap to Spearman correlation.",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,18,29,random,No Link,No Link,"- It is great to see that confidence scores were reported for the obtained results, but how exactly are they calculated?",Fine-tuning on a specific task can potentially update PLMs on what the task is and how to solve it.,True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,18,99,random,No Link,No Link,"- It is great to see that confidence scores were reported for the obtained results, but how exactly are they calculated?","The results approve the assumption: pruned cosine similarity gains around 10% absolute performance boost on WiC, filling the gap to Spearman correlation.",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,19,8,random,No Link,No Link,"- The high-level description helps to understand the approach intuitively, but a more detailed (e.g. mathematical) formulation, for example in the appendix, would be helpful as well.",Introduction,True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,19,55,random,No Link,Link,"- The high-level description helps to understand the approach intuitively, but a more detailed (e.g. mathematical) formulation, for example in the appendix, would be helpful as well.",We then train the same linear model as before on the similarity scores of the training set examples to find the best discriminating threshold.,False,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,19,88,retriever,No Link,No Link,"- The high-level description helps to understand the approach intuitively, but a more detailed (e.g. mathematical) formulation, for example in the appendix, would be helpful as well.",We also include some detailed examples of how SP works for WiC in the Appendix.,True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,19,106,llm,No Link,No Link,"- The high-level description helps to understand the approach intuitively, but a more detailed (e.g. mathematical) formulation, for example in the appendix, would be helpful as well.",Figure 2 (Appendix) illustrates the distribution of values for the most dominant dimension.,True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,20,61,random,No Link,No Link,"Similarly, the figure is supposed to help to understand the problem better, but I find it confusing in two ways: First, the figure is too abstract for me. Maybe having more text labels would help.","We compare our results on WiC with three other methods, all of which use 32 examples for their training.",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,20,69,random,No Link,Link,"Similarly, the figure is supposed to help to understand the problem better, but I find it confusing in two ways: First, the figure is too abstract for me. Maybe having more text labels would help.",The approach makes use of full training set to optimize discrete prompts for each specific target task.,False,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,21,3,retriever,No Link,No Link,"Second, depicting sentiment analysis, it does not align well with the main contribution of the paper, improvements on the WiC task.","However, despite proving competitive on most tasks in the GLUE and SuperGLUE benchmarks, existing prompt-based techniques fail on the semantic distinction task of the Word-in-Context (WiC) dataset.",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,21,47,llm,No Link,No Link,"Second, depicting sentiment analysis, it does not align well with the main contribution of the paper, improvements on the WiC task.","SP for WiC. The surprising failure of existing prompt-based techniques on the Word-in-Context task (Pilehvar and Camacho-Collados, 2019, WiC), motivated us to focus on filling this gap.",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,21,88,random,Link,No Link,"Second, depicting sentiment analysis, it does not align well with the main contribution of the paper, improvements on the WiC task.",We also include some detailed examples of how SP works for WiC in the Appendix.,False,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,21,104,random,No Link,Link,"Second, depicting sentiment analysis, it does not align well with the main contribution of the paper, improvements on the WiC task.","This results in a higher spread on the most dominant dimension in the case of WiC. It is known that the most dominant dimensions in PLMs often encode irrelevant information, such as word frequency (Gao et al., 2019), therefore hampering performance for sensitive metrics such as cosine similarity.",False,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,22,23,random,No Link,No Link,Maybe reworking the figure to depict the WiC task would help with both problems.,In this work we investigate the latter issue by introducing a new configuration for prompting.,True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,22,104,random,No Link,No Link,Maybe reworking the figure to depict the WiC task would help with both problems.,"This results in a higher spread on the most dominant dimension in the case of WiC. It is known that the most dominant dimensions in PLMs often encode irrelevant information, such as word frequency (Gao et al., 2019), therefore hampering performance for sensitive metrics such as cosine similarity.",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,23,5,random,No Link,No Link,"- Qualitative error analysis in the Appendix is great, but the manuscript so far lacks a more detailed analysis of the results.","Trying to fill this gap, we propose a new prompting technique, based on similarity metrics, which boosts few-shot performance to the level of fully supervised methods.",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,23,13,random,No Link,No Link,"- Qualitative error analysis in the Appendix is great, but the manuscript so far lacks a more detailed analysis of the results.","From the practical point of view, prompt-based learning is particularly well-suited for massive models, such as GPT-3, since it does not involve parameter tuning.",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,24,44,random,No Link,No Link,"One could wonder, for the WiC task, are the errors always due to models predicting ""matched"" for ""not matched"" GT? Is this similar to other approaches? For example, a by-class accuracy breakdown could answer some of these questions.","However, this assumes the variance of different classes to be equal in the embedding space.",True,False
d1ba3afd2bfd0e0e7bf886faedb96a9765b75a4bb26dac7dd35c137dc708c4a9616ad866e21b92b2964afad9fe2c693b9fb5c3696e3e1ad365fa2cee476d9da5,24,61,random,No Link,No Link,"One could wonder, for the WiC task, are the errors always due to models predicting ""matched"" for ""not matched"" GT? Is this similar to other approaches? For example, a by-class accuracy breakdown could answer some of these questions.","We compare our results on WiC with three other methods, all of which use 32 examples for their training.",True,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,1,3,mutual,Link,Link,This work proposes a multi-modal contrastive learning objective to learn text representation based on both visual and text information.,"In this work, we propose a multimodal contrastive learning approach that exploits both visual and textual information for learning sentence representations.",True,True
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,1,11,random,No Link,Link,This work proposes a multi-modal contrastive learning objective to learn text representation based on both visual and text information.,"In particular, there has been growing interest in adopting contrastive learning objectives to achieve this goal (Carlsson et al., 2020;Kim et al., 2021;Yan et al., 2021).",False,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,1,16,mutual,Link,Link,This work proposes a multi-modal contrastive learning objective to learn text representation based on both visual and text information.,"To exploit both visual and textual information, we adopt the state-of-the-art (SOTA) contrastive sentence embedding framework Sim-CSE (Gao et al., 2021) and extend it with a multimodal contrastive objective.",True,True
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,1,33,mutual,Link,Link,This work proposes a multi-modal contrastive learning objective to learn text representation based on both visual and text information.,"To exploit both visual and textual information, we adopt SimCSE (Gao et al., 2021) as the textual baseline and extend it with a multimodal contrastive learning objective.",True,True
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,1,109,random,No Link,No Link,This work proposes a multi-modal contrastive learning objective to learn text representation based on both visual and text information.,"In this paper, we propose MCSE that exploits both vision and textual information for sentence embedding learning.",True,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,2,4,mutual,Link,Link,Extensive experiments have been provided to show the improvement of the proposed method on seven semantic textual similarity tasks.,"Through experiments on a variety of semantic textual similarity tasks, we demonstrate that our approach consistently improves the performance across various datasets and pre-trained encoders.",True,True
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,2,16,random,No Link,No Link,Extensive experiments have been provided to show the improvement of the proposed method on seven semantic textual similarity tasks.,"To exploit both visual and textual information, we adopt the state-of-the-art (SOTA) contrastive sentence embedding framework Sim-CSE (Gao et al., 2021) and extend it with a multimodal contrastive objective.",True,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,2,18,mutual,Link,Link,Extensive experiments have been provided to show the improvement of the proposed method on seven semantic textual similarity tasks.,We conduct extensive experiments on standard Semantic Textual Similarity (STS) benchmarks and show the effectiveness of MCSE across various datasets and pre-trained encoders.,True,True
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,2,19,llm,Link,Link,Extensive experiments have been provided to show the improvement of the proposed method on seven semantic textual similarity tasks.,"We find that, using a small amount of multimodal data in addition to text-only corpus yields significant improvements on STS tasks.",True,True
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,2,62,random,Link,No Link,Extensive experiments have been provided to show the improvement of the proposed method on seven semantic textual similarity tasks.,"To further investigate the impact of different datasets, we train models solely on multimodal data and report results in Table 2.",False,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,2,113,retriever,No Link,No Link,Extensive experiments have been provided to show the improvement of the proposed method on seven semantic textual similarity tasks.,"Secondly, the definition of ""semantic similarity"" is highly task-dependent.",True,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,3,14,retriever,Link,No Link,"Besides, additional analyses show that the proposed method leads to better semantical alignment of the learned sentence representation.",We hypothesize that using vision as supplementary semantic information can further promote sentence representation learning.,False,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,3,16,random,No Link,No Link,"Besides, additional analyses show that the proposed method leads to better semantical alignment of the learned sentence representation.","To exploit both visual and textual information, we adopt the state-of-the-art (SOTA) contrastive sentence embedding framework Sim-CSE (Gao et al., 2021) and extend it with a multimodal contrastive objective.",True,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,3,20,llm,Link,Link,"Besides, additional analyses show that the proposed method leads to better semantical alignment of the learned sentence representation.","By analyzing the alignment and uniformity properties of the embedding space (Wang and Isola, 2020), we show that MCSE better aligns the semantically similar sentences while maintaining uniformity, providing an explanation for its superior performance.",True,True
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,3,45,random,No Link,No Link,"Besides, additional analyses show that the proposed method leads to better semantical alignment of the learned sentence representation.",We report the means and standard deviations.,True,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,3,54,llm,Link,No Link,"Besides, additional analyses show that the proposed method leads to better semantical alignment of the learned sentence representation.",Our method further regularizes the sentence representation in such a way that aligns with the image representation in the grounded space.,False,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,3,68,retriever,No Link,No Link,"Besides, additional analyses show that the proposed method leads to better semantical alignment of the learned sentence representation.",Gao et al. ( 2021) empirically showed that sentence embedding models with both low alignment and uniformity achieve better performance in general.,True,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,3,71,mutual,Link,Link,"Besides, additional analyses show that the proposed method leads to better semantical alignment of the learned sentence representation.","This analysis provides further support that by improving the alignment property of the textual embedding space, visually grounding can enhance sentence representation learning.",True,True
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,5,50,random,No Link,No Link,- The paper is well-written and clearly presented;,g ϕ 1 (•) and g ϕ 2 (•) are distinct projection heads for text and image modality respectively.,True,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,5,111,random,No Link,No Link,- The paper is well-written and clearly presented;,"Firstly, we take caption datasets as the source of multimodal information, while these datasets are collected and curated with nonnegligible human efforts.",True,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,6,14,mutual,Link,Link,- The idea of using visually grounded corpus to help learn sentence representation is well-motivated and novel.,We hypothesize that using vision as supplementary semantic information can further promote sentence representation learning.,True,True
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,6,28,retriever,No Link,No Link,- The idea of using visually grounded corpus to help learn sentence representation is well-motivated and novel.,Visually Grounded Representation Learning.,True,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,6,71,mutual,Link,No Link,- The idea of using visually grounded corpus to help learn sentence representation is well-motivated and novel.,"This analysis provides further support that by improving the alignment property of the textual embedding space, visually grounding can enhance sentence representation learning.",False,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,6,73,llm,Link,Link,- The idea of using visually grounded corpus to help learn sentence representation is well-motivated and novel.,"In this paper, we propose MCSE, a novel approach for sentence embedding learning that applies a multimodal contrastive objective to align sentences and corresponding images in a grounded space.",True,True
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,6,74,random,Link,No Link,- The idea of using visually grounded corpus to help learn sentence representation is well-motivated and novel.,Experiments show that MCSE consistently improves the performance on STS tasks.,False,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,6,90,random,No Link,No Link,- The idea of using visually grounded corpus to help learn sentence representation is well-motivated and novel.,We use the dev set of STS-B to tune the trade-off parameter λ and ablation studies are shown in Table 3 and Table 4.,True,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,7,14,random,No Link,No Link,Extensive experiments (with random seeds) have been provided to show the significance of the proposed methods.,We hypothesize that using vision as supplementary semantic information can further promote sentence representation learning.,True,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,7,18,llm,Link,Link,Extensive experiments (with random seeds) have been provided to show the significance of the proposed methods.,We conduct extensive experiments on standard Semantic Textual Similarity (STS) benchmarks and show the effectiveness of MCSE across various datasets and pre-trained encoders.,True,True
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,7,44,mutual,Link,Link,Extensive experiments (with random seeds) have been provided to show the significance of the proposed methods.,All other results are from our implementation and models are trained with 5 random seeds.,True,True
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,7,106,retriever,No Link,No Link,Extensive experiments (with random seeds) have been provided to show the significance of the proposed methods.,Cross-Modal Retrieval We take BERT-based models (same seed) and conduct cross-modal retrieval experiments.,True,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,7,110,random,No Link,No Link,Extensive experiments (with random seeds) have been provided to show the significance of the proposed methods.,"Despite showing a strong performance on STS benchmarks, it has a few limitations as well.",True,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,8,3,random,No Link,No Link,"Detailed ablation studies and analyses have also been provided to show the impact of dataset, pre-trained model and sub-tasks;","In this work, we propose a multimodal contrastive learning approach that exploits both visual and textual information for learning sentence representations.",True,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,8,50,random,No Link,No Link,"Detailed ablation studies and analyses have also been provided to show the impact of dataset, pre-trained model and sub-tasks;",g ϕ 1 (•) and g ϕ 2 (•) are distinct projection heads for text and image modality respectively.,True,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,8,62,retriever,Link,Link,"Detailed ablation studies and analyses have also been provided to show the impact of dataset, pre-trained model and sub-tasks;","To further investigate the impact of different datasets, we train models solely on multimodal data and report results in Table 2.",True,True
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,8,90,llm,Link,Link,"Detailed ablation studies and analyses have also been provided to show the impact of dataset, pre-trained model and sub-tasks;",We use the dev set of STS-B to tune the trade-off parameter λ and ablation studies are shown in Table 3 and Table 4.,True,True
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,9,6,random,No Link,No Link,- The idea can be potentially scaled up to many noisy image-text pairs on the web.,"By analyzing the properties of the textual embedding space, we show that our model excels in aligning semantically similar sentences, providing an explanation for its improved performance.",True,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,9,19,random,No Link,No Link,- The idea can be potentially scaled up to many noisy image-text pairs on the web.,"We find that, using a small amount of multimodal data in addition to text-only corpus yields significant improvements on STS tasks.",True,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,9,112,mutual,Link,Link,- The idea can be potentially scaled up to many noisy image-text pairs on the web.,"It will have great practical value if we can properly leverage noisy imagesentence pairs, or even get rid of the explicit alignments between images and sentences.",True,True
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,12,30,llm,Link,Link,"- Tan and Bansal (2020) shows the significant difference between the image-text corpus and wiki corpus (small sentence length, usually less than 77 and so on.)","Recently, Tan and Bansal (2020) and Tang et al. (2021) train large scale language models with multimodal supervision from scratch, with the goal of improving general language understanding.",True,True
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,12,63,retriever,No Link,No Link,"- Tan and Bansal (2020) shows the significant difference between the image-text corpus and wiki corpus (small sentence length, usually less than 77 and so on.)","We observe that, without the large text-only corpus, the performances decrease considerably compared to results in Table 1.",True,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,12,82,random,No Link,No Link,"- Tan and Bansal (2020) shows the significant difference between the image-text corpus and wiki corpus (small sentence length, usually less than 77 and so on.)",Projection Heads We use distinct projection heads for different modalities and objectives.,True,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,12,105,random,No Link,No Link,"- Tan and Bansal (2020) shows the significant difference between the image-text corpus and wiki corpus (small sentence length, usually less than 77 and so on.)","As shown in Table 5, MCSE models achieve 1.9 point and 2.6 point improvements when using BERT and RoBERTa, respectively.",True,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,13,22,retriever,No Link,Link,"But the dataset statistics have not been discussed here, wondering will become a problem or not for sentence representation learning.",Sentence Representation Learning.,False,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,13,41,random,No Link,No Link,"But the dataset statistics have not been discussed here, wondering will become a problem or not for sentence representation learning.","After training, the [CLS] token outputs of the language encoder are taken as the sentence embeddings.",True,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,13,45,random,No Link,No Link,"But the dataset statistics have not been discussed here, wondering will become a problem or not for sentence representation learning.",We report the means and standard deviations.,True,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,13,62,llm,No Link,No Link,"But the dataset statistics have not been discussed here, wondering will become a problem or not for sentence representation learning.","To further investigate the impact of different datasets, we train models solely on multimodal data and report results in Table 2.",True,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,15,3,random,No Link,No Link,See above,"In this work, we propose a multimodal contrastive learning approach that exploits both visual and textual information for learning sentence representations.",True,False
f747e56ba9dea371a105837138f7a51763dcbff3fd85e6dcae695a2f4b134a331bc69841f737a01318105e56c4606dd67a84e8f4550e265aba969bc471c379d2,15,85,random,No Link,No Link,See above,"We map both sentence embeddings and image feature vectors to a 256-dimensional shared space, and normalize them before calculating the multimodal objective.",True,False
